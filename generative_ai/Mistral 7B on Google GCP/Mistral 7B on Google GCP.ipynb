{
 "cells": [
  {
   "id": "658d867b71d5594ddd65968d",
   "cell_type": "markdown",
   "source": "Mistral 7B on Google GCP and DataRobot\n======================================\n\nThere are a wide variety of open source models. For example, there has been a lot of interest in LLama and variations such as Alpaca or Vicuna, Falcon, Mistral etc. Hosting these is a challenge as they require GPUs which are expensive so often customers want to compare cloud providers to find the best hosting option to meet their own needs. In this example we will work with Google Cloud Platform.\n\nIn addition, customers may want to integrate with the same cloud provider that hosts their VPC. That way they can ensure proper authentication and access only from within their VPC. While this authenticator uses authentication over the public internet, it should then be possible for the user to extend to leverage Google's cloud infrastructure to adjust to suit their cloud architectural needs, including provisioning scale out policies.\n\nFinally, by leveraging Vertex AI in a managed format, it can integrate into the customer's existing infrastructure level monitoring needs. For example, instances can be labelled to correspond to the customer's billing attribution polices, or logs and analytics can be set up to be written into their Big Query for monitoring and analytics. ",
   "metadata": {
    "name": "Overview",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a17dcb47e95b84b730ab81",
   "cell_type": "markdown",
   "source": "Mistral 7B\n========\n\nFor information about Mistral you can read the model card on HuggingFace [https://huggingface.co/mistralai/Mistral-7B-v0.1], the Arxiv page [https://arxiv.org/abs/2310.06825] and the release anouncement [https://mistral.ai/news/announcing-mistral-7b/]. It is available under an Apache 2.0 License [https://www.apache.org/licenses/LICENSE-2.0]\n\nIt features\n* Grouped-Query Attention\n* Rolling Buffer cache\n* Pre-fill and Chunking\n* Sliding-Window Attention\n* Byte-fallback BPE tokenizer\n\nthe standard model is designed around text completion. It is able to fit on a single A10G instance.\n\nSome data about the model:\nParameter Value\ndim 4096\nn_layers 32\nhead_dim 128\nhidden_dim 14336\nn_heads 32\nn_kv_heads 8\nwindow_size 4096\ncontext_len 8192\nvocab_size 32000\n\nMistral 7B-Instruct\n===============\n\nThis is designed to follow user instructions by fine-tuning on instruction datasets available on HuggingFace. As part of this it was trained to use `[INST]` and `[/INST]` controls tokens around user messages as well as begin of system id `<s>`. For example:\n\n* \"`<s>`[INST] What is your favourite condiment? [/INST]\"",
   "metadata": {
    "name": "Mistral Overview",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd65968e",
   "cell_type": "markdown",
   "source": "Overview of GCP\n===============\n    \nGCP instance types that can host Mistral with acceleration\n\n* g2-standard-8 with 1 L4 GPU: 8 vCPUs, 32 GB of RAM, \\$.85 ph + 64 GB (\\$623 per month)\n* n1-standard-16 with 2 V100 GPUs: 16 vCPUs, 60GB of RAM, \\$.76 ph + 32 GB (\\$388 per month)\n* n1-standard-16 with 2 T4 GPUs: 16 vCPUS, 60GB of RAM + 32 GB + 32 GB (\\$388 per month)\n* a2-highgpu-1g with 1 A100 GPU: 12 vCPUs, 85GB of RAM, \\$3.7 ph with  40GB (\\$2,682 per month)\n\n## 1. GCP Projects\n\nEverything in GCP is owned by a project, which tracks billing, authentication, access control, etc. Whenever you interact in either the GUI or using the API clients you will need to be in a project context. You can create a project at [https://console.cloud.google.com/projectcreate] or under IAM & Admin > Create a Project.\n\n## 2. Authentication and Service Accounts\n\nGCP does *not* provide \"API Keys\". Instead it provides auto-expiring dynamic tokens after authorizing. Each separate request you make to Google will have a different token in the headers. After a period of time you will have to reauthorize. If you use Collab, Workbench, or Cloud Shell, Google will hande the authentication in the background for you after authorization. \n\nIn order for our envionsed workflow to work, we will need a service account, an account which will run the cloud function workflow on our behalf (in this situation our account will be the principal). Using a service account, as opposed to a user account, means that multiple people can use our flow. In addition, there are certain things within GCP that can only be done by service accounts.\n\nThis service account will need access to the following roles:\n\n* Vertex AI User\n\nAs well as the following permissions on our Cloud Stoage bucket to be able to write to it.\n* Storage Legacy Bucket Owner\n* Storage Legacy Object Owner\n\nFor more information about service accounts look here: [https://cloud.google.com/iam/docs/service-accounts-create].\n\n## 3. Regions\n\nWe will use us-central1 (Iowa) for everything. This is because it is one of the two regions that have extensive GPU capacity (along with eu-west4 (Netherlands). The instance types available within Vertex AI vary by region.\n\n\n## 4. Cloud Storage Bucket\n\nLastly, before getting started we will need a bucket to hold stuff as we work in GCP. Google in general requires a bucket to be able to execute many of the tasks since they require storing some information somewhere, and a bucket is where that happens\n\nOnce you have decided on a project, location, bucket and storage account fill in the values below:\n                                                                                                                            ",
   "metadata": {
    "name": "Introduction to GCP",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d953e71d5594ddd659b6a",
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_ID = \"octo-385122\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
    "BUCKET_URI = \"gs://octo-ephermal-storage\"  # @param {type:\"string\"}\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = (\n",
    "    \"octo-experiments-service-accou@octo-385122.iam.gserviceaccount.com\"  # @param {type:\"string\"}\n",
    ")\n",
    "\n",
    "# GCP Tags\n",
    "# Fill in this dictionary with the required values to ensure the GCP resources are tagged correctly.\n",
    "# These will be applied as labels to the resources created\n",
    "GCP_TAGS = {\n",
    "    # e.g \"contact\": \"foo\"\n",
    "    #   \"contact\": \"100007\",\n",
    "    #    \"cost-center\": \"octo\",\n",
    "    #    \"label\": \"genai\",\n",
    "    #    \"environment\": \"dev\",\n",
    "    #    \"project\": \"genai\",\n",
    "    \"expiration\": (datetime.datetime.utcnow() + datetime.timedelta(days=2)).strftime(\n",
    "        \"%Y-%m-%d\"\n",
    "    )  # t%H:%M:%Sz\") # deault liftetime is up to 48H\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd65968f",
   "cell_type": "markdown",
   "source": "First time setup \n================\n\nThe following sections consist of the tasks you should do once the first time to setup the enviornment and then you will need to restart the kernel. We will do the following steps:\n\n1. Install the lastest version google cloud ai platform SDK.\n\nThis is the latest version that interacts with VertexAI\n\n2. Set up authentication by using application default credenitals created in a different environment\n\nThe typical authoriztion flow is command line command --> web browser authorization --> enter authorization information. In our notebooks that is tricky as we would need to be able to enter the authorization into the output shell. We also typically would generate both cli credentials via `gcloud init` or `gcloud auth login` which would create credentials in a local credentials db as well as create application default credentials via `gcloud auth application-default login` to generate the credentials usable via the python library. \n\nInstead the flow will be: generate credentials on your local machine --> base64 encode --> update the environment variables to point to the uploaded credentials. These will be stored [https://docs.datarobot.com/en/docs/dr-notebooks/code-nb/dr-env-nb.html#environment-variables encrypted by DataRobot] but keep in mind that base64 encoding itself isn't encryption and treat these base64 encoded credentials like you would treat any password.\n\n1. Create local credentials\n\nInstall the google sdk in your local enivornment. Then run `gcloud init` and make sure to set your default project accordingly. Then you can run `gcloud auth application-default login`\n\nTo install the google command line sdk in your local environment you can run:\n\n```\ncurl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-445.0.0-linux-x86_64.tar.gz\ntar -xf \"google-cloud-cli-445.0.0-linux-x86_64.tar.gz\"\n./google-cloud-sdk/install.sh --usage-reporting=false --quiet\n```\n\nYou should see output like:\n```\nCredentials saved to file: [/Users/mark/.config/gcloud/application_default_credentials.json]\n\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\n\nQuota project \"octo-385122\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n```\n\n2. Base64 credentials\n\nThe recommended version is to base64 encode as an environment variable. This is because any credentials uploaded to file storage is immediately shared when sharing the notebook. This can leverage DataRobot's built in credential storage.\n\n```\nbase64 -i /Users/mark/.config/gcloud/application_default_credentials.json\n```\n\nto generate the string. You can then copy and paste this as the environment variable: `GOOGLE_ENCODED_CREDENTIAL`\n\n3. Create credentials object\n\nIf you base64 encoded you can then create a credentials object in your local notebook environment via running the following:\n\n```\nimport base64\nimport os\nimport google.oauth2.credential\n\nadc_decoded = base64.b64decode(os.getenv(\"GOOGLE_ENCODED_CREDENTIAL\"))\ncredentials = google.oauth2.credentials.Credentials.from_authorized_user_info(json.loads(adc_decoded))\n```\n\n\nNow we can pass the credentials into `aiplatfrom.init` to set the global configuration which will be used in future calls to Vertex AI\n\n```\nfrom google.cloud import aiplatform\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET, credentials=credentials)\n```\n\nThe following two cells will install the required packages and initialize the environment while the third cell will print out the current settings for user verification\n",
   "metadata": {
    "name": "Credential setup",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd659690",
   "cell_type": "code",
   "source": [
    "!pip -q install --upgrade google-cloud-aiplatform\n",
    "!pip -q install requests\n",
    "!pip -q install datarobot-early-access"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d894571d5594ddd6597a3",
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "import google.oauth2.credentials\n",
    "\n",
    "adc_decoded = base64.b64decode(os.getenv(\"GOOGLE_ENCODED_CREDENTIAL\"))\n",
    "assert adc_decoded is not None\n",
    "credentials = google.oauth2.credentials.Credentials.from_authorized_user_info(\n",
    "    json.loads(adc_decoded)\n",
    ")\n",
    "\n",
    "\n",
    "# Bucket for storing intermediate stuff\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# Initialize the ai platform global configuration for future calls\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    credentials=credentials,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d9c4f71d5594ddd659d94",
   "cell_type": "code",
   "source": [
    "# We can verify that the global configuation is set correctly\n",
    "from google.cloud.aiplatform import initializer\n",
    "\n",
    "print(f\"The current project is: {initializer.global_config.project}\")\n",
    "print(f\"The current location is: {initializer.global_config.location}\")\n",
    "print(f\"The current GCP bucket is: {initializer.global_config.staging_bucket}\")\n",
    "print(f\"The current default service account is: {initializer.global_config.service_account}\")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "The current project is: octo-385122\nThe current location is: us-central1\nThe current GCP bucket is: gs://octo-ephermal-storage/temporal\nThe current default service account is: octo-experiments-service-accou@octo-385122.iam.gserviceaccount.com\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "65a17eda3f8961e25a9b23b4",
   "cell_type": "markdown",
   "source": "To deploy the model we will first upload it into Vertex's Model Registry and then we can deploy or undeploy from the endpoint where the actual provisioning of the instance happens. In Vertex AI the resources are set on the endpoint, not the model.\n\nModel Serving with pytorch and vLLM\n=======================\n\nVertex AI models can work with any docker container that provides an http endpoint for Vertex to pass along the generation request to. In this case the Vertex Endpoint will provide traffic sharing and versioning by handling the routing among multiple posssible backend models or instances while the docker container running the model will handle the actual request handling and prociessing.\n\nHence the need for both a model running framework (i.e. the actual loading of the model weights and computation) as well as a model serving framework (i.e. the web server and request processing). In this example we will use pytorch and VLLM.\n\nModel running frameworks\n========================\n\nModel running frameworks provide a variety of options that try to acheive different goals\n\n* pytorch\n* HuggingFace Transformers\n* HuggingFace Accelerate\n* DeepSpeed Inference\n* Nvidia TensorRT-LLM\n* FasterTransformer\n\nModel Serving Frameworks\n========================\n\nModel serving frameworks handle both request level management (e.g. batching and allocating requests among different works) as well as tasks like real time streaming of generation. While outside the scope of this example, they are also able to do things such as efficient request level switching between different fine-tuned variants of a model using techniques like LoRA, watermarking requests, or integration into telemetry infrastructure like Prometheus. While some frameworks like Triton are designed to work with a variety of different model running frameworks, others are tightly coupled to a particular framework.\n\nOpen source options include:\n* FastAPI\n* DJL-Serving [https://github.com/deepjavalibrary/djl-serving], Apache 2.0 license\n* NVIDIA Triton [https://github.com/triton-inference-server/server], BSD-3 license\n* vLLM [https://github.com/vllm-project/vllm], Apache 2.0 license\n* HunggingFace Text Generation Inference (TGI) [https://github.com/huggingface/text-generation-inference], HFOILv1.0 license. \n\nPytorch and vLLM on GCP\n=======================\n\nWe will use pytorch and vLLM for this example. vLLM is designed to provide efficient serving of multiple models in production settings so this should provide a good starting point for future configuration and testing what an actual production deployment might look like. It provides an OpenAI api compatible server with streaming and support for many popular models since it leverages HF Transformers, although because it requires perfomance tweaks not all models are supported out of the box [https://docs.vllm.ai/en/latest/models/supported_models.html]. A description of the various arguments is here [https://docs.vllm.ai/en/latest/models/engine_args.html].\n\nThe first cell defiens the various constants to use. The model id and instance information is easily adjustable to try out different options. We leverage the built in docker container from vertex, it is possible to instead build and upload an image to GCP for your own needs.\n\nThe next cell defines the deployment function. It creates the endpoint, uploads the docker container to create the model and then deploys the model onto the endpoint. The last cell calls it with the parameters from the first cell. Note that deploying can take a while.",
   "metadata": {
    "name": "Uploading to the GCP Model Registry and deploying to Vertex AI Endpoints",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd659694",
   "cell_type": "code",
   "source": [
    "# This is the name of the model on hugging face. When the docker container\n",
    "# launches on vertex AI it will download and start this up\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# The g2 instance come with built in GPUs support\n",
    "# The nvidia L4 is and Ada GPU that comes with 24GB of memory and 7,424 CUDA cores and 232 Tensor cores\n",
    "\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Docker image to be used for serving the model. We will use vertexAI provided vllm server to host the models\n",
    "VLLM_DOCKER_IMAGE = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd659695",
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,  # we will need to set the service account for use later\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys Mistral models with vLLM on Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\", labels=GCP_TAGS)\n",
    "\n",
    "    dtype = \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        dtype = \"float16\"\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--max-num-batched-tokens=8192\",  # trim the context length to work with our instance\n",
    "        \"--max-model-len=8192\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_IMAGE,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",  # route that predicts will be passed to\n",
    "        serving_container_health_route=\"/ping\",  # route used by vertex AI to verify the server is\n",
    "        labels=GCP_TAGS,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=1,  # autoscale to zero is not currently supported by GCP so please terminate your instance\n",
    "    )\n",
    "    return model, endpoint"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd659696",
   "cell_type": "code",
   "source": [
    "# Creating and deploying the endpoint will take some time (e.g. around 20 min)\n",
    "model_without_peft, endpoint_without_peft = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve\"),\n",
    "    model_id=model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")\n",
    "\n",
    "print(\"endpoint_name:\", endpoint_without_peft.name)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "text/plain": "Creating Endpoint\nCreate Endpoint backing LRO: projects/948912860068/locations/us-central1/endpoints/34949076600487936/operations/8008735975373012992\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "text/plain": "Endpoint created. Resource name: projects/948912860068/locations/us-central1/endpoints/34949076600487936\nTo use this Endpoint in another session:\nendpoint = aiplatform.Endpoint('projects/948912860068/locations/us-central1/endpoints/34949076600487936')\nCreating Model\nCreate Model backing LRO: projects/948912860068/locations/us-central1/models/3642125669938233344/operations/3111071380607598592\nModel created. Resource name: projects/948912860068/locations/us-central1/models/3642125669938233344@1\nTo use this Model in another session:\nmodel = aiplatform.Model('projects/948912860068/locations/us-central1/models/3642125669938233344@1')\nDeploying model to Endpoint : projects/948912860068/locations/us-central1/endpoints/34949076600487936\nDeploy Endpoint model backing LRO: projects/948912860068/locations/us-central1/endpoints/34949076600487936/operations/2167567258673479680\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "text/plain": "Endpoint model deployed. Resource name: projects/948912860068/locations/us-central1/endpoints/34949076600487936\nendpoint_name: 34949076600487936\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd659697",
   "cell_type": "markdown",
   "source": "The endpoint created is not public. Often a service is put in front to handle public requests, e.g. using cloud functions[https://cloud.google.com/functions] to authenticate the user request and then calling the endpoint. For services running in Goggle cloud on the same server account, Google will seamlessly hand the authentication. Since this examples uses a DataRobot deployment, any such logic can be added there so it is not include. Since the aiplatform is initalized with appropriate credentials it can just be called with that. Vertex AI expects that requests are in an `instances` array, it will take each element and pass that along to the model. It will then return the values from the endpoint in a `predictions` array.\n\nFor example as json:\n\n```\n{\ninstances: [{...}]\n}\n\n{\npredictions: [{...}]\n}\n```\n\nSince the endpoint is hitting the /generate route which is not OpenAI compatible, the input format is defined here: [https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py].\n\n```\n    The request should be a JSON object with the following fields:\n    - prompt: the prompt to use for the generation.\n    - stream: whether to stream the results or not.\n    - other fields: the sampling parameters (See `SamplingParams` for details).`\n```\n\nThe reponse will be a single JSON dict containing the key: `text` and the combined prompt + generated text as the value.",
   "metadata": {
    "name": "Prompting on GCP via credentials",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd659698",
   "cell_type": "code",
   "source": [
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"<s><INST>How can bagging trees boost my Random Forest?</INST>\",\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_without_peft.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "Prompt:\n<s><INST>How can bagging trees boost my Random Forest?</INST>\nOutput:\n Bagging, or Bootstrap Aggregating, is a method used to improve the performance and reduce overfitting of ensemble learning models such as Random Forests. Here's how it works:\n1. **Bootstrap samples**: Each tree in the Random Forest is built on a different bootstrap sample of the training data, drawn with replacement. This means that each tree will see a subset of the original training data, with some instances possibly being included multiple times.\n2. **Reducing variance**: By training each tree on slightly different data, Bagging helps to decrease the variance of the Random Forest. This is important because trees are prone to overfitting on the training data. When each tree is exposed to different instances in the training set, it will learn slightly different decision surfaces, and the errors will average out, resulting in better generalization performance.\n3. **Diversity**: Bagging also enhances the diversity of the trees in the\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd659699",
   "cell_type": "markdown",
   "source": "The endpoint URL can be called directly via REST. Since this a python environment, `requests` is a common library for making REST calls. To do so the authorization JWT token is included in the header. Since in this situation there are generated from personal credentials they will be short lived. To do so, the Google authentication libraries provide helper utilities to create a `requests.Session` object that manages adding the authentication token to the request.\n\nIf running locally, e.g. using `curl` the `gcloud` CLI can generate the tokens. To call via a subprocess invoke the following:\n```\nsubprocess.run([\"gcloud\", \"auth\", \"print-access-token\"], capture_output=True).stdout.decode().removesuffix(\"\\n\")\n```\nOr to invoke in the terminal:\n\n```\ngcloud auth print-access-token\n```\n",
   "metadata": {
    "name": "Prompting via REST",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd65969a",
   "cell_type": "code",
   "source": [
    "endpoint_url = f\"https://{endpoint_without_peft.location}-aiplatform.googleapis.com/v1/{endpoint_without_peft.resource_name}\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd65969b",
   "cell_type": "code",
   "source": [
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "# We will grab the acces token from the command line for our use to stick in the request headers\n",
    "# Note that for curl the command looks like\n",
    "# curl \\\n",
    "# -X POST \\\n",
    "# -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "# -H \"Content-Type: application/json\" \\\n",
    "# https://...\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "authed_session = AuthorizedSession(credentials)  # This creates a request.Session object\n",
    "\n",
    "response = authed_session.post(\n",
    "    endpoint_url + \":predict\", json={\"instances\": instances}, headers=headers\n",
    ")\n",
    "assert response.status_code == 200, response.json()\n",
    "print(response.json()[\"predictions\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 10,
     "data": {
      "text/plain": "Prompt:\n<s><INST>How can bagging trees boost my Random Forest?</INST>\nOutput:\n Bagging, or Bootstrap Aggregating, is a technique used to improve the performance and stability of machine learning models, including Random Forests, by creating multiple base models from different sub-samples of the original training dataset. Here's how it can boost Random Forest:\n1. Diversity: Bagging reduces overfitting by creating several Decision Trees and combining their outputs to increase the diversity of the model. By training trees on different samples of the data, Bagging helps ensure that the trees learn different aspects of the data, which in turn helps to reduce the correlation among the trees in the Random Forest.\n2. Reducing variance: Bagging reduces the variance of the model by averaging the predictions of individual Decision Trees. Since each tree is trained on a different bootstrap sample, the errors made by one tree may be different from the errors made by another tree. By combining the predictions of multiple trees, Bagging helps to reduce\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd65969c",
   "cell_type": "markdown",
   "source": " Since the URL is not public by default, it returns 401 unauthorized without a proper token",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd65969d",
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    endpoint_url + \":predict\",\n",
    "    json={\"instances\": instances},\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    ")\n",
    "assert response.status_code == 401\n",
    "response.json()"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 11,
     "data": {
      "text/plain": "{'error': {'code': 401,\n  'message': 'Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.',\n  'status': 'UNAUTHENTICATED',\n  'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo',\n    'reason': 'CREDENTIALS_MISSING',\n    'domain': 'googleapis.com',\n    'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.Predict',\n     'service': 'aiplatform.googleapis.com'}}]}}"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd65969e",
   "cell_type": "markdown",
   "source": "Creating a DataRobot Custom model deployment\n==================================\n\nTo bring this into DataRobot we can create a custom model. This will us to setup all the monitoring goodness of DataRobot around this model.\n\nBecause we would like to be able to have the deployment and be shareable without sharing our own private credentials (like we used above to interact with the deployment) we will use theservice account. \n\nFor local development we can impersonate the service account by using our existing credentials to generate service account credentials. This way we can validate that the service account is properly set up and has all the appropriate permissions.\n\n\n\nThen when we run as a deployment we can use Credential Management to hold the service account's private key. By doing this, the service account can then generate tokens using the private key for as long as the private key is valid. Also, by using a service account, on the google side it is no longer tied to the individual account and can be revoked separately. Lastly, if the key is compromised, the service account will only have the privileges granted it which are typically far less than a developer.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd65969f",
   "cell_type": "code",
   "source": [
    "# Redefine the constants from above so that can start right here with the session without redeploying on GCP\n",
    "SERVICE_ACCOUNT = \"octo-experiments-service-accou@octo-385122.iam.gserviceaccount.com\"\n",
    "\n",
    "# GCP Credential id from credential management with the uploaded private key\n",
    "DR_GCP_CREDENTIAL_ID = \"658c77e4ad86cd2ebecc9d7e\"\n",
    "\n",
    "# DR Prediction enviornment\n",
    "DR_PREDICTION_ENVIRONMENT_ID = \"5f06612df1740600260aca72\"\n",
    "\n",
    "\n",
    "custom_dir = \"mistral_gcp\"\n",
    "# api_token = subprocess.run([\"./gcloud\", \"auth\", \"print-access-token\", f\"--impersonate-service-account={SERVICE_ACCOUNT}\"], capture_output=True).stdout.decode().removesuffix(\"\\n\")\n",
    "\n",
    "# Here are the default runtime parameters we will need to set\n",
    "LOCATION = REGION  # or \"us-central1\"\n",
    "NUMERIC_PROJECT_ID = \"948912860068\"\n",
    "ENDPOINT_ID = endpoint_without_peft.name\n",
    "MAX_TOKENS = \"1000\"\n",
    "TEMPERATURE = \"1.0\"\n",
    "TOP_P = \"1.0\"\n",
    "TOP_K = \"10\"\n",
    "\n",
    "# This will be the prompt template that we wrap the prompt with. Mostly to handle the special tokens etc.\n",
    "SYSTEM_PROMPT_TEMPLATE = \"<s><INST>You are a helpful AI assistant, created by a French AI company. Be helpful and complete but also strive for concision. {}</INST> \""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd6596a0",
   "cell_type": "markdown",
   "source": "As is typical for custom inference models, we will need to provide a model-metadata.yaml and a custom.py. The existing GenAI builtin environment has the google cloud libraries built in along with the other libraries used so a requirements.txt is not required. In the model metadata, the target type is set as `textgeneration` to have access to text generation specific monitoring capabilities and the various runtime parameters are set as options. Runtime parameters allow easy updating of the DataRobot model endpoint if needed as part of a governed upgrade version. By changing the endpoint parameters and creating a new version, it is then possible to compare an older model with a newer version, if an update is required. It also allows configuration of the temperature etc. as part of the deployment and exposes these settings as metadata within DataRobot for reference. ",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596a1",
   "cell_type": "code",
   "source": [
    "model_metadata = \"\"\"\n",
    "name: gcp-vertex-proxy\n",
    "type: inference\n",
    "targetType: textgeneration\n",
    "\n",
    "runtimeParameterDefinitions:\n",
    "  - fieldName: location\n",
    "    type: string\n",
    "    defaultValue: us-central1\n",
    "    description: The GCP location the endpoint resides in.\n",
    "\n",
    "  - fieldName: gcp_credentials\n",
    "    type: credential\n",
    "    credentialtype: gcp\n",
    "    description: The GCP service account key. This will be set to use the credential id set in credential manageament\n",
    "    \n",
    "  - fieldName: projectId\n",
    "    type: string\n",
    "    description: Numeric GCP projet for the endpoint\n",
    "  \n",
    "  - fieldName: endpointId\n",
    "    type: string\n",
    "    description: Numeric Vertex AI endpoint id\n",
    "\n",
    "  - fieldName: maxTokens\n",
    "    type: string\n",
    "    defaultValue: \"200\"\n",
    "    description: Maximum number of tokens to return\n",
    "    \n",
    "  - fieldName: temperature\n",
    "    type: string\n",
    "    defaultValue: \"1.0\"\n",
    "    description: temperature of the model\n",
    "\n",
    "  - fieldName: promptTemplate\n",
    "    type: string\n",
    "    description: promptTemplate for the model. It should contain {} which will be filled in with the user prompt\n",
    "\n",
    "  - fieldName: topP\n",
    "    type: string\n",
    "    defaultValue: \"1.0\"\n",
    "    description: Top p for the model\n",
    "    \n",
    "  - fieldName: topK\n",
    "    type: string\n",
    "    defaultValue: \"10\"\n",
    "    description: Top k for the model\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a7f3eec808871bb8436a04",
   "cell_type": "markdown",
   "source": "And here is the custom py file written as text.\n\nIn the load model function we load the runtime parameters and verify that we can connect to the endpoint. It will prefer the passed token over the GCP service account credentials if provided. \n\nIt will then take the user provide prompt, inject it into the promptTemplate with a format and then make a prediction. Lastly, because Mistral returns the prompt in the response, the original prompt is stripped out. Because the custom model API provides the data as pandas DataFrame into the `score` function and expects the output as a DataFrame as well, this is all run in a loop over the dataframe, causing each row of the DataFrame to be processed as a separate request. \n\nThe prompt column is set to `prompt` to match the name that will be set when the model is registercd with the playground. The generated text column is set to `response` and will be registered as the `target name` when the DR Deployment is created.\n\nLastly, this and the model metadata yaml are then written to the filesystem inside a directory for uploading to datarobot.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596a2",
   "cell_type": "code",
   "source": [
    "custom_py = \"\"\"\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import ssl\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datarobot_drum import RuntimeParameters\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import google.auth.credentials\n",
    "from google.auth.transport.urllib3 import AuthorizedHttp\n",
    "import google.oauth2.credentials\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _test_connectivity(location, project, endpoint_id, creds):\n",
    "    # The root path of the endpoint can be used for health checks\n",
    "    url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/endpoints/{endpoint_id}\"\n",
    "    authed_http = AuthorizedHttp(creds)  # Implements a urllib.RequestsMethods like a PoolManager\n",
    "    try:\n",
    "        authed_http.request('GET', url=url)\n",
    "    except urllib.error.HTTPError as error:\n",
    "        logger.error(\n",
    "            \"Failed to connect to %s status_code=%s\\\\n%s\",\n",
    "            url,\n",
    "            error.code,\n",
    "            error.read().decode(\"utf8\", \"ignore\"),\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_model(code_dir):\n",
    "    ''' Load runtime parameters and verify the endpoint is up'''\n",
    "    logger.info(\"Loading Runtime Parameters...\")\n",
    "    cred_parameter = RuntimeParameters.get('gcp_credentials')['gcpKey']\n",
    "\n",
    "    endpoint_id = RuntimeParameters.get(\"endpointId\")\n",
    "    location = RuntimeParameters.get(\"location\")\n",
    "    project = RuntimeParameters.get(\"projectId\")\n",
    "    prompt_template = RuntimeParameters.get(\"promptTemplate\")\n",
    "    max_tokens = int(RuntimeParameters.get(\"maxTokens\"))\n",
    "    temperature = float(RuntimeParameters.get(\"temperature\"))\n",
    "    top_p = float(RuntimeParameters.get(\"topP\"))\n",
    "    top_k = int(RuntimeParameters.get(\"topK\"))\n",
    "    \n",
    "    gcp_token = os.getenv('GOOGLE_ENCODED_SERVICE_ACCOUNT_TOKEN', None)\n",
    "    if gcp_token:\n",
    "        # Handle case user provided base64 encoded credentials, e.g. in a notebook\n",
    "        creds = google.oauth2.credentials.Credentials(token=gcp_token)\n",
    "    else:\n",
    "        # Handle case it pulled the values from the runtime parameters storage\n",
    "        creds = service_account.Credentials.from_service_account_info(cred_parameter)\n",
    "        creds = creds.with_scopes(['https://www.googleapis.com/auth/cloud-platform'])\n",
    "    _test_connectivity(location, project, endpoint_id, creds)\n",
    "\n",
    "    # Can return any object as a placeholder for a model that we can\n",
    "    # then use again in the `score()` function.\n",
    "    return SimpleNamespace(**locals())\n",
    "\n",
    "\n",
    "def make_vertex_prediction(user_prompt, model):\n",
    "    prompt = model.prompt_template.format(user_prompt)\n",
    "    instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": model.max_tokens,\n",
    "        \"temperature\": model.temperature,\n",
    "        \"top_p\": model.top_p,\n",
    "        \"top_k\": model.top_k,\n",
    "    },\n",
    "]\n",
    " \n",
    "    endpoint = aiplatform.Endpoint(model.endpoint_id, project=model.project,\n",
    "      location=model.location, credentials=model.creds)\n",
    "    response = endpoint.predict(instances=instances)\n",
    "\n",
    "    for prediction in response.predictions:\n",
    "        # Remove everything from the response but the generated text\n",
    "        out = prediction.partition(\"Output:\\\\n\")[2]\n",
    "    return out\n",
    "\n",
    "def score(data, model, **kwargs):\n",
    "    '''\n",
    "    This hook is only needed if you would like to use **drum** with a framework not natively\n",
    "    supported by the tool.\n",
    "\n",
    "    Note: While best practice is to include the score hook, if the score hook is not present\n",
    "    DataRobot will add a score hook and call the default predict method for the library\n",
    "    See https://github.com/datarobot/datarobot-user-models#built-in-model-support for details\n",
    "\n",
    "    This dummy implementation reverses all input text and returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : is the dataframe to make predictions against.\n",
    "    model : is the deserialized model loaded by **drum** or by `load_model`, if supplied\n",
    "    kwargs : additional keyword arguments to the method\n",
    "    Returns\n",
    "    -------\n",
    "    This method should return results as a dataframe with the following format:\n",
    "      Text Generation: must have column with target, containing text data for each input row.\n",
    "    '''\n",
    "    data = list(data[\"prompt\"])\n",
    "    generated_responses = [\"\".join(make_vertex_prediction(inp, model)) for inp in data]\n",
    "    result = pd.DataFrame({\"response\": generated_responses})\n",
    "    return result\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd6596a3",
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.listdir(custom_dir)\n",
    "except:\n",
    "    os.mkdir(custom_dir)\n",
    "with open(f\"{custom_dir}/model-metadata.yaml\", \"w\") as f:\n",
    "    f.write(model_metadata)\n",
    "\n",
    "with open(f\"{custom_dir}/custom.py\", \"w\") as f:\n",
    "    f.write(custom_py)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd6596a4",
   "cell_type": "markdown",
   "source": "We can test locally using our local credentials and drum to make sure everything is working correctly. We will use our local credentials to create a token for the service account via a method call impersonation. Impersonation allows a user to use their credentials, the source, to then get the credentials of a different entity, the target. Because both the scopes, what services the token is valid for, and the permissions are able to be narrowly defined, these tokens are generally far less powerful than the source credentials.\n\nSeparately, this makes it easier to manage access across multiple accounts. As long as a user has access to the service account, they have access to the resources to make a prediction without worrying about needing to grant the appropriate permissions for each indiviudal resource, it also means that a user can be removed easily without breaking things (e.g. if the creator leaves the organization).\n\nThis code makes a subprocess call to the `drum` custom model utility and checks that it completed successful. The output predictions should be displayed.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596a5",
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "from google.auth import impersonated_credentials\n",
    "\n",
    "runtimeparams = f\"\"\"\n",
    "location: \"{LOCATION}\"\n",
    "projectId: \"{NUMERIC_PROJECT_ID}\"\n",
    "endpointId: \"{ENDPOINT_ID}\"\n",
    "maxTokens: \"{MAX_TOKENS}\"\n",
    "temperature: \"{TEMPERATURE}\"\n",
    "topP: \"{TOP_P}\"\n",
    "topK: \"{TOP_K}\"\n",
    "\n",
    "promptTemplate: \"{SYSTEM_PROMPT_TEMPLATE}\"\n",
    "\n",
    "gcp_credentials:\n",
    "  credentialType: \"gcp\"\n",
    "  gcpKey:  \"FAKE_CREDS\"\n",
    "\"\"\"\n",
    "\n",
    "input_csv = \"prompt,\\nHow soon is now?,\\nWhat are the names of the Greek winds?,\"\n",
    "\n",
    "with open(\"mistral_values\", \"w\") as f:\n",
    "    f.write(runtimeparams)\n",
    "\n",
    "with open(\"input.csv\", \"w\") as f:\n",
    "    f.write(input_csv)\n",
    "\n",
    "\n",
    "target_scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "target_credentials = impersonated_credentials.Credentials(\n",
    "    source_credentials=credentials,\n",
    "    target_principal=SERVICE_ACCOUNT,\n",
    "    target_scopes=target_scopes,\n",
    "    lifetime=500,\n",
    ")\n",
    "\n",
    "# Call refresh to generate the token\n",
    "target_credentials.refresh(request=google.auth.transport.requests.Request())\n",
    "\n",
    "envs = os.environ\n",
    "envs[\"GOOGLE_ENCODED_SERVICE_ACCOUNT_TOKEN\"] = target_credentials.token\n",
    "subprocess.check_call(\n",
    "    [\n",
    "        \"drum\",\n",
    "        \"score\",\n",
    "        \"--code-dir\",\n",
    "        f\"{custom_dir}\",\n",
    "        \"--target-type\",\n",
    "        \"textgeneration\",\n",
    "        \"--input\",\n",
    "        \"input.csv\",\n",
    "        \"--runtime-params-file\",\n",
    "        \"mistral_values\",\n",
    "    ],\n",
    "    env=envs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 46,
     "data": {
      "text/plain": "                                         Predictions\n0  The phrase \"How soon is now?\" can be interpret...\n1   Certainly, I'd be happy to help you with that...\n0"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "65a1a1c67cd753986c55ef32",
   "cell_type": "markdown",
   "source": "Setting Runtime Parameters and Registering the Custom Model\n===========================================================\n\nNow that the it works locally it can be uploaded to DataRobot and added to the Model Registry. This will\n\n* Execution Environment - The python enviornment in which the code will run. This will use the prebuilt `Python 3,11 GenAI` environment, the last prerequisite for creating the Custom Model\n* Custom Inference Model - Created in the Custom Model Workshop with the appropriate metadata\n* Custom Inference Model Vesion - A particular version of the environment, python code and runtime parameters under the Custom Inferene Model\n* Custom Model Test -  Verification that the Custom Model works in the DataRobot environment\n* Registered Model - Moved from the Workshop to the Registry. At this point appropriate governance policies can be applied\n* Prediction Environment - Where the model is hosted if the account has multiple hosting options.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596a6",
   "cell_type": "code",
   "source": [
    "import datarobot as dr\n",
    "\n",
    "execution_environment = dr.ExecutionEnvironment.list(\"Python 3.11 GenAI\")[0]"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd6596a7",
   "cell_type": "code",
   "source": [
    "# Patch to asllow TextGeneration target types in the python client.\n",
    "dr.enums.CUSTOM_MODEL_TARGET_TYPE.ALL = dr.enums.CUSTOM_MODEL_TARGET_TYPE.ALL + [\"TextGeneration\"]\n",
    "\n",
    "custom_model = dr.CustomInferenceModel.create(\n",
    "    name=\"Mistral-7B Instruct on GCP\",\n",
    "    target_type=\"TextGeneration\",\n",
    "    target_name=\"response\",  # Name as the output column we generate\n",
    "    description=\"Mistral-7B proxy model\",\n",
    "    language=\"python\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a7fbe8c808871bb8436ca8",
   "cell_type": "markdown",
   "source": "After creating the specific model version, where we set that the container will have public network egress to be able to make calls over the internet.\n\nThe runtime parameters can be set via patching the REST API route until the datarobot sdk is updated to include setting runtime parameters.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596a8",
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "model_version = dr.CustomModelVersion.create_clean(\n",
    "    custom_model_id=custom_model.id,\n",
    "    base_environment_id=execution_environment.id,\n",
    "    folder_path=custom_dir,\n",
    "    network_egress_policy=dr.NETWORK_EGRESS_POLICY.PUBLIC,  # need to be able to reach GCP over the public internet\n",
    ")\n",
    "\n",
    "url = model_version._path.format(model_version.custom_model_id)\n",
    "path = f\"{url}\"\n",
    "payload = {\n",
    "    \"baseEnvironmentId\": execution_environment.id,\n",
    "    \"runtimeParameterValues\": json.dumps(\n",
    "        [\n",
    "            {\"fieldName\": \"temperature\", \"type\": \"string\", \"value\": TEMPERATURE},\n",
    "            {\"fieldName\": \"location\", \"type\": \"string\", \"value\": LOCATION},\n",
    "            {\"fieldName\": \"projectId\", \"type\": \"string\", \"value\": NUMERIC_PROJECT_ID},\n",
    "            {\"fieldName\": \"endpointId\", \"type\": \"string\", \"value\": ENDPOINT_ID},\n",
    "            {\"fieldName\": \"maxTokens\", \"type\": \"string\", \"value\": MAX_TOKENS},\n",
    "            {\"fieldName\": \"topP\", \"type\": \"string\", \"value\": TOP_P},\n",
    "            {\"fieldName\": \"topK\", \"type\": \"string\", \"value\": TOP_K},\n",
    "            {\"fieldName\": \"promptTemplate\", \"type\": \"string\", \"value\": SYSTEM_PROMPT_TEMPLATE},\n",
    "            {\"fieldName\": \"gcp_credentials\", \"type\": \"credential\", \"value\": DR_GCP_CREDENTIAL_ID},\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "response = model_version._client.patch(path, json=payload)\n",
    "model_version = dr.CustomModelVersion.get(custom_model.id, response.json()[\"id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a7fdfcc808871bb8436d51",
   "cell_type": "markdown",
   "source": "To validste that everything works, there is a testing facility provided. This requires uploading a dataset to then pass into the test to verify it works. After uploading and asserting that it succeeded this uploaded dataset can then be deleted. ",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658daee08d68ba5f8f1f20bc",
   "cell_type": "code",
   "source": [
    "# Upload and create a dataset for testing custom model to validate it works correctly\n",
    "\n",
    "test_dataset = dr.Dataset.create_from_file(\"input.csv\")\n",
    "test_dataset.id"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 50,
     "data": {
      "text/plain": "'659711536646d197efceb6af'"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd6596a9",
   "cell_type": "code",
   "source": [
    "custom_test = dr.CustomModelTest.create(\n",
    "    custom_model_id=custom_model.id,\n",
    "    custom_model_version_id=model_version.id,\n",
    "    dataset_id=test_dataset.id,\n",
    ")\n",
    "assert custom_test.overall_status == \"succeeded\", custom_test.get_log()"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "658f16757f3d2f88f81e426c",
   "cell_type": "code",
   "source": [
    "# now delete the uploaded test dataset for the custom model test\n",
    "test_dataset.delete(dataset_id=test_dataset.id)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a83f4db7e1adf036c9035e",
   "cell_type": "markdown",
   "source": "Now that it has been verified to work, the model can be added to the registry and prepared for deployment",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596aa",
   "cell_type": "code",
   "source": [
    "registered_model = dr.RegisteredModelVersion.create_for_custom_model_version(model_version.id)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a1a2327cd753986c55ef52",
   "cell_type": "markdown",
   "source": "Creating and Testing the DR Deployment\n======================================\n\nFrom the registry it is strightforward to deploy given a prediction environment to deploy the model into.\n\nPrediction Options\n\n* Predict Batch: Predict using a Dataframe and the DataRobot client with the prompts as a column. The name of the column should match what we set in the custom model. This can score a large number of requests and can work in an async fashion.\n* Realtime Predictions: Predict using JSON or CSV directly against the deployment in a synchronous fashion\n\nTo setup a synchronous prediction using either a cli like cURL or using the python requests library see the Predictions tab on the deployment and select Real-Time. \n",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658dac9dd4ee5de6e651c6d3",
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "deployment = dr.Deployment.create_from_registered_model_version(\n",
    "    registered_model.id,\n",
    "    \"Mistal 7B instruct on GCP\",\n",
    "    prediction_environment_id=DR_PREDICTION_ENVIRONMENT_ID,\n",
    ")\n",
    "\n",
    "prompt_df = pd.DataFrame({\"prompt\": [\"How soon is now?\", \"What is your favorite wind?\"]})\n",
    "deployment.predict_batch(prompt_df)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 54,
     "data": {
      "text/plain": "Streaming DataFrame as CSV data to DataRobot\nCreated Batch Prediction job ID 659712c7bc8c7fd1baccff39\nWaiting for DataRobot to start processing\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 54,
     "data": {
      "text/plain": "Job has started processing at DataRobot. Streaming results.\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 54,
     "data": {
      "text/plain": "                        prompt  \\\n0             How soon is now?   \n1  What is your favorite wind?   \n\n                                 response_PREDICTION  \\\n0  I'm an AI and don't have a concept of time or ...   \n1  As a helpful and concise AI assistant, I don't...   \n\n  DEPLOYMENT_APPROVAL_STATUS  \n0                   APPROVED  \n1                   APPROVED  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response_PREDICTION</th>\n      <th>DEPLOYMENT_APPROVAL_STATUS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How soon is now?</td>\n      <td>I'm an AI and don't have a concept of time or ...</td>\n      <td>APPROVED</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is your favorite wind?</td>\n      <td>As a helpful and concise AI assistant, I don't...</td>\n      <td>APPROVED</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/vnd.dataframe+json": {
       "data": [
        {
         "_dr_df_index": 0,
         "prompt": "How soon is now?",
         "response_PREDICTION": "I'm an AI and don't have a concept of time or the ability to experience it. However, I can help you understand that the phrase \"How soon is now?\" can be interpreted in different ways. If you mean \"What is the current time?\", I can tell you that based on the information from my database or the system clock of the device I'm running on. If you mean \"When can I expect an event or action to occur?\", I'd be glad to help you check your calendar or find the answer to that question based on the information you provide. But in either case, I can't provide an answer as you haven't specified a particular time or event. So, could you please provide more context or details?",
         "DEPLOYMENT_APPROVAL_STATUS": "APPROVED"
        },
        {
         "_dr_df_index": 1,
         "prompt": "What is your favorite wind?",
         "response_PREDICTION": "As a helpful and concise AI assistant, I don't have personal experiences or preferences. However, I can provide information on various topics. Regarding your question, there isn't a concept of a \"favorite wind\" for an object or entity like me. Wind is a natural phenomenon, and each type  be it a gentle breeze or a powerful gale  has its unique characteristics. For humans or other organisms, a pleasant wind might be a gentle breeze that provides comfort or helps cool down during hot weather. But for me, I don't feel temperature or discomfort, so the wind itself doesn't have a preference.",
         "DEPLOYMENT_APPROVAL_STATUS": "APPROVED"
        }
       ],
       "columns": [
        {
         "name": "_dr_df_index",
         "type": "integer"
        },
        {
         "name": "prompt",
         "type": "string"
        },
        {
         "name": "response_PREDICTION",
         "type": "string"
        },
        {
         "name": "DEPLOYMENT_APPROVAL_STATUS",
         "type": "string"
        }
       ],
       "count": 2,
       "totalCount": 2,
       "offset": 0,
       "limit": 10,
       "referenceId": 140103659541168,
       "sortedBy": "",
       "indexKey": "_dr_df_index",
       "error": []
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "65a1a27d558a7f7f2e72ca60",
   "cell_type": "markdown",
   "source": "Making the Deployment Available in the GenAI Playground\n=======================================================\n\nUsing the datarobot-early-access client wwe can now register the model with the playground after it passes validation and then it will be available inside the playground to explore, compare against other models and hook up to grounding data in a Vector Database",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596ab",
   "cell_type": "code",
   "source": [
    "from datarobot._experimental.models.genai.custom_model_llm_validation import (\n",
    "    CustomModelLLMValidation,\n",
    ")\n",
    "\n",
    "# If this doesn't work, try reinstalling or upgrading datarobot-early-accesss\n",
    "\n",
    "custom_model_llm_validation = CustomModelLLMValidation.create(\n",
    "    prompt_column_name=\"prompt\",\n",
    "    target_column_name=\"response\",\n",
    "    deployment_id=deployment.id,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "assert custom_model_llm_validation.validation_status == \"PASSED\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 55,
     "data": {
      "text/plain": "You have imported from the _experimental directory.\nThis directory is used for unreleased datarobot features.\nUnless you specifically know better, you don't have the access to use this functionality in the app, so this code will not work.\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "id": "658d867b71d5594ddd6596ac",
   "cell_type": "markdown",
   "source": "Deleting Resources\n================\n\nTo delete the created endpoint, set the following to true and then run the following.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "658d867b71d5594ddd6596ad",
   "cell_type": "code",
   "source": [
    "delete_dr_deployment = False\n",
    "archive_dr_model_package = False\n",
    "delete_dr_custom_model = False\n",
    "delete_dr_llm_blueprint = False\n",
    "delete_gcp_endpoint = False\n",
    "delete_gcp_model = False\n",
    "\n",
    "\n",
    "def delete_gcp_endpoint_by_id(endpoint_id):\n",
    "    endpoint = aiplatform.Endpoint(endpoint_id, project=PROJECT_ID)\n",
    "    endpoint.undeploy_all()\n",
    "    endpoint.delete()\n",
    "\n",
    "\n",
    "def delete_gcp_model_by_id(model_id):\n",
    "    aiplatform.Model(model_id, project=PROJECT_ID).delete()\n",
    "\n",
    "\n",
    "def delete_dr_deployment_by_id(deployment_id):\n",
    "    dr.Deployment.get(deployment_id).delete()\n",
    "\n",
    "\n",
    "def archive_dr_registered_model_by_id(registered_model_id):\n",
    "    dr.RegisteredModel.get(registered_model_id).archive(registered_model_id)\n",
    "\n",
    "\n",
    "def delete_dr_custom_model_by_id(custom_model_id):\n",
    "    dr.CustomInferenceModel.get(custom_model_id).delete()\n",
    "\n",
    "\n",
    "def delete_dr_llm_validation_by_id(genai_validation_id):\n",
    "    dr._experimental.models.genai.custom_model_llm_validation.CustomModelLLMValidation.get(\n",
    "        genai_validation_id\n",
    "    ).delete()\n",
    "\n",
    "\n",
    "if delete_gcp_endpoint:\n",
    "    try:\n",
    "        delete_gcp_endpoint_by_id(endpoint_without_peft.name)\n",
    "        print(f\"GCP Endpoint {endpoint_without_peft.name} deleted\")\n",
    "    except google.api_core.exceptions.NotFound:\n",
    "        pass\n",
    "if delete_gcp_model:\n",
    "    try:\n",
    "        delete_gcp_model_by_id(model_without_peft.name)\n",
    "        print(f\"GCP Model {model_without_peft.name} deleted\")\n",
    "    except google.api_core.exceptions.NotFound:\n",
    "        pass\n",
    "if delete_dr_deployment:\n",
    "    try:\n",
    "        delete_dr_deployment_by_id(deployment.id)\n",
    "        print(f\"DR Deployment {deployment.id} deleted\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise\n",
    "if archive_dr_model_package:\n",
    "    try:\n",
    "        archive_dr_registered_model_by_id(registered_model.model_id)\n",
    "        print(f\"DR Registered Model {registered_model.model_id} archived\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise\n",
    "if delete_dr_custom_model:\n",
    "    try:\n",
    "        delete_dr_custom_model_by_id(custom_model.id)\n",
    "        print(f\"DR Custom Model {custom_model.id} deleted\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise\n",
    "if delete_dr_llm_blueprint:\n",
    "    try:\n",
    "        delete_dr_llm_validation_by_id(custom_model_llm_validation.id)\n",
    "        print(f\"DR Custom Model LLM Blueprint {custom_model_llm_validation.id} deleted\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python",
   "language": "python",
   "display_name": "Python 3.9.18"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}