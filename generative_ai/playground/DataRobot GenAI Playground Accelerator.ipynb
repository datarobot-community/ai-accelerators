{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d74",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Datarobot generative AI capabilities overview\n",
    "\n",
    "**Author**: Gustav von Zitzewitz</br>\n",
    "**Date**: May 29, 2024\n",
    "\n",
    "In November 2023, DataRobot launched its new generative AI products including LLM playgrounds and vector databases for doing Retriever Augmented Generation (RAG) workflows.</br>\n",
    "This accelerator walks through all of the new DataRobot generative AI capabilities and serves as a template for code-first users.\n",
    "It showcases how to create and use playgrounds, connect vector databases and LLMs, and how to query and compare them. It uses the official DataRobot documentation as an example. Furthermore, it combines DataRobot's well-known custom model capabilities to allow users to bring their own LLMs and vector databases. It shows how to link, use, and deploy them together with the ones offered by DataRobot.\n",
    "\n",
    "## Task overview\n",
    "\n",
    "- Create DataRobot generative AI playgrounds\n",
    "- Create DataRobot LLM-Blueprints\n",
    "- Chat prompt single LLM-Blueprints\n",
    "- Create DataRobot vector databases\n",
    "- Investigate vector database assets\n",
    "- Create LLM-Blueprints that use vector databases (RAG)\n",
    "- Create and deploy external vector databases\n",
    "- Create and deploy external LLMs\n",
    "- Validate and register external vector databases and LLMs to playgrounds\n",
    "- Comparison prompt multiple LLM-Blueprints\n",
    "- Deploy LLM-Blueprints and make predictions\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d75",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d76",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We start with installing the newest datarobotx package\n",
    "# This is necessary to leverage sophisticated model deployment features\n",
    "!pip install -U datarobotx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d77",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "Next, install all external libraries that you are going to use in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d78",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.333 faiss-cpu==1.7.4 sentence-transformers==2.2.2 pyarrow==16.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d79",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6645f80c0bc6d56121831d7a",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After installing the packages we import all modules that we'll use in this notebook\n",
    "import json\n",
    "\n",
    "import datarobot as dr\n",
    "from datarobot.enums import VectorDatabaseChunkingMethod, VectorDatabaseEmbeddingModel\n",
    "from datarobot.models.genai.chat_prompt import ChatPrompt\n",
    "from datarobot.models.genai.comparison_prompt import ComparisonPrompt\n",
    "from datarobot.models.genai.custom_model_llm_validation import CustomModelLLMValidation\n",
    "from datarobot.models.genai.llm import LLMDefinition\n",
    "from datarobot.models.genai.llm_blueprint import LLMBlueprint, VectorDatabaseSettings\n",
    "from datarobot.models.genai.playground import Playground\n",
    "from datarobot.models.genai.vector_database import (\n",
    "    ChunkingParameters,\n",
    "    CustomModelVectorDatabaseValidation,\n",
    "    VectorDatabase,\n",
    ")\n",
    "import datarobotx as drx\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d7b",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "Read more about different options for [connecting to DataRobot from the Python client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d7c",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dr.Client(token=\"PUT_YOUR_DR_API_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d7d",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Create and use DataRobot generative AI playgrounds\n",
    "\n",
    "In the following cells, you will create new or load existing use cases, playgrounds, and LLM blueprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d7e",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UseCase(id=6645f9391bdeb4610c5f198d, name=DataRobot GenAI Accelerator Use Case, description=None, models=0, projects=0, datasets=0, notebooks=0, applications=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GenAI works within a use case. Create a new use case.\n",
    "use_case = dr.UseCase.create(\"DataRobot GenAI Accelerator Use Case\")\n",
    "use_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6645f80c0bc6d56121831d7f",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UseCase(id=6645ef9015de77fa7c5f1b9a, name=DataRobot GenAI Accelerator Use Case, description=None, models=0, projects=0, datasets=0, notebooks=0, applications=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, load an existing use case instead\n",
    "use_case = dr.UseCase.get(\"6645ef9015de77fa7c5f1b9a\")\n",
    "use_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d80",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Playground(id=6645f94aff64531a95c4ce46, name=Datarobot Documentation Playground)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GenAI LLMs live within a playground. Create a new playground.\n",
    "playground = Playground.create(\n",
    "    name=\"Datarobot Documentation Playground\", use_case=use_case\n",
    ")\n",
    "playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6645f80c0bc6d56121831d81",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Playground(id=6646f44bdcf3d388f96a2779, name=Playground 2024-05-17 08:08:09)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, load an existing playground instead.\n",
    "playground = Playground.get(\"6646f44bdcf3d388f96a2779\")\n",
    "playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d82",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azure-openai-gpt-3.5-turbo', 'azure-openai-gpt-3.5-turbo-16k', 'azure-openai-gpt-4', 'azure-openai-gpt-4-32k', 'amazon-titan', 'anthropic-claude-2', 'google-bison']\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what kind of LLMs are available. By default this is returned as a dict\n",
    "# for easy readability. This includes a list of allowed settings for each LLM along with\n",
    "# any constraints on the values.\n",
    "llms_dict = LLMDefinition.list()\n",
    "print([llm[\"id\"] for llm in llms_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d83",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'system_prompt', 'name': 'System prompt', 'description': 'System prompt guides the style of the LLM response. It is a \"universal\" prompt, prepended to all individual prompts.', 'type': 'string', 'format': 'multiline', 'is_nullable': True, 'constraints': {'type': 'string', 'min_length': None, 'max_length': 500000, 'allowed_choices': None}, 'default_value': ''}\n",
       "{'id': 'max_completion_length', 'name': 'Max completion tokens', 'description': \"Maximum number of tokens allowed in the completion. The combined count of this value and prompt tokens must be below the model's maximum context size (4096), where prompt token count is comprised of system prompt, user prompt, recent chat history, and vector database citations.\", 'type': 'integer', 'format': None, 'is_nullable': True, 'constraints': {'type': 'integer', 'min_value': 1, 'max_value': 4096}, 'default_value': 1024}\n",
       "{'id': 'temperature', 'name': 'Temperature', 'description': 'Temperature controls the randomness of model output. Enter a value between 0 and 2, where higher values return more diverse output and lower values return more deterministic results. A value of 0 may return repetitive results.', 'type': 'float', 'format': None, 'is_nullable': True, 'constraints': {'type': 'float', 'min_value': 0.0, 'max_value': 2.0}, 'default_value': 1.0}\n",
       "{'id': 'top_p', 'name': 'Token selection probability cutoff (Top P)', 'description': 'Top P sets a threshold that controls the selection of words included in the response, based on a cumulative probability cutoff for token selection. For example, 0.2 considers only the top 20% probability mass. Higher numbers return more diverse options for outputs.', 'type': 'float', 'format': None, 'is_nullable': True, 'constraints': {'type': 'float', 'min_value': 0.0, 'max_value': 1.0}, 'default_value': 1.0}\n",
       "[None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPT 3.5 for the example\n",
    "llms = LLMDefinition.list(as_dict=False)\n",
    "gpt = llms[0]\n",
    "\n",
    "# To interact with GPT you need to create an LLM blueprint with the settings that you want.\n",
    "# Since the allowed settings depend on the LLM type you take a generic dict with those values\n",
    "# that will be validated during the creation of the blueprint. Review the allowed\n",
    "# settings for GPT here.\n",
    "[print(setting.to_dict()) for setting in gpt.settings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d84",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=6645f956dcf3d388f96a21f6, name=Overconfident DataRobot Engineer GPT 3.5, is_saved=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the LLM blueprint with custom settings defining it's \"personality\"\n",
    "llm_settings = {\n",
    "    \"system_prompt\": (\n",
    "        \"You are a DataRobot engineer and know everything about the datarobot platform. \"\n",
    "        \"If you don't know the answer just pretend to be confident and make up something that sounds reasonable.\"\n",
    "    ),\n",
    "    \"max_completion_length\": 1024,  # With decent completion length\n",
    "    \"temperature\": 1.0,  # With a high degree of variability\n",
    "}\n",
    "\n",
    "llm_blueprint = LLMBlueprint.create(\n",
    "    playground=playground,\n",
    "    name=\"Overconfident DataRobot Engineer GPT 3.5\",\n",
    "    llm=gpt,\n",
    "    llm_settings=llm_settings,\n",
    ")\n",
    "llm_blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d85",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "To deploy a custom model in DataRobot, you can follow these steps:\n",
       "1. Prepare your custom model: Develop your custom model using your preferred programming language or framework.\n",
       "2. Prepare your data: Ensure the data format is compatible with DataRobot and meets the necessary requirements, such as proper labeling.\n",
       "3. Create a DataRobot project: Log in to your DataRobot account and create a new project. Upload your dataset and configure the project settings.\n",
       "4. Train multiple models: Use DataRobot's automated machine learning capabilities to train multiple models on your dataset. This step helps you establish a baseline performance and compare it against your custom model.\n",
       "5. Assess model performance: Evaluate the performance of the models trained by DataRobot. Identify the best-performing model as a benchmark to compare against your custom model.\n",
       "6. Deploy custom model: Once you have established the benchmark performance, deploy your custom model using DataRobot's Deployment feature. Specify the necessary parameters and settings, such as input and output format, performance metrics, and resource allocation.\n",
       "7. Monitor and evaluate: After deploying your custom model, monitor its performance using DataRobot's monitoring capabilities. Assess its performance compared to the benchmark model and iterate on improvements as needed.\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now chat with the LLM blueprint\n",
    "prompt = ChatPrompt.create(\n",
    "    \"How to deploy a custom model in datarobot?\",\n",
    "    llm_blueprint,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "print(prompt.result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d86",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=6645f956dcf3d388f96a21f6, name=Overconfident DataRobot Engineer GPT 3.5, is_saved=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save that LLM blueprint so that you lock in the settings and can use it with\n",
    "# a ComparisonPrompt or create new blueprints form it.\n",
    "llm_blueprint = llm_blueprint.update(is_saved=True)\n",
    "llm_blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d87",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Create vector databases\n",
    "\n",
    "In the following cells you will create new or load existing datasets and vector databases. Then you will investigate the vector database assets.\n",
    "\n",
    "Note: The dataset used in this accelerator is a ZIP file that contains DataRobot's publicly available English documentation. The dataset does not necessarily reflect the most recent version of the documentation, however, as the content is regularly updated. This does not impact the usefulness of the dataset for the purposes of the accelerator. The date that the content was extracted is appended to the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d88",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='datarobot_english_documentation_5th_December.zip', id='6645f96a7701710b15f4cc18')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a vector database with DataRobot generative AI models. Upload a dataset with DataRobot documentation.\n",
    "dataset_url = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/datarobot_english_documentation_5th_December.zip\"\n",
    "dataset = dr.Dataset.create_from_url(dataset_url)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6645f80c0bc6d56121831d89",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optionally, load an existing dataset instead\n",
    "dataset = dr.Dataset.get(\"6645f96a7701710b15f4cc18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d8a",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase(id=6645f9a3ff64531a95c4ce4b, name=Vector database for datarobot_english_documentation_5th_December.zip, execution_status=NEW)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector database with a set of chunking parameters\n",
    "chunking_parameters = ChunkingParameters(\n",
    "    embedding_model=VectorDatabaseEmbeddingModel.JINA_EMBEDDING_T_EN_V1,\n",
    "    chunking_method=VectorDatabaseChunkingMethod.RECURSIVE,\n",
    "    chunk_size=256,\n",
    "    chunk_overlap_percentage=25,\n",
    "    separators=[\"#\", \"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "vdb = VectorDatabase.create(dataset.id, chunking_parameters, use_case)\n",
    "vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6645f80c0bc6d56121831d8b",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase(id=6645f9a3ff64531a95c4ce4b, name=Vector database for datarobot_english_documentation_5th_December.zip, execution_status=COMPLETED)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, load an existing dataset instead\n",
    "vdb = VectorDatabase.get(\"6645f9a3ff64531a95c4ce4b\")\n",
    "vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d8c",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase(id=6645f9a3ff64531a95c4ce4b, name=Vector database for datarobot_english_documentation_5th_December.zip, execution_status=COMPLETED)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the vector database and make sure it has completed\n",
    "assert vdb.execution_status == \"COMPLETED\"\n",
    "vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6645f80c0bc6d56121831d8d",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunks</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: Edit and use applications\\ndescrip...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0160643, -0.061523117, -0.0508623, 0.005448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#edit-and-use-applications }\\n\\nWorkbench appl...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.035402834, -0.0073612314, 0.0076815276, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>See the image and table below for a brief desc...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.036530703, -0.08172059, 4.838106e-05, 0.133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#application-folders).\\n     ![](images/icon-w...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.07840027, -0.034945183, -0.020708812, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>## Change themes {: #change-themes }\\n\\nIn the...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0402755, -0.091552384, 0.04012868, 0.086053...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_chunks  \\\n",
       "0  ---\\ntitle: Edit and use applications\\ndescrip...   \n",
       "1  #edit-and-use-applications }\\n\\nWorkbench appl...   \n",
       "2  See the image and table below for a brief desc...   \n",
       "3  #application-folders).\\n     ![](images/icon-w...   \n",
       "4  ## Change themes {: #change-themes }\\n\\nIn the...   \n",
       "\n",
       "                                              source  page  \\\n",
       "0  datarobot_english_documentation/datarobot_docs...     0   \n",
       "1  datarobot_english_documentation/datarobot_docs...     0   \n",
       "2  datarobot_english_documentation/datarobot_docs...     0   \n",
       "3  datarobot_english_documentation/datarobot_docs...     0   \n",
       "4  datarobot_english_documentation/datarobot_docs...     0   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.0160643, -0.061523117, -0.0508623, 0.005448...  \n",
       "1  [0.035402834, -0.0073612314, 0.0076815276, 0.0...  \n",
       "2  [0.036530703, -0.08172059, 4.838106e-05, 0.133...  \n",
       "3  [0.07840027, -0.034945183, -0.020708812, 0.113...  \n",
       "4  [0.0402755, -0.091552384, 0.04012868, 0.086053...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the created vector database assets\n",
    "# and manually investigate whether the resulting chunks are meaningful\n",
    "vdb_assets_file_name = \"chunks_and_embeddings.parquet.gzip\"\n",
    "vdb.download_text_and_embeddings_asset(vdb_assets_file_name)\n",
    "vdb_assets_df = pd.read_parquet(vdb_assets_file_name)\n",
    "vdb_assets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d8e",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Create LLM-Blueprints with vector databases (RAG)\n",
    "- Copy and modify existing LLM-Blueprints\n",
    "- Add vector databases to LLM-Blueprints\n",
    "- Chat prompt LLM-Blueprints with vector databases (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d8f",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=6645fb33ff64531a95c4ce53, name=Copied LLM-Blueprint, is_saved=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new LLM blueprint from the existing one so that you can change some\n",
    "# settings and add a vector database.\n",
    "new_llm_blueprint = LLMBlueprint.create_from_llm_blueprint(\n",
    "    llm_blueprint, name=\"Copied LLM-Blueprint\"\n",
    ")\n",
    "new_llm_blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d90",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=6645fb33ff64531a95c4ce53, name=GPT 3.5 + DatRobot Documentation VD, is_saved=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give the LLM a new personality\n",
    "llm_settings[\"system_prompt\"] = (\n",
    "    \"Try to answer the question only with the given context. \"\n",
    "    \"If you don't know the answer don't make it up but instead reply with: 'I don't know'\"\n",
    ")\n",
    "llm_settings[\"temperature\"] = 0.0  # With the lowest degree of variability\n",
    "\n",
    "# Add a vector database and define settings that could be different from the defaults\n",
    "# when the vdb was created.\n",
    "vector_database_settings = VectorDatabaseSettings(\n",
    "    max_documents_retrieved_per_prompt=5,\n",
    "    max_tokens=2560,\n",
    ")\n",
    "\n",
    "# Update the LLM-blueprint with the new settings including the vector database\n",
    "llm_blueprint_with_vdb = new_llm_blueprint.update(\n",
    "    name=\"GPT 3.5 + DatRobot Documentation VD\",\n",
    "    llm_settings=llm_settings,\n",
    "    vector_database=vdb,\n",
    "    # vector_database_settings=vector_database_settings,\n",
    "    is_saved=True,\n",
    ")\n",
    "\n",
    "llm_blueprint_with_vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d91",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=6645fb33ff64531a95c4ce53, name=GPT 3.5 + DatRobot Documentation VD, is_saved=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally load an existing LLMBlueprint instead\n",
    "llm_blueprint_with_vdb = LLMBlueprint.get(\"6645fb33ff64531a95c4ce53\")\n",
    "llm_blueprint_with_vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d92",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "To deploy a custom model in DataRobot, you can follow the workflow outlined below:\n",
       "1. Prepare your custom model by providing the model content, which includes the compiled artifact, source code, and additional supporting files related to the model.\n",
       "2. Define the model environment, which can be either a drop-in or custom environment. Drop-in environments are provided by DataRobot, while custom environments are required for specialized models and use cases.\n",
       "3. Test your custom model in the Custom Model Workshop.\n",
       "4. Register your custom model in the Model Registry.\n",
       "5. Deploy the custom model to a centralized deployment hub, where you can monitor, manage, and govern it alongside your deployed DataRobot models.\n",
       "Note: DataRobot supports custom models built in various programming languages, including Python, R, and Java.\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat with the new LLM blueprint\n",
    "prompt_with_vdb = ChatPrompt.create(\n",
    "    \"How to deploy a custom model in datarobot?\",\n",
    "    llm_blueprint_with_vdb,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "print(prompt_with_vdb.result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d93",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#deploy-a-custom-model-in-a-datarobot-environment }\n",
       "Custom inference models allow you to bring your pre-trained models into DataRobot. To deploy a custom model to a DataRobot prediction environment, you can create a custom model in the Custom Model Workshop. Then, you can prepare, test, and register that model, and deploy it to a centralized deployment hub where you can monitor, manage, and govern it alongside your deployed DataRobot models. DataRobot supports custom models built in various programming languages, including Python, R, and Java.\n",
       "To create and deploy a custom model in DataRobot, follow the workflow outlined below:\n",
       "# Prepare custom models for deployment\n",
       "Custom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you've created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\n",
       "* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\n",
       "* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\n",
       "#deploy-a-custom-model-in-a-portable-prediction-server }\n",
       "Custom inference models allow you to bring your pre-trained models into DataRobot through the Custom Model Workshop. DataRobot supports custom models built in various programming languages, including Python, R, and Java. Once you've created a custom model in DataRobot, you can deploy it to a containerized DataRobot prediction environment called a Portable Prediction Server (PPS). To deploy a custom model to a PPS, you can prepare and test it in the Custom Model Workshop, and then add it to the Model Registry. You can then deploy the custom model using a PPS bundle, which includes everything you need to deploy the model externally while monitoring it alongside models deployed within DataRobot.\n",
       "To create and deploy a custom model in a PPS, follow the workflow outlined below:\n",
       "---\n",
       "title: Custom model in a DataRobot environment\n",
       "description: How to deploy a custom model in a DataRobot prediction environment.\n",
       "---\n",
       "# Deploy a custom model in a DataRobot Environment {:\n",
       "#custom-model-assembly }\n",
       "While DataRobot provides hundreds of built-in models, there are situations where you need preprocessing or modeling methods that are not currently supported out of the box. To create a custom inference model, you must provide a serialized model artifact with a file extension corresponding to the chosen environment language and any additional custom code required to use the model.\n",
       "Before adding custom models and environments to DataRobot, you must prepare and structure the files required to run them successfully. The tools and templates necessary to prepare custom models are hosted in the <a target=\"_blank\" href=\"https://github.com/datarobot/datarobot-user-models\">Custom Model GitHub repository</a>. ({% include 'includes/github-sign-in.md' %}) DataRobot recommends understanding the following requirements to prepare your custom model for upload to the Custom Model Workshop.\n",
       "Running this script returns a deployment ID and initial model ID that can be used to instrument your deployment. Alternatively, create the deployment from the DataRobot GUI.\n",
       "![](images/int_disamwma_large_024.png)\n",
       "To create a deployment from the DataRobot GUI, use the the following steps:\n",
       "1. Log in to the DataRobot GUI.\n",
       "2. Select **Model Registry** (1) and click **Add New Package** (2).\n",
       "3. In the dropdown, select **New external model package** (3).\n",
       "    ![](images/int_disamwma_large_017.png)\n",
       "4. Complete all the information needed for your deployment, and then click **Create package.**\n",
       "    ![](images/int_disamwma_large_009.png)\n",
       "5. Select the **Deployments** tab and click **Deploy Model Package**, validate the details on this page, and click **Create deployment** (right-hand side at the top of the page).\n",
       "6. You can use the toggle buttons to enable drift tracking, segment analysis of predictions, and more deployment settings.\n",
       "    ![](images/int_disamwma_large_002.png)\n",
       "---\n",
       "title: Start modeling\n",
       "description: Provides a quick overview of modeling and deploying models with DataRobot.\n",
       "---\n",
       "# Start modeling  {: #start-modeling }\n",
       "To build models in DataRobot, you first create a project by importing a dataset, selecting a target feature, and clicking **Start** to begin the modeling process. A DataRobot project contains all of the models built with the imported dataset. The following steps provide a quick overview of how to begin modeling data with DataRobot. Links within the steps point to the full documentation if you need assistance.\n",
       "## 1: Create a new DataRobot project {: #create-a-new-datarobot-project }\n",
       "Importing a dataset using any one of the methods on the new project page to [create a new DataRobot project](import-to-dr):\n",
       "![](images/new-project.png)\n",
       "You can see [the file type reference](file-types) for information about file size limitations.\n",
       "## 2: Configure modeling settings {:\n",
       "To add a DataRobot model as a registered model or version:\n",
       "1. On the **Leaderboard**, select the model to use for generating predictions. DataRobot recommends a model with the **Recommended for Deployment** and **Prepared for Deployment** badges. The [model preparation](model-rec-process) process runs Feature Impact, retrains the model on a reduced feature list, and trains on a higher sample size, followed by the entire sample (latest data for date/time partitioned projects).\n",
       "    ![](images/prepared-for-deployment.png)\n",
       "    !!! important\n",
       "        The **Deploy** tab behaves differently in environments without a dedicated prediction server, as described in the section on [shared modeling workers](deploy-model#use-shared-modeling-workers).\n",
       "2. Click **Predict > Deploy**. If the Leaderboard model doesn't have the **Prepare for Deployment** badge, DataRobot recommends you click **Prepare for Deployment** to run the [model preparation](model-rec-process\n",
       "#versioning-support-in-the-model-registry }\n",
       "Now generally available for [app.eu.datarobot.com](https://app.eu.datarobot.com/) users, the new Model Registry is an organizational hub for the variety of models used in DataRobot. Models are registered as deployment-ready model packages. These model packages are grouped into _registered models_ containing _registered model versions_, allowing you to categorize them based on the business problem they solve. Registered models can contain DataRobot, custom, external, challenger, and automatically retrained models as versions.\n",
       "During this update, packages from the **Model Registry > Model Packages** tab are converted to registered models and migrated to the new **Registered Models** tab. Each migrated registered model contains a registered model version, and the original packages can be identified in the new tab by the model package ID appended to the registered model name.\n",
       "---\n",
       "title: Deploy DataRobot models\n",
       "description: How to create new deployments from DataRobot AutoML models.\n",
       "---\n",
       "# Deploy DataRobot models {: #deploy-datarobot-models }\n",
       "You can register and deploy models you build with DataRobot AutoML using the Model Registry. In most cases, before deployment, you should unlock holdout and [retrain your model](creating-addl-models#retrain-a-model) at 100% to improve predictive accuracy. Additionally, DataRobot automatically runs [**Feature Impact**](feature-impact) for the model (this also calculates **Prediction Explanations**, if available).\n",
       "## Register and deploy a model {: #register-and-deploy-a-model }\n",
       "{% include 'includes/deploy-leaderboard.md' %}\n",
       "## Deploy a registered model {:\n",
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that the LLM is using information from the vector database.\n",
    "[print(citation.text) for citation in prompt_with_vdb.citations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d94",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Create external vector databases\n",
    "- Load an split text datasets into documents\n",
    "- Load embedding models and generate embeddings from documents\n",
    "- Create FAISS indexes from embeddings\n",
    "- Do similarity search on indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6645f80c0bc6d56121831d95",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>document_file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: Edit and use applications\\ndescrip...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: User Activity Monitor reference\\nd...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHAP Prediction Explanations estimate how much...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: Prepare custom models\\ndescription...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---\\ntitle: Prediction Distribution graph\\ndes...</td>\n",
       "      <td>datarobot_english_documentation/datarobot_docs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  ---\\ntitle: Edit and use applications\\ndescrip...   \n",
       "1  ---\\ntitle: User Activity Monitor reference\\nd...   \n",
       "2  SHAP Prediction Explanations estimate how much...   \n",
       "3  ---\\ntitle: Prepare custom models\\ndescription...   \n",
       "4  ---\\ntitle: Prediction Distribution graph\\ndes...   \n",
       "\n",
       "                                  document_file_path  \n",
       "0  datarobot_english_documentation/datarobot_docs...  \n",
       "1  datarobot_english_documentation/datarobot_docs...  \n",
       "2  datarobot_english_documentation/datarobot_docs...  \n",
       "3  datarobot_english_documentation/datarobot_docs...  \n",
       "4  datarobot_english_documentation/datarobot_docs...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an external vector database using the same dataset\n",
    "df = dataset.get_as_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6645f80c0bc6d56121831d96",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted docs: 5546\n"
     ]
    }
   ],
   "source": [
    "# Read the dataframe into langchain documents and split them using an appropriate splitter\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=row[\"document\"], metadata={\"source\": row[\"document_file_path\"]}\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "splitter = MarkdownTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=1000,\n",
    ")\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "\n",
    "texts = [doc.page_content for doc in splitted_docs]\n",
    "metadatas = [doc.metadata for doc in splitted_docs]\n",
    "print(f\"Splitted docs: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6645f80c0bc6d56121831d97",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS VectorDB has 5546 documents\n"
     ]
    }
   ],
   "source": [
    "# Build a FAISS index using huggingface embeddings\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    cache_folder=\"storage/deploy/sentencetransformers\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Load existing db from disk if previously built\n",
    "    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\n",
    "except:\n",
    "    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n",
    "    db.save_local(folder_path=\"storage/deploy/faiss-db\")\n",
    "\n",
    "print(f\"FAISS VectorDB has {db.index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6645f80c0bc6d56121831d98",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='1. Define and test model content locally (i.e., on your computer).\\n\\n2. Optionally, create a container environment where the model will run.\\n\\n3. Upload the model content and environment (if applicable) into DataRobot.\\n\\n## Model content {: #model-content }\\n\\nTo define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\\n\\n!!! tip\\n    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.\\n\\nFile | Description | Required\\n-----|-------------|---------\\nModel artifact file<br>_or_<br>`custom.py`/`custom.R` file | Provide a model artifact and/or a custom code file. <ul><li>Model artifact: a serialized model artifact with a file extension corresponding to the chosen environment language.</li><li>Custom code: custom capabilities implemented with hooks (or functions) that enable DataRobot to run the code and integrate it with other capabilities. | Yes\\n`model-metadata.yaml` | A file describing model\\'s metadata, including input/output data requirements. You can supply a schema that can then be used to validate the model when building and training a blueprint. A schema lets you specify whether a custom model supports or outputs: <ul><li>Certain data types</li><li>Missing values</li><li>Sparse data</li><li>A certain number of columns</li> | Required when a custom model outputs non-numeric data. If not provided, a default schema is used.\\n`requirements.txt` | A list of Python or R packages to add to the base environment. This list pre-installs Python or R packages that the custom model is using but are not a part of the base environment | No\\nAdditional files | Other files used by the model (for example, a file that defines helper functions used inside `custom.py`). | No\\n\\n=== \"requirements.txt Python example\"', metadata={'source': 'datarobot_english_documentation/datarobot_docs|en|mlops|deployment|custom-models|custom-model-assembly|custom-model-components.txt'}), Document(page_content='```\\n    Configure the following fields:\\n\\n    * `user_provided_model_id`: Provide any descriptive and unique string value. DataRobot recommends following a naming pattern, such as `<user>/<model-unique-id>`.\\n\\n        !!! note\\n            By default, this ID will reside in a unique namespace, the GitHub repository ID. Alternatively, you can configure the namespace as an input argument to the custom models action.\\n\\n    * `target_type`: Provide the correct target type for your custom model.\\n\\n    * `target_name`: Provide the correct target name for your custom model.\\n\\n    * `model_environment_id`: Provide the DataRobot execution environment required for your custom model. You can find these environments in the DataRobot application under [**Model Registry** > **Custom Model Workshop** > **Environments**](custom-environments).\\n\\n        ![](images/pp-cus-model-github3.png)\\n\\n4. In any directory in your repository, add a deployment definition YAML file (with any filename) containing the following YAML:\\n\\n    ```yaml\\n    user_provided_deployment_id: user/my-awesome-deployment-id\\n    user_provided_model_id: user/model-unique-id-1', metadata={'source': 'datarobot_english_documentation/datarobot_docs|en|mlops|deployment|custom-models|custom-model-assembly|custom-model-github-action.txt'}), Document(page_content=\"---\\ntitle: Prepare custom models\\ndescription: Prepare to create deployments from custom models\\n\\n---\\n\\n# Prepare custom models for deployment\\n\\nCustom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you've created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\\n\\n* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\\n\\n* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\\n\\n!!! note\\n    Custom inference models are _not_ custom DataRobot models. They are _user-defined_ models created outside of DataRobot and assembled in the Custom Model Workshop for deployment, monitoring, and governance.\\n\\nSee the associated [feature considerations](#feature-considerations) for additional information.\\n\\n## Custom Model Workshop\", metadata={'source': 'datarobot_english_documentation/datarobot_docs|en|mlops|deployment|custom-models|index.txt'}), Document(page_content='* The [model environment](#model-environment) is defined using a Docker file and additional files that will allow DataRobot to build an image where the model will run. There are a variety of built-in environments; you only need to build your own environment when you need to install Linux packages. For more detailed information, see the section on [custom model environments](custom-model-environments/index).\\n\\nAt a high level, the steps to define a custom model with these components include:\\n\\n1. Define and test model content locally (i.e., on your computer).\\n\\n2. Optionally, create a container environment where the model will run.\\n\\n3. Upload the model content and environment (if applicable) into DataRobot.\\n\\n## Model content {: #model-content }\\n\\nTo define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\\n\\n!!! tip\\n    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.', metadata={'source': 'datarobot_english_documentation/datarobot_docs|en|mlops|deployment|custom-models|custom-model-assembly|custom-model-components.txt'})]\n"
     ]
    }
   ],
   "source": [
    "# Test vector database retrieval / similarity search\n",
    "retrieved_documents = db.similarity_search(\"How to deploy a custom model in datarobot?\")\n",
    "print(retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831d99",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Deploy external vector databases\n",
    "- Define custom model hooks for external vector databases\n",
    "- Create and deploy external vector databases as custom models\n",
    "- Make predictions on deployed external vector databases\n",
    "- Validate and register external vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d9a",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define hooks for deploying an unstructured custom model\n",
    "\n",
    "\n",
    "def load_model(input_dir):\n",
    "    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n",
    "    import os\n",
    "\n",
    "    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "    from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "    embedding_function = SentenceTransformerEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        cache_folder=f\"{input_dir}/storage/deploy/sentencetransformers\",\n",
    "    )\n",
    "    db = FAISS.load_local(f\"{input_dir}/storage/deploy/faiss-db\", embedding_function)\n",
    "    return db\n",
    "\n",
    "\n",
    "def score_unstructured(model, data, query, **kwargs) -> str:\n",
    "    \"\"\"Custom model hook for making completions with our vector store.\"\"\"\n",
    "    import json\n",
    "\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "    from langchain.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "    try:\n",
    "        db = model\n",
    "        data_dict = json.loads(data)\n",
    "        retriever = VectorStoreRetriever(vectorstore=db)\n",
    "        documents = retriever.get_relevant_documents(data_dict[\"question\"])\n",
    "        relevant_text_list = [doc.page_content for doc in documents]\n",
    "        rv = {\"relevant\": relevant_text_list}\n",
    "    except Exception as e:\n",
    "        rv = {\"error\": f\"{e.__class__.__name__}: {str(e)}\"}\n",
    "    return json.dumps(rv), {\"mimetype\": \"application/json\", \"charset\": \"utf8\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d9b",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('{\"relevant\": [\"1. Define and test model content locally (i.e., on your computer).\\\\n\\\\n2. Optionally, create a container environment where the model will run.\\\\n\\\\n3. Upload the model content and environment (if applicable) into DataRobot.\\\\n\\\\n## Model content {: #model-content }\\\\n\\\\nTo define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\\\\n\\\\n!!! tip\\\\n    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.\\\\n\\\\nFile | Description | Required\\\\n-----|-------------|---------\\\\nModel artifact file<br>_or_<br>`custom.py`/`custom.R` file | Provide a model artifact and/or a custom code file. <ul><li>Model artifact: a serialized model artifact with a file extension corresponding to the chosen environment language.</li><li>Custom code: custom capabilities implemented with hooks (or functions) that enable DataRobot to run the code and integrate it with other capabilities. | Yes\\\\n`model-metadata.yaml` | A file describing model\\'s metadata, including input/output data requirements. You can supply a schema that can then be used to validate the model when building and training a blueprint. A schema lets you specify whether a custom model supports or outputs: <ul><li>Certain data types</li><li>Missing values</li><li>Sparse data</li><li>A certain number of columns</li> | Required when a custom model outputs non-numeric data. If not provided, a default schema is used.\\\\n`requirements.txt` | A list of Python or R packages to add to the base environment. This list pre-installs Python or R packages that the custom model is using but are not a part of the base environment | No\\\\nAdditional files | Other files used by the model (for example, a file that defines helper functions used inside `custom.py`). | No\\\\n\\\\n=== \\\\\"requirements.txt Python example\\\\\"\", \"```\\\\n    Configure the following fields:\\\\n\\\\n    * `user_provided_model_id`: Provide any descriptive and unique string value. DataRobot recommends following a naming pattern, such as `<user>/<model-unique-id>`.\\\\n\\\\n        !!! note\\\\n            By default, this ID will reside in a unique namespace, the GitHub repository ID. Alternatively, you can configure the namespace as an input argument to the custom models action.\\\\n\\\\n    * `target_type`: Provide the correct target type for your custom model.\\\\n\\\\n    * `target_name`: Provide the correct target name for your custom model.\\\\n\\\\n    * `model_environment_id`: Provide the DataRobot execution environment required for your custom model. You can find these environments in the DataRobot application under [**Model Registry** > **Custom Model Workshop** > **Environments**](custom-environments).\\\\n\\\\n        ![](images/pp-cus-model-github3.png)\\\\n\\\\n4. In any directory in your repository, add a deployment definition YAML file (with any filename) containing the following YAML:\\\\n\\\\n    ```yaml\\\\n    user_provided_deployment_id: user/my-awesome-deployment-id\\\\n    user_provided_model_id: user/model-unique-id-1\", \"---\\\\ntitle: Prepare custom models\\\\ndescription: Prepare to create deployments from custom models\\\\n\\\\n---\\\\n\\\\n# Prepare custom models for deployment\\\\n\\\\nCustom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you\\'ve created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\\\\n\\\\n* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\\\\n\\\\n* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\\\\n\\\\n!!! note\\\\n    Custom inference models are _not_ custom DataRobot models. They are _user-defined_ models created outside of DataRobot and assembled in the Custom Model Workshop for deployment, monitoring, and governance.\\\\n\\\\nSee the associated [feature considerations](#feature-considerations) for additional information.\\\\n\\\\n## Custom Model Workshop\", \"* The [model environment](#model-environment) is defined using a Docker file and additional files that will allow DataRobot to build an image where the model will run. There are a variety of built-in environments; you only need to build your own environment when you need to install Linux packages. For more detailed information, see the section on [custom model environments](custom-model-environments/index).\\\\n\\\\nAt a high level, the steps to define a custom model with these components include:\\\\n\\\\n1. Define and test model content locally (i.e., on your computer).\\\\n\\\\n2. Optionally, create a container environment where the model will run.\\\\n\\\\n3. Upload the model content and environment (if applicable) into DataRobot.\\\\n\\\\n## Model content {: #model-content }\\\\n\\\\nTo define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\\\\n\\\\n!!! tip\\\\n    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.\"]}',\n",
       " {'mimetype': 'application/json', 'charset': 'utf8'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the hooks locally\n",
    "score_unstructured(\n",
    "    load_model(\".\"),\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"question\": \"How to deploy a custom model in datarobot?\",\n",
    "        }\n",
    "    ),\n",
    "    None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d9c",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Deploying custom model\u001b[0m\n",
       "\u001b[1m  - \u001b[0mUnable to auto-detect model type; any provided paths and files will be\n",
       "    exported - dependencies should be explicitly specified using\n",
       "    `extra_requirements` or `environment_id`\n",
       "\u001b[1m  - \u001b[0mPreparing model and environment...\n",
       "\u001b[1m  - \u001b[0mUsing environment [[DataRobot] Python 3.11 GenAI\n",
       "    v9](https://app.datarobot.com/model-registry/custom-environments/64d2ba178dd3f0b1fa2162f0)\n",
       "    for deployment\n",
       "\u001b[1m  - \u001b[0mConfiguring and uploading custom model...\n",
       "\r    100%|| 201M/201M [00:01<00:00, 156MB/s]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mRegistered custom model [External DR Documentation Vector Database\n",
       "    Deployment](https://app.datarobot.com/model-registry/custom-models/66460174267e4c2ca8f82ebd/info)\n",
       "    with target type: Unstructured\n",
       "\u001b[1m  - \u001b[0mInstalling additional dependencies...\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mCreating and deploying model package...\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mCreated deployment [External DR Documentation Vector Database\n",
       "    Deployment](https://app.datarobot.com/deployments/664602284569e66a83a14196/overview)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Custom model deployment complete\u001b[0m\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's deploy the external vector database\n",
    "external_vdb_deployment = drx.deploy(\n",
    "    \"storage/deploy/\",\n",
    "    name=\"External DR Documentation Vector Database Deployment\",\n",
    "    hooks={\"score_unstructured\": score_unstructured, \"load_model\": load_model},\n",
    "    extra_requirements=[\n",
    "        \"langchain==0.0.333\",\n",
    "        \"faiss-cpu==1.7.4\",\n",
    "        \"sentence-transformers==2.2.2\",\n",
    "    ],\n",
    "    # environment_id=\"64c964448dd3f0c07f47d040\", # Python 3.9 for app.datarobot.com\n",
    "    environment_id=\"64d2ba178dd3f0b1fa2162f0\",  # Python 3.11 for app.datarobot.com\n",
    ")\n",
    "# Enable storing prediction data, necessary for data export for monitoring purposes\n",
    "external_vdb_deployment.dr_deployment.update_predictions_data_collection_settings(\n",
    "    enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d9d",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optionally load an already created external vector database deployment\n",
    "external_vdb_deployment = drx.Deployment(\"664602284569e66a83a14196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d9e",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment [External DR Documentation Vector\n",
       "    Database\n",
       "    Deployment](https://app.datarobot.com/deployments/664602284569e66a83a14196/overview)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "{'relevant': ['1. Define and test model content locally (i.e., on your computer).\\n\\n2. Optionally, create a container environment where the model will run.\\n\\n3. Upload the model content and environment (if applicable) into DataRobot.\\n\\n## Model content {: #model-content }\\n\\nTo define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\\n\\n!!! tip\\n    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.\\n\\nFile | Description | Required\\n-----|-------------|---------\\nModel artifact file<br>_or_<br>`custom.py`/`custom.R` file | Provide a model artifact and/or a custom code file. <ul><li>Model artifact: a serialized model artifact with a file extension corresponding to the chosen environment language.</li><li>Custom code: custom capabilities implemented with hooks (or functions) that enable DataRobot to run the code and integrate it with other capabilities. | Yes\\n`model-metadata.yaml` | A file describing model\\'s metadata, including input/output data requirements. You can supply a schema that can then be used to validate the model when building and training a blueprint. A schema lets you specify whether a custom model supports or outputs: <ul><li>Certain data types</li><li>Missing values</li><li>Sparse data</li><li>A certain number of columns</li> | Required when a custom model outputs non-numeric data. If not provided, a default schema is used.\\n`requirements.txt` | A list of Python or R packages to add to the base environment. This list pre-installs Python or R packages that the custom model is using but are not a part of the base environment | No\\nAdditional files | Other files used by the model (for example, a file that defines helper functions used inside `custom.py`). | No\\n\\n=== \"requirements.txt Python example\"', '```\\n    Configure the following fields:\\n\\n    * `user_provided_model_id`: Provide any descriptive and unique string value. DataRobot recommends following a naming pattern, such as `<user>/<model-unique-id>`.\\n\\n        !!! note\\n            By default, this ID will reside in a unique namespace, the GitHub repository ID. Alternatively, you can configure the namespace as an input argument to the custom models action.\\n\\n    * `target_type`: Provide the correct target type for your custom model.\\n\\n    * `target_name`: Provide the correct target name for your custom model.\\n\\n    * `model_environment_id`: Provide the DataRobot execution environment required for your custom model. You can find these environments in the DataRobot application under [**Model Registry** > **Custom Model Workshop** > **Environments**](custom-environments).\\n\\n        ![](images/pp-cus-model-github3.png)\\n\\n4. In any directory in your repository, add a deployment definition YAML file (with any filename) containing the following YAML:\\n\\n    ```yaml\\n    user_provided_deployment_id: user/my-awesome-deployment-id\\n    user_provided_model_id: user/model-unique-id-1', \"---\\ntitle: Prepare custom models\\ndescription: Prepare to create deployments from custom models\\n\\n---\\n\\n# Prepare custom models for deployment\\n\\nCustom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you've created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\\n\\n* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\\n\\n* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\\n\\n!!! note\\n    Custom inference models are _not_ custom DataRobot models. They are _user-defined_ models created outside of DataRobot and assembled in the Custom Model Workshop for deployment, monitoring, and governance.\\n\\nSee the associated [feature considerations](#feature-considerations) for additional information.\\n\\n## Custom Model Workshop\", '* The [model environment](#model-environment) is defined using a Docker file and additional files that will allow DataRobot to build an image where the model will run. There are a variety of built-in environments; you only need to build your own environment when you need to install Linux packages. For more detailed information, see the section on [custom model environments](custom-model-environments/index).\\n\\nAt a high level, the steps to define a custom model with these components include:\\n\\n1. Define and test model content locally (i.e., on your computer).\\n\\n2. Optionally, create a container environment where the model will run.\\n\\n3. Upload the model content and environment (if applicable) into DataRobot.\\n\\n## Model content {: #model-content }\\n\\nTo define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\\n\\n!!! tip\\n    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the deployment\n",
    "external_vdb_deployment.predict_unstructured(\n",
    "    {\n",
    "        \"question\": \"How to deploy a custom model in datarobot?\",\n",
    "        \"source_documents\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831d9f",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModelVectorDatabaseValidation(id=66460366dcf3d388f96a221a)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a custom model vector database validation\n",
    "# which is necessary to validate if an external vector database can be registered\n",
    "external_vdb_validation = CustomModelVectorDatabaseValidation.create(\n",
    "    prompt_column_name=\"question\",\n",
    "    target_column_name=\"relevant\",\n",
    "    deployment_id=external_vdb_deployment.dr_deployment.id,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "external_vdb_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831da0",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase(id=66460376dcf3d388f96a221b, name=External DR Documentation Vector Database, execution_status=COMPLETED)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector database from the validated custom model\n",
    "external_vdb = VectorDatabase.create_from_custom_model(\n",
    "    use_case=use_case,\n",
    "    name=\"External DR Documentation Vector Database\",\n",
    "    validation_id=external_vdb_validation.id,\n",
    ")\n",
    "external_vdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831da1",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Deploy external LLMs\n",
    "- Add Cohere API token to the notebook\n",
    "- Define custom model hooks for Cohere API as external LLM\n",
    "- Create and deploy cohere LLM API as custom models\n",
    "- Make predictions on external deployed LLMs\n",
    "- Validate and register external LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831da2",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cohere credentials already added to DataRobot\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the cohere credentials in DataRobot\n",
    "# You can get your free cohere api token here: https://dashboard.cohere.com/api-keys\n",
    "cohere_api_token = \"YOUR_COHERE_API_TOKEN\"\n",
    "client = dr.client.get_client()\n",
    "try:\n",
    "    res = client.post(\n",
    "        \"credentials\",\n",
    "        json={\n",
    "            \"name\": \"cohere_api_token\",\n",
    "            \"description\": \"Added from notebook\",\n",
    "            \"credentialType\": \"api_token\",\n",
    "            \"apiToken\": cohere_api_token,\n",
    "        },\n",
    "    )\n",
    "    print(\"Cohere credentials added to DataRobot\")\n",
    "except Exception:\n",
    "    print(\"Cohere credentials already added to DataRobot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6645f80c0bc6d56121831da3",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'66549628a471c96799883414'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define hooks for deploying an unstructured custom model\n",
    "\n",
    "\n",
    "def load_model(*args, **kwargs):\n",
    "    import cohere\n",
    "    from datarobot_drum import RuntimeParameters\n",
    "\n",
    "    cohere_api_key = RuntimeParameters.get(\"cohere_api_token\")[\"apiToken\"]\n",
    "    return cohere.Client(cohere_api_key)\n",
    "\n",
    "\n",
    "def score(data, model, **kwargs):\n",
    "    import pandas as pd\n",
    "\n",
    "    prompts = data[\"promptText\"].tolist()\n",
    "    responses = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = model.generate(prompt)\n",
    "        responses.append(response[0].text)\n",
    "\n",
    "    return pd.DataFrame({\"responseText\": responses})\n",
    "\n",
    "\n",
    "# Deploy the external LLM as a custom model\n",
    "external_llm_deployment = drx.deploy(\n",
    "    model=None,\n",
    "    name=\"External Cohere LLM Deployment\",\n",
    "    target_type=\"TextGeneration\",\n",
    "    target=\"responseText\",\n",
    "    hooks={\n",
    "        \"load_model\": load_model,\n",
    "        \"score\": score,\n",
    "    },\n",
    "    runtime_parameters=[\"cohere_api_token\"],\n",
    "    extra_requirements=[\"cohere==4.27\", \"datarobot-drum==1.10.10\"],\n",
    "    # environment_id=\"64c964448dd3f0c07f47d040\", # Python 3.9 for app.datarobot.com\n",
    "    environment_id=\"64d2ba178dd3f0b1fa2162f0\",  # Python 3.11 for app.datarobot.com\n",
    ")\n",
    "# Enable storing prediction data, necessary for data export for monitoring purposes\n",
    "external_llm_deployment.dr_deployment.update_predictions_data_collection_settings(\n",
    "    enabled=True\n",
    ")\n",
    "external_llm_deployment.dr_deployment.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6645f80c0bc6d56121831da4",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load an already created external LLM deployment\n",
    "external_llm_deployment = drx.Deployment(\"66549628a471c96799883414\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6645f80c0bc6d56121831da5",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To deploy a custom model in Datarobot, you can follow these generalized steps:\n",
      "\n",
      "1. Train your model: Start by training your model using appropriate training data. The specific steps and tools for training depend on the type of model (e.g., machine learning, deep learning, etc.) and the framework or library being used.\n",
      "\n",
      "2. Package your model and dependencies: Prepare your trained model and the necessary code and dependencies to run the model in a standalone environment. This may involve creating a container or package that encapsulates your model and its requirements.\n",
      "\n",
      "3. Upload your model to Datarobot: Utilize the Datarobot platform features and tools to upload and manage your custom model. This often involves creating a new model asset in the Datarobot environment and providing the necessary files and metadata.\n",
      "\n",
      "4. Create a new experiment: Define a new experiment that incorporates your custom model. Link your dataset to the experiment and configure any required preprocessing steps or evaluation metrics.\n",
      "\n",
      "5. Test and validate: Utilize experimentation and validation tools within Datarobot to assess the performance of your custom model. This may involve splitting the data into training and test sets, evaluating metrics like accuracy, precision, or recall, and refining your model as necessary.\n",
      "\n",
      "6. Deploy the model: Upon satisfactory performance, deploy your custom model as a new model endpoint within Datarobot. This makes the model available for real-time predictions against new data.\n",
      "\n",
      "7. Monitor and improve: Continuously monitor the performance of your deployed model and iterate on the model improvement cycle as new data becomes available or when performance degradation is observed.\n",
      "\n",
      "It's important to note that the specifics of these steps may vary depending on the version of Datarobot you are using and the specific details of your custom model implementation. It is recommended to consult the official Datarobot documentation and resources for detailed instructions tailored to your specific use case and version of the platform. \n"
     ]
    }
   ],
   "source": [
    "# Do a test prediction with the external LLM deployment\n",
    "prompt = \"How to deploy a custom model in datarobot?\"\n",
    "\n",
    "prediction = external_llm_deployment.predict(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"context\": [\"\"],\n",
    "            \"promptText\": [prompt],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(prediction[\"prediction\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6645f80c0bc6d56121831da6",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModelLLMValidation(id=665498444ca568110e7e3a7c)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a custom model LLM from a deployment. Developing the deployment is outside\n",
    "# the scope of this example. This example uses a previously deployed text generation model.\n",
    "# Validate it for use as an LLM.\n",
    "custom_model_llm_validation = CustomModelLLMValidation.create(\n",
    "    prompt_column_name=\"promptText\",\n",
    "    target_column_name=\"responseText\",\n",
    "    deployment_id=external_llm_deployment.dr_deployment.id,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "assert custom_model_llm_validation.validation_status == \"PASSED\"\n",
    "custom_model_llm_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831da7",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Create LLM-Blueprints from external LLMs and vector databases\n",
    "- Create LLM-Blueprints from external LLMS and vector databases\n",
    "- Chat prompt LLM-Blueprints build from external resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6645f80c0bc6d56121831da8",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optionally load playground and external vector database\n",
    "playground = Playground.get(\"6645f94aff64531a95c4ce46\")\n",
    "external_vdb = VectorDatabase.get(\"66460376dcf3d388f96a221b\")\n",
    "custom_model_llm_validation = CustomModelLLMValidation.get(\"665498444ca568110e7e3a7c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6645f80c0bc6d56121831da9",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=6654a7d1f2b2bf398ae4d019, name=external Cohere LLM with external Faiss vdb, is_saved=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create an LLM blueprint using the custom model LLM and the custom model vector database\n",
    "external_llm_blueprint = LLMBlueprint.create(\n",
    "    playground=playground,\n",
    "    name=\"external Cohere LLM with external Faiss vdb\",\n",
    "    llm=\"custom-model\",\n",
    "    llm_settings={\"validation_id\": custom_model_llm_validation.id},\n",
    "    # vector_database=external_vdb,\n",
    ")\n",
    "external_llm_blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6645f80c0bc6d56121831daa",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '665498874ca568110e7e3a82',\n",
       " 'name': 'external Cohere LLM with external Faiss vdb',\n",
       " 'description': '',\n",
       " 'is_saved': False,\n",
       " 'is_starred': False,\n",
       " 'playground_id': '6645f94aff64531a95c4ce46',\n",
       " 'llm_id': 'custom-model',\n",
       " 'llm_name': 'Deployed LLM',\n",
       " 'llm_settings': {'system_prompt': '',\n",
       "  'validation_id': '665498444ca568110e7e3a7c',\n",
       "  'external_llm_context_size': 4096},\n",
       " 'creation_date': '2024-05-27T14:28:23.814111+00:00',\n",
       " 'creation_user_id': '6181286e26c3fde5b5b44430',\n",
       " 'last_update_date': '2024-05-27T14:28:23.814116+00:00',\n",
       " 'last_update_user_id': '6181286e26c3fde5b5b44430',\n",
       " 'prompt_type': 'CHAT_HISTORY_AWARE',\n",
       " 'vector_database_id': '66460376dcf3d388f96a221b',\n",
       " 'vector_database_settings': None,\n",
       " 'vector_database_name': 'External DR Documentation Vector Database',\n",
       " 'vector_database_status': 'COMPLETED',\n",
       " 'vector_database_error_message': None,\n",
       " 'vector_database_error_resolution': None,\n",
       " 'custom_model_llm_validation_status': 'PASSED',\n",
       " 'custom_model_llm_error_message': None,\n",
       " 'custom_model_llm_error_resolution': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the external LLMBlueprint's attributes\n",
    "vars(external_llm_blueprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6645f80c0bc6d56121831dab",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To deploy a custom model on DataRobot, you need to define two components:\n",
      "\n",
      "* **Model content**: This refers to the compiled artifact, source code, and supporting files for the model. The files required for this include:\n",
      "  - Model artifact file or a `custom.py`/`custom.R` file - this is the trained model file or the code file with custom code implemented.\n",
      "  - `model-metadata.yaml` - a file describing the model's metadata including input and output data requirements. This is used to validate the model when building and training a blueprint.\n",
      "  - `requirements.txt` - a list of Python or R packages to add to the base environment.\n",
      "* **Model environment**: This is the Docker image where the model will run. Environments can be default or custom environments, containing a Docker file and any necessary supporting files. \n",
      "\n",
      "The steps to define a custom model with these components include:\n",
      "\n",
      "1. Define and test model content locally.\n",
      "2. Optionally, create a container environment where the model will run.\n",
      "3. Upload the model content and environment into DataRobot. \n",
      "\n",
      "You can find examples of these files on the DataRobot model template repository on GitHub. \n",
      "\n",
      "Additionally, there are some general recommendations for deploying custom models on DataRobot:\n",
      "\n",
      "- If your custom model relies on external data, ensure you have the ability to access this data in the deployed environment.\n",
      "- Make sure you set up the deployment so it can scale with demand.\n",
      "- Ensure your model is secure and that only authorized users can access it. \n",
      "- Regularly test your custom model to ensure it is still performing at the expected level and make updates as needed. G\n"
     ]
    }
   ],
   "source": [
    "# Chat with the external LLM blueprint with an external vector database\n",
    "prompt_external = ChatPrompt.create(\n",
    "    \"How to deploy a custom model in datarobot?\",\n",
    "    external_llm_blueprint,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "print(prompt_external.result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831dac",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1. Define and test model content locally (i.e., on your computer).\n",
       "2. Optionally, create a container environment where the model will run.\n",
       "3. Upload the model content and environment (if applicable) into DataRobot.\n",
       "## Model content {: #model-content }\n",
       "To define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\n",
       "!!! tip\n",
       "    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.\n",
       "File | Description | Required\n",
       "-----|-------------|---------\n",
       "Model artifact file<br>_or_<br>`custom.py`/`custom.R` file | Provide a model artifact and/or a custom code file. <ul><li>Model artifact: a serialized model artifact with a file extension corresponding to the chosen environment language.</li><li>Custom code: custom capabilities implemented with hooks (or functions) that enable DataRobot to run the code and integrate it with other capabilities. | Yes\n",
       "`model-metadata.yaml` | A file describing model's metadata, including input/output data requirements. You can supply a schema that can then be used to validate the model when building and training a blueprint. A schema lets you specify whether a custom model supports or outputs: <ul><li>Certain data types</li><li>Missing values</li><li>Sparse data</li><li>A certain number of columns</li> | Required when a custom model outputs non-numeric data. If not provided, a default schema is used.\n",
       "`requirements.txt` | A list of Python or R packages to add to the base environment. This list pre-installs Python or R packages that the custom model is using but are not a part of the base environment | No\n",
       "Additional files | Other files used by the model (for example, a file that defines helper functions used inside `custom.py`). | No\n",
       "=== \"requirements.txt Python example\"\n",
       "```\n",
       "    Configure the following fields:\n",
       "    * `user_provided_model_id`: Provide any descriptive and unique string value. DataRobot recommends following a naming pattern, such as `<user>/<model-unique-id>`.\n",
       "        !!! note\n",
       "            By default, this ID will reside in a unique namespace, the GitHub repository ID. Alternatively, you can configure the namespace as an input argument to the custom models action.\n",
       "    * `target_type`: Provide the correct target type for your custom model.\n",
       "    * `target_name`: Provide the correct target name for your custom model.\n",
       "    * `model_environment_id`: Provide the DataRobot execution environment required for your custom model. You can find these environments in the DataRobot application under [**Model Registry** > **Custom Model Workshop** > **Environments**](custom-environments).\n",
       "        ![](images/pp-cus-model-github3.png)\n",
       "4. In any directory in your repository, add a deployment definition YAML file (with any filename) containing the following YAML:\n",
       "    ```yaml\n",
       "    user_provided_deployment_id: user/my-awesome-deployment-id\n",
       "    user_provided_model_id: user/model-unique-id-1\n",
       "---\n",
       "title: Prepare custom models\n",
       "description: Prepare to create deployments from custom models\n",
       "---\n",
       "# Prepare custom models for deployment\n",
       "Custom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you've created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\n",
       "* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\n",
       "* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\n",
       "!!! note\n",
       "    Custom inference models are _not_ custom DataRobot models. They are _user-defined_ models created outside of DataRobot and assembled in the Custom Model Workshop for deployment, monitoring, and governance.\n",
       "See the associated [feature considerations](#feature-considerations) for additional information.\n",
       "## Custom Model Workshop\n",
       "* The [model environment](#model-environment) is defined using a Docker file and additional files that will allow DataRobot to build an image where the model will run. There are a variety of built-in environments; you only need to build your own environment when you need to install Linux packages. For more detailed information, see the section on [custom model environments](custom-model-environments/index).\n",
       "At a high level, the steps to define a custom model with these components include:\n",
       "1. Define and test model content locally (i.e., on your computer).\n",
       "2. Optionally, create a container environment where the model will run.\n",
       "3. Upload the model content and environment (if applicable) into DataRobot.\n",
       "## Model content {: #model-content }\n",
       "To define a custom model, create a local folder containing the files listed in the table below (detailed descriptions follow the table).\n",
       "!!! tip\n",
       "    To ensure your assembled custom model folder has the correct contents, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates){ target=_blank } on GitHub.\n",
       "[None, None, None, None]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that the LLM is using information from the vector database.\n",
    "[print(citation.text) for citation in prompt_external.citations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6645f80c0bc6d56121831dad",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=665498874ca568110e7e3a82, name=external Cohere LLM with external Faiss vdb, is_saved=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the custom model blueprint so you can use it in a comparison\n",
    "external_llm_blueprint = external_llm_blueprint.update(is_saved=True)\n",
    "external_llm_blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831dae",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Comparison prompts using mulitple LLM-Blueprints\n",
    "- Compare the chat prompt results of multiple LLM-Blueprints build from internal and external resources, with and without vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f80c0bc6d56121831daf",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " To deploy a custom model on DataRobot, you need to define two components:\n",
       "* **Model content**: this includes the compiled artifact, source code, and additional supporting files related to the model.\n",
       "* **Model environment**: this is the Docker image where the model will run. This can be a default environment provided by DataRobot, or a custom environment that you can build yourself. \n",
       "The steps to deploy your model on DataRobot are the following:\n",
       "1. Define and test the model content locally on your computer.\n",
       "2. (Optional) Create a container environment where the model will run.\n",
       "3. Upload the model content and environment to DataRobot. \n",
       "You can define the model content by creating a local folder containing the files listed in the model content table, as described in the original text that you provided. Also, you can find examples of these files in the [DataRobot model template repository](https://github.com/datarobot/datarobot-user-models/tree/master/model_templates) on GitHub. \n",
       "When you have assembled your custom model folder, you can create a deployment definition YAML file (with any filename) containing the user-provided deployment ID and model IDs, as described in the original text that you provided. \n",
       "Finally, you can use the YAML file to create deployments from custom models.\n",
       "To deploy a custom model in DataRobot, you can follow the workflow outlined below:\n",
       "1. Prepare your custom model by providing the model content, which includes the compiled artifact, source code, and additional supporting files related to the model.\n",
       "2. Define the model environment, which can be either a drop-in or custom environment. Drop-in environments are provided by DataRobot, while custom environments are required for specialized models and use cases.\n",
       "3. Test your custom model in the Custom Model Workshop.\n",
       "4. Register your custom model in the Model Registry.\n",
       "5. Deploy the custom model to a centralized deployment hub, where you can monitor, manage, and govern it alongside your deployed DataRobot models.\n",
       "Note: DataRobot supports custom models built in various programming languages, including Python, R, and Java.\n",
       "To deploy a custom model in DataRobot, you can follow these steps:\n",
       "1. Prepare your custom model: First, ensure that your custom model is trained and saved in a format that is compatible with DataRobot. This could be in the form of a serialized object or a packaged model. \n",
       "2. Create a prediction model: In the DataRobot UI, navigate to the Models tab and click on \"Create Model\". Select the option to create a custom model, and provide the necessary details such as model name and target variable.\n",
       "3. Upload your custom model: In the next step, you will be prompted to upload your custom model file. DataRobot will then attempt to process and validate the model to ensure compatibility.\n",
       "4. Configure custom model settings: Once the upload is successful, you can configure various settings related to your custom model, such as execution environment, memory allocation, and other deployment-specific options. \n",
       "5. Deploy the custom model: After you have reviewed and finalized the settings, you can deploy the custom model by clicking on the \"Deploy\" button. DataRobot will handle the deployment process and provide you with the necessary API endpoint for making predictions.\n",
       "Remember to monitor the model's performance and iterate on improvements as needed. DataRobot provides various tools and features to help you track and optimize custom model deployments.\n",
       "[None, None, None]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_prompt = ComparisonPrompt.create(\n",
    "    [llm_blueprint, llm_blueprint_with_vdb, external_llm_blueprint],\n",
    "    \"How to deploy a custom model in datarobot?\",\n",
    "    wait_for_completion=True,\n",
    ")  # Compare the LLM blueprints\n",
    "\n",
    "[print(result.result_text) for result in comparison_prompt.results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831db0",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Deploy LLM-Blueprints as custom model to make predictions\n",
    "- Create and register custom models from LLM-Blueprints\n",
    "- Deploy LLM-Blueprint custom models\n",
    "- Make predictions on deployed LLM-Blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6645f80c0bc6d56121831db1",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMBlueprint(id=665498874ca568110e7e3a82, name=external Cohere LLM with external Faiss vdb, is_saved=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally load an existing LLM Blueprint with external LLM and vector database\n",
    "external_llm_blueprint = LLMBlueprint.get(\"665498874ca568110e7e3a82\")\n",
    "external_llm_blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6645f80c0bc6d56121831db2",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CustomModelVersion('v1.0'), '6654af244572240ea3cd11bb')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a custom model from the external LLM-Blueprint\n",
    "custom_model_version = external_llm_blueprint.register_custom_model(\n",
    "    prompt_column_name=\"promptText\",\n",
    "    target_column_name=\"responseText\",\n",
    ")\n",
    "(custom_model_version, custom_model_version.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16658bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CustomModelVersion('v2.1'), '665584c36c8386e31e82a080')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally load the last custom model version from a custom model ID\n",
    "custom_model_version = dr.CustomModelVersion.list(\"6654af23828abe299dcd1318\")[0]\n",
    "(custom_model_version, custom_model_version.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6645f80c0bc6d56121831db3",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment(Deployment from custom model with external vector database and LLM) 665585998b9b7fecc34e7f31\n"
     ]
    }
   ],
   "source": [
    "# Deloy the custom model version\n",
    "deployment = dr.Deployment.create_from_custom_model_version(\n",
    "    custom_model_version_id=custom_model_version.id,\n",
    "    label=f\"Deployment from custom model with external vector database and LLM\",\n",
    "    default_prediction_server_id=dr.PredictionServer.list()[0].id,\n",
    "    max_wait=1200,\n",
    ")\n",
    "print(deployment, deployment.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6645f80c0bc6d56121831db4",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deployment(Deployment from custom model with external vector database and LLM)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast it as a drx deployment because it has a more convenient predict method\n",
    "deployment = drx.Deployment(\"665585998b9b7fecc34e7f31\")\n",
    "deployment.dr_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6645f80c0bc6d56121831db5",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To deploy a custom model in DataRobot, you need to define and test the model content locally on your computer, create a container environment where the model will run, and upload the model content and environment into DataRobot. These are the general steps to follow, along with some considerations:\n",
      "\n",
      "1. **Define and test model content locally**: Create a local folder containing the required files for the custom model, including the model artifact file or a custom code file (e.g., `custom.py` or `custom.R`), a `model-metadata.yaml` file describing the model's metadata, and optionally, a `requirements.txt` file with a list of required Python or R packages. You can find examples of these files in the DataRobot model template repository on GitHub.\n",
      "\n",
      "2. **Create a container environment (optional)**: If the model requires a specific environment, you may need to create a Docker container with the required libraries and dependencies. You can use the provided Dockerfile and supporting files to define the environment.\n",
      "\n",
      "3. **Upload the model content and environment to DataRobot**: Once you have defined the model content and optionally created the container environment, you can upload these components to DataRobot by following the instructions in the documentation. You will need to provide a user-provided model ID and optionally a user-provided deployment ID if you want to include additional deployment information.\n",
      "\n",
      "After uploading the model and environment, you can create deployments from your custom models through the DataRobot UI, APIs, or through the `dr-cmd` command-line interface.\n",
      "\n",
      "When creating a deployment, you will need to specify the environment, provide additional deployment-specific configurations, and select the target usage type (e.g., single or batch inference).\n",
      "\n",
      "Finally, it is important to note that the deployment process may vary depending on the specific use case, model requirements, and your organization's resources and infrastructure. It is recommended to consult the DataRobot documentation for more detailed instructions and additional considerations when working with custom models. \n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the deployed LLM-Blueprint\n",
    "# using our external Cohere LLM and our external FAISS index deployment\n",
    "prompt = \"How to deploy a custom model in datarobot?\"\n",
    "\n",
    "prediction = deployment.predict(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"context\": [\"\"],\n",
    "            \"promptText\": [prompt],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(prediction[\"prediction\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645f80c0bc6d56121831db6",
   "metadata": {
    "chart_settings": null,
    "collapsed": false,
    "custom_llm_metric_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This acclerator showcases the capabilities of DataRobot's new generative AI capabilities, including LLM playgrounds and vector databases essential for Retriever Augmented Generation (RAG) workflows. It provided a comprehensive guide on creating and using DataRobot generative AI playgrounds, LLM-Blueprints, and vector databases. The accelerator demonstrated how to deploy and integrate both internal and external vector databases and LLMs, including the use of external resources like the Cohere API. Additionally, it covered the comparative analysis of multiple LLM-Blueprints and the deployment of these blueprints for practical applications and predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
