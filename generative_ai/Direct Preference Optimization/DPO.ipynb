{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad9c4103-876a-4bcc-8210-2242544d48b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Direct Preference Optimization: Instruction Tuning a LLM Using Preference Data\n",
    "* Author: mark@datarobot.com\n",
    "* Date: 2026-02-10\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook outlines how to take preference data and use that to update a model to perform instruction tuninging. It will take a dataset of prompt, prefered response, rejected response and use that to update a model using Direct Preference Optimization, all in a single session without leaving the DataRobot platfrom.\n",
    "\n",
    "1. Download the preference data from DataRobot Registry\n",
    "2. Train a model using Direct Preference Optimization (DPO)\n",
    "3. Upload the new model weights to DataRobot custom model workshop ready to register and then deploy with goverance and monitoring. \n",
    "\n",
    "DPO reformulates the Reinforcement Learning with Human Feedback (RLHF) objective into a classification problem on preference pairs:\n",
    "\n",
    "Takes pairs of responses (preferred vs. rejected) for the same prompt\n",
    "Directly optimizes the language model to increase the likelihood of preferred responses relative to rejected ones\n",
    "Uses a simple binary cross-entropy loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accelerator uses the following libraries:\n",
    "\n",
    "datasets (Hugging Face)\n",
    "A library for easily accessing, processing, and sharing datasets. load_dataset fetches datasets from Hugging Face Hub or local files.\n",
    "\n",
    "trl (Transformer Reinforcement Learning)\n",
    "A library for training language models with reinforcement learning techniques. DPOConfig and DPOTrainer implement Direct Preference Optimization - a method to align LLMs with human preferences without explicit reward modeling.\n",
    "\n",
    "transformers (Hugging Face)\n",
    "The core library for working with pre-trained transformer models. AutoModelForCausalLM loads text generation models, and AutoTokenizer loads the corresponding tokenizer to convert text to/from tokens.\n",
    "\n",
    "datarobot\n",
    "Enterprise MLOps platform SDK for building, deploying, and managing machine learning models. Provides programmatic access to DataRobot's AutoML and deployment features.\n",
    "\n",
    "torch (PyTorch)\n",
    "Deep learning framework providing tensor computation and automatic differentiation. The backbone for training and running neural networks.\n",
    "\n",
    "os\n",
    "Python standard library for interacting with the operating system - file paths, environment variables, directory operations, etc.\n",
    "\n",
    "accelerate (Hugging Face)\n",
    "Simplifies running PyTorch code across different hardware setups (multi-GPU, TPU, mixed precision). notebook_launcher specifically enables launching distributed training directly from Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import datarobot as dr\n",
    "import requests\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataRobot connection settings\n",
    "# These variables can also be fetched from a secret store or config files\n",
    "from zlib import MAX_WBITS\n",
    "\n",
    "\n",
    "# Uncomment if want to manually specify credentals\n",
    "# DATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n",
    "# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n",
    "\n",
    "# DATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n",
    "# The API Token can be found by click the avatar icon and then </> Developer Tools\n",
    "\n",
    "# Dataset settings\n",
    "# DATASET_ID = \"<DATASET_ID>\"\n",
    "DATASET_NAME = \"preference_training_dataset.jsonl\"\n",
    "DATASET_FORMAT = \"json\"\n",
    "\n",
    "# Model settings\n",
    "MODEL_ID = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "TMP_DIR = \"/tmp/qwen2-0.5b-dpo\"\n",
    "OUTPUT_DIR = \"/home/notebooks/storage/qwen2-0.5b-dpo-final\"\n",
    "LOGGING_DIR = \"/tmp/qwen2-0.5b-dpo-logs\"\n",
    "SHARD_WRAP_CLASS = \"Qwen2DecoderLayer\"\n",
    "\n",
    "# Training settings\n",
    "NUM_PROCESSES = 4  # Number of GPUs to use\n",
    "\n",
    "# Custom model workshop settings\n",
    "RUNTIME_ID = \"662d6a54ef58f64c5a07d122\"\n",
    "CUSTOM_MODEL_NAME = \"DPO_Trained_Model\"\n",
    "MAX_WAIT = 6000\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "You can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client()\n",
    "\n",
    "# If running outside of the DataRobot platform pass without envvars in credential\n",
    "# dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section allows downloadiing a precreated dataset from the DataRobot registry. For this example, a dataset from S3 is used instead. Uncomment the function call to download from the DataRobot registry, e.g. from a dataset generated from a deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_registry_file(dataset_id, local_path):\n",
    "    \"\"\"Download a dataset from the DataRobot registry.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: The ID of the dataset in DataRobot registry\n",
    "        local_path: The local path where the file will be saved\n",
    "    \n",
    "    Returns:\n",
    "        The path to the downloaded file\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the download fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the dataset object from the registry\n",
    "        dataset = dr.Dataset.get(dataset_id)\n",
    "\n",
    "        # Download the file\n",
    "        print(f\"Downloading {dataset.name}...\")\n",
    "        dataset.get_file(local_path)\n",
    "        print(f\"File saved to: {local_path}\")\n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def download_s3_example_file(local_path):\n",
    "\n",
    "    url = 'https://s3.us-east-1.amazonaws.com/datarobot_public_datasets/ai_accelerators/preference_training.jsonl'\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status() # Check if the download was successful\n",
    "\n",
    "        with open(local_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"File successfully downloaded and saved to {local_path}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download the preference dataset from the registry\n",
    "# download_registry_file(DATASET_ID, DATASET_NAME)\n",
    "\n",
    "# Download from S3\n",
    "download_s3_example_file(DATASET_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Dataset\n",
    "\n",
    "Ensure the dataset has the required columns for DPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_preference_dataset(file_path):\n",
    "    \"\"\"Validate that the dataset has required columns for DPO training.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        The loaded dataset if valid\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required columns are missing\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"csv\", data_files=file_path)[\"train\"]\n",
    "    \n",
    "    required_cols = {\"chosen\", \"rejected\"}\n",
    "    actual_cols = set(dataset.column_names)\n",
    "    \n",
    "    if not required_cols.issubset(actual_cols):\n",
    "        missing = required_cols - actual_cols\n",
    "        raise ValueError(\n",
    "            f\"Dataset is missing required columns: {missing}. \"\n",
    "            f\"Found columns: {actual_cols}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Dataset validated successfully!\")\n",
    "    print(f\"Number of examples: {len(dataset)}\")\n",
    "    print(f\"Columns: {dataset.column_names}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the downloaded dataset\n",
    "dataset = validate_preference_dataset(DATASET_NAME)\n",
    "\n",
    "# Preview a sample\n",
    "print(\"\\nSample from dataset:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning with DPO\n",
    "\n",
    "This section uses trl and HuggingFace accelrate to perform Direct Preference Optimization (https://arxiv.org/abs/2305.18290) \n",
    "\n",
    "This example is designed to run on 4 A10s. \n",
    "\n",
    "Key Features\n",
    "Distributed Training: Designed to work with Accelerate's notebook_launcher for multi-GPU training\n",
    "Memory Optimization: Implements FSDP (Fully Sharded Data Parallel) with parameter offloading to handle large models efficiently\n",
    "Mixed Precision: Uses bfloat16 for faster training and reduced memory footprint\n",
    "Gradient Checkpointing: Trades computation for memory to enable training larger models\n",
    "Monitoring: Integrated TensorBoard logging for tracking training metrics\n",
    "\n",
    "In the terminal session tensorboard can be launched on host 0.0.0.0 and if the appropriate ports are exposed it can be viewed to track learning progress.'\n",
    "\n",
    "This leverages the HuggingFace accelerate framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGGING_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_function = f\"\"\"\n",
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "def dpo_train():\n",
    "    # 1. Load Dataset (Format: prompt, chosen, rejected)\n",
    "    dataset = load_dataset(\"{DATASET_FORMAT}\", data_files=\"{DATASET_NAME}\")\n",
    "\n",
    "    # 2. Load Model & Tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"{MODEL_ID}\", \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_ID}\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 3. DPO Configuration\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=\"{TMP_DIR}\",\n",
    "        per_device_train_batch_size=4, # Increase this if VRAM allows\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-7,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=1,\n",
    "        max_steps=50,\n",
    "        bf16=True,\n",
    "        fsdp=\"full_shard auto_wrap\",\n",
    "        fsdp_config={{\n",
    "            \"transformer_layer_cls_to_wrap\": \"{SHARD_WRAP_CLASS}\",\n",
    "            \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\n",
    "            \"fsdp_offload_params\": True,               # Move gathered weights to CPU\n",
    "        }},\n",
    "        gradient_checkpointing=True,\n",
    "        remove_unused_columns=False,\n",
    "        logging_dir=\"{LOGGING_DIR}\",          # Where TensorBoard events will be saved\n",
    "        report_to=[\"tensorboard\"],       # Enables TensorBoard logging\n",
    "        logging_first_step=True,\n",
    "        # Turn this OFF to avoid the ValueError during the run\n",
    "        save_only_model=False,\n",
    "        save_strategy=\"no\",\n",
    "    \n",
    "    )\n",
    "\n",
    "    # 4. Initialize Trainer\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    # 5. Train the model\n",
    "    print(\"Starting DPO training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # 6. Save the model\n",
    "    save_trained_model(trainer, tokenizer, \"{OUTPUT_DIR}\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_function = '''\n",
    "def save_trained_model(trainer, tokenizer, output_dir):\n",
    "    \"\"\"Save the trained model handling FSDP distributed training.\n",
    "    \n",
    "    Args:\n",
    "        trainer: The DPOTrainer instance\n",
    "        tokenizer: The tokenizer to save\n",
    "        output_dir: Directory to save the model\n",
    "    \"\"\"\n",
    "    # Wait for all processes to catch up\n",
    "    trainer.accelerator.wait_for_everyone()\n",
    "\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "        from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n",
    "\n",
    "        # Set FSDP to gather weights for a single file\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "        # Configure FSDP to output a full state dict (not shards)\n",
    "        save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "        \n",
    "        with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy):\n",
    "            cpu_state_dict = trainer.model.state_dict()\n",
    "\n",
    "        # Save only on Rank 0\n",
    "        if trainer.accelerator.is_main_process:\n",
    "            # Save the model with the consolidated state dict\n",
    "            trainer.model.save_pretrained(\n",
    "                output_dir,\n",
    "                state_dict=cpu_state_dict,\n",
    "                safe_serialization=True\n",
    "            )\n",
    "            # Save the tokenizer\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "    else:\n",
    "        # If not using FSDP, standard save\n",
    "        if trainer.accelerator.is_main_process:\n",
    "            trainer.save_model(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "\n",
    "    # Final sync\n",
    "    trainer.accelerator.wait_for_everyone()\n",
    "\n",
    "def main():\n",
    "    dpo_train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Training\n",
    "\n",
    "Launch the distributed training across multiple GPUs using accelerate's notebook_launcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Launch the training job\n",
    "print(f\"Launching DPO training on {NUM_PROCESSES} GPUs...\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Logging directory: {LOGGING_DIR}\")\n",
    "\n",
    "train_filepath = \"train_dpo.py\"\n",
    "\n",
    "try:\n",
    "    with open(train_filepath, \"w\") as f:\n",
    "        f.write(train_function + \"\\n\\n\" + save_function)\n",
    "    print(f\"Successfully wrote string to {train_filepath}\")\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Saved Model\n",
    "\n",
    "Verify that the model was saved correctly by checking the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_saved_model(output_dir):\n",
    "    \"\"\"Verify that the model was saved correctly.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory where the model was saved\n",
    "    \n",
    "    Returns:\n",
    "        True if verification passes, False otherwise\n",
    "    \"\"\"\n",
    "    required_files = [\n",
    "        \"config.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "    ]\n",
    "    \n",
    "    # Check for model weights (either safetensors or pytorch format)\n",
    "    model_files = [\n",
    "        \"model.safetensors\",\n",
    "        \"pytorch_model.bin\",\n",
    "        \"model.safetensors.index.json\",\n",
    "        \"pytorch_model.bin.index.json\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Checking saved model in {output_dir}...\")\n",
    "    print(f\"\\nFiles found:\")\n",
    "    \n",
    "    found_files = os.listdir(output_dir)\n",
    "    for f in found_files:\n",
    "        file_path = os.path.join(output_dir, f)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  - {f} ({size / 1024 / 1024:.2f} MB)\")\n",
    "    \n",
    "    # Check required files\n",
    "    missing_required = [f for f in required_files if f not in found_files]\n",
    "    if missing_required:\n",
    "        print(f\"\\nWarning: Missing required files: {missing_required}\")\n",
    "        return False\n",
    "    \n",
    "    # Check for at least one model file\n",
    "    has_model_file = any(f in found_files for f in model_files) or \\\n",
    "                     any(f.startswith(\"model-\") and f.endswith(\".safetensors\") for f in found_files)\n",
    "    \n",
    "    if not has_model_file:\n",
    "        print(f\"\\nWarning: No model weights file found\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nModel verification passed!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved model\n",
    "verify_saved_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to DataRobot Workshop\n",
    "\n",
    "For use in the vLLM image we need to specify to load the saved model files.\n",
    "\n",
    "Note: serving models with older versions of transformers is incompatiable with the generated json from newer versions. This can be fixed by changing the \"extra_special_tokens\" field to \"additional_special_tokens\" by uncommenting and running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sed -i 's/\"extra_special_tokens\"/\"additional_special_tokens\"/g' tokenizer_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_json = f\"\"\"\n",
    "{{\n",
    "  \"args\": [\n",
    "    \"--model\", \"/opt/code\", \"--served-model-name\", \"{MODEL_ID}\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "engine_json_path = os.path.join(OUTPUT_DIR, \"engine_config.json\")\n",
    "\n",
    "try:\n",
    "    with open(engine_json_path, \"w\") as f:\n",
    "        f.write(engine_json)\n",
    "    print(f\"Successfully wrote string to {engine_json_path}\")\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_custom_workshop(model_name, local_folder_path, runtime_id, max_wait=MAX_WAIT):\n",
    "    \"\"\"Upload a model to DataRobot Custom Model Workshop.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name for the custom model in DataRobot\n",
    "        local_folder_path: Path to the folder containing model files\n",
    "        runtime_id: The runtime environment ID to use\n",
    "        max_wait: Maximum wait time in seconds for API calls\n",
    "    \n",
    "    Returns:\n",
    "        The created CustomModelVersion object\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If upload fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating custom model: {model_name}\")\n",
    "        \n",
    "        # 1. Create the Custom Model shell\n",
    "        custom_model = dr.CustomInferenceModel.create(\n",
    "            name=model_name,\n",
    "            target_type=dr.TARGET_TYPE.TEXT_GENERATION,\n",
    "            target_name=\"promptText\"\n",
    "        )\n",
    "        print(f\"Custom model created with ID: {custom_model.id}\")\n",
    "\n",
    "        # 2. Upload files and create a version\n",
    "        print(f\"Uploading model files from {local_folder_path}...\")\n",
    "        print(\"This may take several minutes depending on model size.\")\n",
    "        \n",
    "        model_version = dr.CustomModelVersion.create_clean(\n",
    "            custom_model_id=custom_model.id,\n",
    "            base_environment_id=runtime_id,\n",
    "            folder_path=local_folder_path,\n",
    "            max_wait=max_wait\n",
    "        )\n",
    "\n",
    "        print(f\"\\nUpload successful!\")\n",
    "        print(f\"Custom Model ID: {custom_model.id}\")\n",
    "        print(f\"Model Version ID: {model_version.id}\")\n",
    "        print(f\"\\nNext steps:\")\n",
    "        print(f\"1. Go to DataRobot Custom Model Workshop\")\n",
    "        print(f\"2. Find model: {model_name}\")\n",
    "        print(f\"3. Register and deploy the model\")\n",
    "        \n",
    "        return model_version\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Custom Model Workshop: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the fine-tuned model to DataRobot\n",
    "model_version = upload_to_custom_workshop(\n",
    "    model_name=CUSTOM_MODEL_NAME,\n",
    "    local_folder_path=OUTPUT_DIR,\n",
    "    runtime_id=RUNTIME_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Utility Functions\n",
    "\n",
    "Additional helper functions for common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_runtimes():\n",
    "    \"\"\"List available runtime environments in DataRobot.\n",
    "    \n",
    "    Useful for finding the correct RUNTIME_ID for your model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        environments = dr.ExecutionEnvironment.list()\n",
    "        print(\"Available Runtime Environments:\")\n",
    "        print(\"-\" * 80)\n",
    "        for env in environments:\n",
    "            print(f\"ID: {env.id}\")\n",
    "            print(f\"Name: {env.name}\")\n",
    "            print(f\"Description: {env.description}\")\n",
    "            print(\"-\" * 80)\n",
    "        return environments\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing environments: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_custom_models():\n",
    "    \"\"\"List existing custom models in DataRobot.\n",
    "    \n",
    "    Useful for checking existing models before creating new ones.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = dr.CustomInferenceModel.list()\n",
    "        print(\"Existing Custom Models:\")\n",
    "        print(\"-\" * 80)\n",
    "        for model in models:\n",
    "            print(f\"ID: {model.id}\")\n",
    "            print(f\"Name: {model.name}\")\n",
    "            print(f\"Target Type: {model.target_type}\")\n",
    "            print(f\"Created: {model.created_at}\")\n",
    "            print(f\"Updated: {model.updated_at}\")\n",
    "            print(\"-\" * 80)\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_custom_model(model_id):\n",
    "    \"\"\"Delete a custom model from DataRobot.\n",
    "    \n",
    "    Args:\n",
    "        model_id: The ID of the custom model to delete\n",
    "    \n",
    "    Warning: This action cannot be undone!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = dr.CustomInferenceModel.get(model_id)\n",
    "        model_name = model.name\n",
    "        \n",
    "        # Confirm deletion\n",
    "        confirm = input(f\"Are you sure you want to delete '{model_name}'? (yes/no): \")\n",
    "        if confirm.lower() == 'yes':\n",
    "            model.delete()\n",
    "            print(f\"Model '{model_name}' deleted successfully.\")\n",
    "        else:\n",
    "            print(\"Deletion cancelled.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting model: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
