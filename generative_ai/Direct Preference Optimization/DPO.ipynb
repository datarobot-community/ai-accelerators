{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad9c4103-876a-4bcc-8210-2242544d48b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Direct Preference Optimization: Fine Tuning a LLM Using Preference Data\n",
    "* Author: mark@datarobot.com\n",
    "* Date: 2026-02-10\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook outlines how to take preference data and use that to update a model. It will take a dataset of query, good response, bad response and use that to update a model using DPO, in a single session.\n",
    "\n",
    "1. Download the preference data from DataRobot\n",
    "2. Train a model using Direct Preference Optimization (DPO)\n",
    "3. Upload the new model weights to DataRobot ready to register and then deploy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import datarobot as dr\n",
    "import torch\n",
    "\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables can aso be fetched from a secret store or config files\n",
    "DATAROBOT_ENDPOINT=\"https://app.eu.datarobot.com/api/v2\"\n",
    "# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n",
    "\n",
    "DATAROBOT_API_TOKEN=\"<INSERT YOUR DataRobot API Token>\"\n",
    "# The API Token can be found by click the avatar icon and then </> Developer Tools\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "You can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is section will download a precreated dataset from the DataRobot registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_registry_file(dataset_id, local_path):\n",
    "    # Retrieve the dataset object from the registry\n",
    "    dataset = dr.Dataset.get(dataset_id)\n",
    "\n",
    "    # Download the file\n",
    "    print(f\"Downloading {dataset.name}...\")\n",
    "    dataset.get_file(local_path)\n",
    "    print(f\"File saved to: {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = '<DATASET_IT'\n",
    "DATASET_NAME = 'preference_training_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning with DPO\n",
    "\n",
    "This section uses trl and HuggingFace accelrate to perform Direct Preference Optimization (https://arxiv.org/abs/2305.18290) \n",
    "\n",
    "This example is designed to run on 4 A10s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TMP_DIR = '/tmp/qwen2-0.5b-dpo'\n",
    "OUTPUT_DIR = '/home/notebooks/storage/qwen2-0.5b-dpo'\n",
    "SHARD_WRAP_CLASS = \"Qwen2DecoderLayer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def dpo_train():\n",
    "    # 1. Load Dataset (Format: prompt, chosen, rejected)\n",
    "    dataset = load_dataset(\"csv\", data_files=DATASET_NAME)\n",
    "\n",
    "    # 2. Load Model & Tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 3. DPO Configuration\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=4, # Increase this if VRAM allows\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-7,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=1,\n",
    "        max_steps=500,\n",
    "        bf16=True,\n",
    "        fsdp=\"full_shard auto_wrap\",\n",
    "        fsdp_config={\n",
    "            \"transformer_layer_cls_to_wrap\": SHARD_WRAP_CLASS,\n",
    "            \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\n",
    "            \"fsdp_offload_params\": True,               # Move gathered weights to CPU\n",
    "        },\n",
    "        gradient_checkpointing=True,\n",
    "        remove_unused_columns=False,\n",
    "        logging_dir=LOGGING_DIR,          # Where TensorBoard events will be saved\n",
    "        report_to=[\"tensorboard\"],       # Enables TensorBoard logging\n",
    "        logging_first_step=True,\n",
    "        # Turn this OFF to avoid the ValueError during the run\n",
    "        save_only_model=False,\n",
    "        save_strategy=\"no\",\n",
    "    \n",
    "    )\n",
    "\n",
    "\n",
    "    # 4. Initialize Trainer\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    # 1. Wait for all processes to catch up\n",
    "    trainer.accelerator.wait_for_everyone()\n",
    "\n",
    "    # 2. Set FSDP to gather weights for a single file\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "        from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n",
    "   \n",
    "        # This force-sets the plugin to FULL_STATE_DICT for the final save\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "    \n",
    "        # Save the model\n",
    "        # Rank 0 will handle the consolidation automatically here\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "        # Configure FSDP to output a full state dict (not shards)\n",
    "        save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, save_policy):\n",
    "            cpu_state_dict = model.state_dict()\n",
    "    \n",
    "        # 3. Save only on Rank 0\n",
    "        if trainer.accelerator.is_main_process:\n",
    "            # Ensure the tokenizer is also saved there\n",
    "            tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    else:\n",
    "        # If not using FSDP, standard save\n",
    "        if trainer.accelerator.is_main_process:\n",
    "            trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "notebook_launcher(dpo_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to DataRobot Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNTIME_ID = '662d6a54ef58f64c5a07d122'\n",
    "CUSTOM_MODEL_NAME = 'DPO_Trained_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_custom_workshop(model_name, local_folder_path, runtime_id):\n",
    "    # 1. Create the Custom Model shell\n",
    "    custom_model = dr.CustomInferenceModel.create(\n",
    "        name=model_name,\n",
    "        target_type=dr.TARGET_TYPE.TEXT_GENERATION, # Options: BINARY, REGRESSION, MULTICLASS\n",
    "        target_name=\"promptText\",\n",
    "        )\n",
    "\n",
    "    # 2. Upload files and create a version\n",
    "    # 'local_folder_path' should contain your model.py, requirements.txt, etc.\n",
    "    model_version = dr.CustomModelVersion.create_clean(\n",
    "        custom_model_id=custom_model.id,\n",
    "        base_environment_id=runtime_id,\n",
    "        folder_path=local_folder_path\n",
    "    )\n",
    "\n",
    "    print(f\"Model Created: {custom_model.id}\")\n",
    "    print(f\"Version Created: {model_version.id}\")\n",
    "    return model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    upload_to_custom_workshop(CUSTOM_MODEL_NAME, OUTPUT_DIR, RUNTIME_ID)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
