{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7a029c-9187-4444-b76c-4a3109f934ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter optimization workflow\n",
    "\n",
    "Author: Matthias Kullowatz, Bogdan Tsal-Tsalko\n",
    "<br>\n",
    "Version Date: 2023-07-12\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds on the [hyperparameter optimizaton functionality](https://github.com/datarobot-community/ai-accelerators/blob/main/advanced-experimentation/Hyperparameter_Optimization/HyperParam_Opt_Core_Concepts.ipynb) exhibited by Bogdan Tsal-Tsalko in the first of DataRobot's parameter tuning accelerators. This AI Accelerator builds on the native DataRobot hyperparameter tuning by integrating the [hyperopt](http://hyperopt.github.io/hyperopt/) module into DataRobot workflows. The hyperopt module allows for a particular Bayesian approach to parameter tuning, though more generally this notebook should be seen as an example of how to leverage DataRobot's API to integrate with <i>any</i> parameter tuning framework.\n",
    "\n",
    "### Hyperparameters for DataRobot modeling\n",
    "\n",
    "In machine learning, hyperparameter tuning is the act of adjusting the \"settings\" (referred to as hyperparameters) in a machine learning algorithm, whether that's the learning rate for an XGBoost model or the activation function in a neural network. Many methods for doing this exist, with the simplest being a brute force search over every feasible combination. While this requires little effort, it's extremely time-consuming as each combination requires fitting the machine learning algorithm. To this end, practitioners strive to find more efficient ways to search for the best combination of hyperparameters to use in a given prediction problem. DataRobot employs a proprietary version of [pattern search](https://app.datarobot.com/docs/modeling/analyze-models/evaluate/adv-tuning.html#set-the-search-type) for optimization not only for the machine learning algorithm's specific hyperparameters, but also the *respective data pre-processing needed to fit the algorithm*, with the goal of quickly producing high-performance models tailored to your dataset.\n",
    "\n",
    "While the approach used at DataRobot is sufficient in most cases, you may want to build upon DataRobot's Autopilot modeling process with custom tuning methods such as hyperopt's \"Tree of Parzen Estimators\" approach. In this AI Accelerator, you will explore using non-DataRobot-native hyperparameter optimization methods on the DataRobot model leaderboard. Note that best practices generally recommend to wait until the model is in a near-finished state before searching for the best hyperparameters to use. Specifically, the following have already been finalized:\n",
    "\n",
    "- Training data (e.g., data sources)\n",
    "- Model validation method (e.g., group cross-validation, random cross-validation, or backtesting. How the problem is framed influences all subsequent steps, as it changes error minimization.)\n",
    "- Feature engineering (particularly, calculations driven by subject matter expertise)\n",
    "- Preprocessing and data transformations (e.g., word or character tokenizers, PCA, embeddings, normalization, etc.)\n",
    "- Algorithm type (e.g. GLM, tree-based, neural net)\n",
    "\n",
    "These decisions typically have a larger impact on model performance compared to adjusting a machine learning algorithm's hyperparameters (especially when using DataRobot, as the hyperparameters chosen automatically are pretty competitive).  \n",
    "\n",
    "### Problem setup\n",
    "\n",
    "You have settled on a near-final model, and getting that last extra bit of performance matters for your use case. You want to perform a more exhaustive search, or perhaps as an advanced data scientist you have certain parameters you'd like to experiment with. \n",
    "\n",
    "### Advanced tuning and gridsearch approach\n",
    "\n",
    "This notebook helps you learn how to access, understand, and tune blueprints for both preprocessing and model hyperparameters. You'll programmatically work with DataRobot [advanced tuning](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/adv-tuning.html) which you can then adapt to your other projects.\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "* Identify specific blueprints and review hyperparameters\n",
    "* Define a search space and optimization algorithm with hyperopt\n",
    "* Tune hyperparameters with hyperopt's Tree of Parzen Estimators (Bayesian) approach\n",
    "    \n",
    "### Prerequsites\n",
    "\n",
    "- A completed DataRobot project (example data can be accessed here: [API](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.25.1/examples/lending_club/Predicting_Bad_Loans.html) and a [notebook](https://docs.datarobot.com/en/docs/api/guide/common-case/loan-default/loan-default-nb.html)).\n",
    "\n",
    "- Review requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e375a98-dc8a-43d8-a7b9-797477e3374b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb22cb7-abd9-4eb1-96d0-3c5d027a9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import datarobot as dr\n",
    "from datarobot.errors import AsyncProcessUnsuccessfulError, JobNotFinished\n",
    "from datarobot.models.job import Job\n",
    "import graphviz\n",
    "from helpers import *\n",
    "from hyperopt import fmin, space_eval, STATUS_OK, tpe, Trials\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efb180-8732-4e30-ba3c-a5c7c163fd35",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d846ce18-de7b-49c6-a993-bc0df0a9de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x7f93f9326d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.Client(config_path=\"drconfig.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068ddb7-603f-4db4-b1c9-ac700c8c618a",
   "metadata": {},
   "source": [
    "### Create a project (optional)\n",
    "\n",
    "Create a project from a URL in DataRobot and follow the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad2cac4-7a37-4aac-98b2-c1d50e94af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: 64af2b88d6f032a15e39865a\n",
      "CPU times: user 1 s, sys: 188 ms, total: 1.19 s\n",
      "Wall time: 19min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://s3.amazonaws.com/datarobot_public_datasets/10K_Lending_Club_Loans.csv\"\n",
    "project = dr.Project.create(url, project_name=\"Loan_default_hyperopt\")\n",
    "print(\"Project ID: {}\".format(project.id))\n",
    "project.analyze_and_model(\n",
    "    target=\"is_bad\",\n",
    "    worker_count=-1,\n",
    "    mode=dr.enums.AUTOPILOT_MODE.FULL_AUTO,\n",
    ")\n",
    "project.wait_for_autopilot(verbosity=dr.enums.VERBOSITY_LEVEL.SILENT)\n",
    "\n",
    "# Alteratively, import an existing DataRobot project\n",
    "# project = dr.Project.get('<project_id>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294ac47-01e8-4e46-918e-223023e21b7a",
   "metadata": {},
   "source": [
    "## Prepare for tuning a model\n",
    "\n",
    "### Select a model\n",
    "\n",
    "To start tuning, select the model you want to tune from an existing DataRobot project. Your project ID and model ID might be accessed through the API via `dr.Project.list(search_params=({'project_name':'YOUR_SEARCH_KEY'}))`, or simply from the URL of your project.  Alternatively, the helper function below returns a summary of the Leaderboard for a given project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3335c418-2317-4ba0-baab-fa39e9152a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project URL: https://app.datarobot.com/projects/64af2b88d6f032a15e39865a/eda\n",
      "Project ID: 64af2b88d6f032a15e39865a\n",
      "Unique blueprints tested: 37\n",
      "Feature lists tested: 2\n",
      "Models trained: 63\n",
      "Blueprints in the project repository: 83\n",
      "Feature engineering and preprocessing steps ran:  487\n",
      "\n",
      "\n",
      "Top models in the leaderboard:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>model</th>\n",
       "      <th>pct</th>\n",
       "      <th>validation_LogLoss</th>\n",
       "      <th>cross_validation_LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64af2ce96737bffdab113858</td>\n",
       "      <td>Light Gradient Boosted Trees Classifier with E...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36734</td>\n",
       "      <td>0.357414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64af2cea6737bffdab11385c</td>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36780</td>\n",
       "      <td>0.357710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64af2ce96737bffdab113857</td>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36571</td>\n",
       "      <td>0.357786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64af2ce96737bffdab113859</td>\n",
       "      <td>Gradient Boosted Trees Classifier with Early S...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36608</td>\n",
       "      <td>0.357796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64af2e632bb2ec97f642888c</td>\n",
       "      <td>Light Gradient Boosted Trees Classifier with E...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36827</td>\n",
       "      <td>0.357800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model_id  \\\n",
       "0  64af2ce96737bffdab113858   \n",
       "1  64af2cea6737bffdab11385c   \n",
       "2  64af2ce96737bffdab113857   \n",
       "3  64af2ce96737bffdab113859   \n",
       "4  64af2e632bb2ec97f642888c   \n",
       "\n",
       "                                               model   pct  \\\n",
       "0  Light Gradient Boosted Trees Classifier with E...  64.0   \n",
       "1  eXtreme Gradient Boosted Trees Classifier with...  64.0   \n",
       "2  eXtreme Gradient Boosted Trees Classifier with...  64.0   \n",
       "3  Gradient Boosted Trees Classifier with Early S...  64.0   \n",
       "4  Light Gradient Boosted Trees Classifier with E...  64.0   \n",
       "\n",
       "   validation_LogLoss  cross_validation_LogLoss  \n",
       "0             0.36734                  0.357414  \n",
       "1             0.36780                  0.357710  \n",
       "2             0.36571                  0.357786  \n",
       "3             0.36608                  0.357796  \n",
       "4             0.36827                  0.357800  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access an existing project set your project ID below\n",
    "# project = dr.Project.get(\"project_id_see_example_below\")\n",
    "\n",
    "print(\"Project URL: \" + \"https://app.datarobot.com/projects/\" + project.id + \"/eda\")\n",
    "print(\"Project ID: \" + project.id)\n",
    "\n",
    "leaderboard_top = get_top_of_leaderboard(project, metric=\"LogLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3186171-3df3-40bd-b2f9-ee852803a3fd",
   "metadata": {},
   "source": [
    "The example cell below retrieves the XGBoost blueprint with Unsupervised Learning Features, which will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3af9d5-34a1-4ad4-8c85-a9c1ebcda7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_from_search = [\n",
    "    bp for bp in project.get_blueprints() if \"Unsupervised Learning Features\" in bp.model_type\n",
    "][\n",
    "    0\n",
    "].id  # search the blueprint repository\n",
    "\n",
    "model = Job.get(\n",
    "    project_id=project.id,\n",
    "    job_id=project.train(bp_from_search),  # train the blueprint or get the model results\n",
    ").get_result_when_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf48c9-10bc-4e8b-ac54-ba5aaf601708",
   "metadata": {},
   "source": [
    "Alternative approach to select a model from the top models output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ff6ebc-331c-4661-83c7-1b1f00dec9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model('eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dr.Model.get(\n",
    "    project=project.id,\n",
    "    model_id=leaderboard_top.iloc[\n",
    "        2\n",
    "    ][  # Select a model from the top-performing models in the Leaderboard\n",
    "        \"model_id\"\n",
    "    ],\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e2984-4f95-4ac8-b65a-5c32fef303f5",
   "metadata": {},
   "source": [
    "### Inspect a blueprint\n",
    "\n",
    "Before you start tuning, it is conceptually important to reiterate what is contained within a blueprint. In addition to the learning algorithm hyperparmeters, you also have the option to experiment with the hyperparameters of the tasks prior to the learner. \n",
    "\n",
    "For more information please visit the documentation on [blueprints](https://docs.datarobot.com/en/docs/modeling/analyze-models/describe/blueprints.html). \n",
    "\n",
    "You can visualize the processes for your selected model using the cell below to generate a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258bf5ed-876a-4a7c-97f0-edf9a7e56c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlueprintChart(24 nodes, 31 edges)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Title: Blueprint Chart Pages: 1 -->\n",
       "<svg width=\"2724pt\" height=\"435pt\"\n",
       " viewBox=\"0.00 0.00 2723.50 435.02\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 431.02)\">\n",
       "<title>Blueprint Chart</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-431.02 2719.5,-431.02 2719.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"28.32\" cy=\"-128.02\" rx=\"28.32\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"28.32\" y=\"-122.97\" font-family=\"Times,serif\" font-size=\"14.00\">Data</text>\n",
       "</g>\n",
       "<!-- &#45;3 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>&#45;3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"172.65\" cy=\"-301.02\" rx=\"64.15\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.65\" y=\"-295.97\" font-family=\"Times,serif\" font-size=\"14.00\">Text Variables</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;&#45;3 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;&#45;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M42.46,-143.98C67.11,-173.95 120.52,-238.87 150.37,-275.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"148.02,-276.8 157.07,-282.3 153.42,-272.35 148.02,-276.8\"/>\n",
       "</g>\n",
       "<!-- &#45;2 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>&#45;2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"172.65\" cy=\"-128.02\" rx=\"80.01\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.65\" y=\"-122.97\" font-family=\"Times,serif\" font-size=\"14.00\">Numeric Variables</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;&#45;2 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>0&#45;&gt;&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.1,-128.02C64.41,-128.02 72.7,-128.02 81.4,-128.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.18,-131.52 91.18,-128.02 81.18,-124.52 81.18,-131.52\"/>\n",
       "</g>\n",
       "<!-- &#45;1 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>&#45;1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-47.02\" rx=\"89.74\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-41.97\" font-family=\"Times,serif\" font-size=\"14.00\">Categorical Variables</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;&#45;1 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>0&#45;&gt;&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.43,-117.12C63.38,-111.6 78.55,-105.21 92.64,-101.02 164.05,-79.78 247.49,-65.72 309.06,-57.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.47,-60.63 318.91,-55.83 308.53,-53.69 309.47,-60.63\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-409.02\" rx=\"110.21\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-403.97\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;5 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.42,-318.44C215.27,-336.83 252.23,-365.39 288.67,-382.02 297.12,-385.88 306.21,-389.23 315.4,-392.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.21,-395.74 324.79,-395.24 316.21,-389.03 314.21,-395.74\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-355.02\" rx=\"110.21\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-349.97\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.31,-312.73C252.17,-319.93 291,-329.28 324.55,-337.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"323.7,-340.99 334.24,-339.93 325.33,-334.19 323.7,-340.99\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-301.02\" rx=\"110.21\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-295.97\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.01,-301.02C249.8,-301.02 263.65,-301.02 277.67,-301.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.41,-304.52 287.41,-301.02 277.41,-297.52 277.41,-304.52\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-247.02\" rx=\"110.21\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-241.97\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;11 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.31,-289.3C252.17,-282.11 291,-272.76 324.55,-264.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325.33,-267.85 334.24,-262.1 323.7,-261.04 325.33,-267.85\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-409.02\" rx=\"258.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-403.97\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M509.55,-409.02C517.39,-409.02 525.48,-409.02 533.73,-409.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"533.67,-412.52 543.67,-409.02 533.67,-405.52 533.67,-412.52\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1252.06\" cy=\"-301.02\" rx=\"64.15\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1252.06\" y=\"-295.97\" font-family=\"Times,serif\" font-size=\"14.00\">Bind branches</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M989.74,-396.24C1014.02,-392.61 1038.41,-387.97 1061.33,-382.02 1115.43,-367.97 1174.08,-340.95 1211.62,-321.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1212.85,-324.78 1220.17,-317.12 1209.67,-318.55 1212.85,-324.78\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>19</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2194.82\" cy=\"-122.02\" rx=\"387.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2194.82\" y=\"-116.97\" font-family=\"Times,serif\" font-size=\"14.00\">eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;19 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>13&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1306,-290.95C1456.5,-262.31 1888.87,-180.04 2089.39,-141.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2089.75,-145.19 2098.92,-139.88 2088.45,-138.31 2089.75,-145.19\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>20</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2666.71\" cy=\"-122.02\" rx=\"48.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2666.71\" y=\"-116.97\" font-family=\"Times,serif\" font-size=\"14.00\">Prediction</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2582.25,-122.02C2590.96,-122.02 2599.2,-122.02 2606.84,-122.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2606.62,-125.52 2616.62,-122.02 2606.62,-118.52 2606.62,-125.52\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-355.02\" rx=\"258.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-349.97\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M509.55,-355.02C517.39,-355.02 525.48,-355.02 533.73,-355.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"533.67,-358.52 543.67,-355.02 533.67,-351.52 533.67,-358.52\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;13 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M955.23,-340.1C990.1,-336.39 1027.03,-332.26 1061.33,-328.02 1102.35,-322.95 1148.24,-316.44 1184.62,-311.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1184.72,-314.46 1194.1,-309.53 1183.69,-307.53 1184.72,-314.46\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-301.02\" rx=\"258.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-295.97\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M509.55,-301.02C517.39,-301.02 525.48,-301.02 533.73,-301.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"533.67,-304.52 543.67,-301.02 533.67,-297.52 533.67,-304.52\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;13 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1061.56,-301.02C1103.74,-301.02 1144.27,-301.02 1176.94,-301.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1176.72,-304.52 1186.71,-301.02 1176.71,-297.52 1176.72,-304.52\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-247.02\" rx=\"258.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-241.97\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M509.55,-247.02C517.39,-247.02 525.48,-247.02 533.73,-247.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"533.67,-250.52 543.67,-247.02 533.67,-243.52 533.67,-250.52\"/>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M955.23,-261.94C990.1,-265.65 1027.03,-269.78 1061.33,-274.02 1102.35,-279.09 1148.24,-285.6 1184.62,-290.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1183.69,-294.5 1194.1,-292.5 1184.72,-287.58 1183.69,-294.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-155.02\" rx=\"101.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-149.97\" font-family=\"Times,serif\" font-size=\"14.00\">Missing Values Imputed</text>\n",
       "</g>\n",
       "<!-- &#45;2&#45;&gt;2 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>&#45;2&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243.98,-136.49C262.62,-138.73 283.06,-141.19 302.78,-143.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.29,-147.15 312.64,-144.87 303.13,-140.2 302.29,-147.15\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"398.87\" cy=\"-101.02\" rx=\"101.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"398.87\" y=\"-95.97\" font-family=\"Times,serif\" font-size=\"14.00\">Missing Values Imputed</text>\n",
       "</g>\n",
       "<!-- &#45;2&#45;&gt;14 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>&#45;2&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243.98,-119.55C262.62,-117.3 283.06,-114.84 302.78,-112.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.13,-115.83 312.64,-111.16 302.29,-108.88 303.13,-115.83\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-193.02\" rx=\"92.81\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-187.97\" font-family=\"Times,serif\" font-size=\"14.00\">Search for differences</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M489.41,-163.47C554.45,-169.62 642.26,-177.91 708.28,-184.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"707.82,-187.71 718.1,-185.17 708.48,-180.74 707.82,-187.71\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1252.06\" cy=\"-151.02\" rx=\"64.15\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1252.06\" y=\"-145.97\" font-family=\"Times,serif\" font-size=\"14.00\">Bind branches</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M500.44,-154.55C673.57,-153.73 1024.04,-152.08 1176.93,-151.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1176.61,-154.86 1186.59,-151.32 1176.58,-147.86 1176.61,-154.86\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M887.47,-185.19C971.64,-177.28 1100.32,-165.19 1179.88,-157.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1180.14,-161.11 1189.77,-156.68 1179.49,-154.14 1180.14,-161.11\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>4&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1316.25,-149.07C1426.13,-145.68 1659.68,-138.48 1858.23,-132.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1858.26,-135.83 1868.15,-132.03 1858.05,-128.84 1858.26,-135.83\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-101.02\" rx=\"54.42\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-95.97\" font-family=\"Times,serif\" font-size=\"14.00\">Standardize</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M500.88,-101.02C575.14,-101.02 673.86,-101.02 737.72,-101.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"737.44,-104.52 747.44,-101.02 737.44,-97.52 737.44,-104.52\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1252.06\" cy=\"-97.02\" rx=\"154.74\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1252.06\" y=\"-91.97\" font-family=\"Times,serif\" font-size=\"14.00\">Partial Principal Components Analysis</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M858.11,-100.54C914.65,-100.03 1006.32,-99.21 1086.9,-98.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1086.72,-101.98 1096.69,-98.39 1086.66,-94.98 1086.72,-101.98\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>18</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1607.26\" cy=\"-99.02\" rx=\"86.67\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1607.26\" y=\"-93.97\" font-family=\"Times,serif\" font-size=\"14.00\">K&#45;Means Clustering</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1406.95,-97.89C1441.86,-98.09 1477.97,-98.29 1509.71,-98.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1509.27,-101.97 1519.29,-98.53 1509.31,-94.97 1509.27,-101.97\"/>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1692.78,-102.34C1744.3,-104.36 1813.64,-107.08 1884.3,-109.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1884.06,-113.39 1894.19,-110.29 1884.33,-106.4 1884.06,-113.39\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1607.26\" cy=\"-18.02\" rx=\"164.46\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1607.26\" y=\"-12.97\" font-family=\"Times,serif\" font-size=\"14.00\">Ordinal encoding of categorical variables</text>\n",
       "</g>\n",
       "<!-- &#45;1&#45;&gt;1 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>&#45;1&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.58,-33.38C485.68,-28.08 516.74,-22.66 545.08,-20.02 864.74,9.75 1240.91,-0.08 1448.55,-9.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1448.38,-12.94 1458.53,-9.89 1448.7,-5.94 1448.38,-12.94\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"803.21\" cy=\"-47.02\" rx=\"81.04\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"803.21\" y=\"-41.97\" font-family=\"Times,serif\" font-size=\"14.00\">One&#45;Hot Encoding</text>\n",
       "</g>\n",
       "<!-- &#45;1&#45;&gt;16 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>&#45;1&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M488.67,-47.02C554.65,-47.02 644.38,-47.02 710.94,-47.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"710.84,-50.52 720.84,-47.02 710.84,-43.52 710.84,-50.52\"/>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;19 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>1&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1695.93,-33.59C1799.08,-51.91 1970.5,-82.35 2082.83,-102.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2082.06,-105.9 2092.52,-104.2 2083.28,-99.01 2082.06,-105.9\"/>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M877.67,-54.42C929.11,-59.67 999.45,-67.02 1061.33,-74.02 1084.63,-76.65 1109.53,-79.59 1133.36,-82.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1132.88,-86.04 1143.23,-83.76 1133.72,-79.09 1132.88,-86.04\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7f93fea06d60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpChart = model.blueprint.get_chart()\n",
    "print(bpChart)\n",
    "src = graphviz.Source(bpChart.to_graphviz())\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd445f-997c-42a0-b9e2-28c52dd2cd6f",
   "metadata": {},
   "source": [
    "To further understand what is happening above:\n",
    "\n",
    "- Each of the four text variables in the feature list used by the model is being passed through an Auto-Tuned Word N-Gram Text Modeler, which fits a single-word n-gram model to each text feature in the input dataset, then uses the predictions from these models as inputs to an ElasticNet classifier. Further post-processing optimizes the weights of the text-vector via grid search.\n",
    "\n",
    "- Numeric variables go through two paths:\n",
    "    - A greedy search for differences between columns to identify new features.\n",
    "    - Standardizaztion by removing the median and scaling to unit variance or mean absolute deviation. Scaled features are combined with one-hot encoded categoricals, which produces a wide matrix that is fed into PCA, then into k-means to identify clusters of latent features.\n",
    "    \n",
    "- Categoricals go through the above path into PCA, as well as through ordinal encoding in a separate path.\n",
    "\n",
    "Each of these paths are fed into XGBoost. You can also access documentation around each process using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b20945-f710-43bd-9975-0e36ba968b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "references for {}: Truncated SVD tasks\n",
      "0 - sklearn TruncatedSVD: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
      "1 - sklearn decomposition (matrix factorization) user guide: http://scikit-learn.org/stable/modules/decomposition.html#lsa\n"
     ]
    }
   ],
   "source": [
    "bpDoc = model.blueprint.get_documents()[-1]\n",
    "print(\"references for {}:\", format(bpDoc.task))\n",
    "for i, link in enumerate(bpDoc.links):\n",
    "    print(\"{} - {}: {}\".format(i, link[\"name\"], link[\"url\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9563dd60-541e-428f-a816-064febb59de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features\n",
      "processes:\n",
      "0 - Ordinal encoding of categorical variables\n",
      "1 - Missing Values Imputed\n",
      "2 - Search for differences\n",
      "3 - Converter for Text Mining\n",
      "4 - Auto-Tuned Word N-Gram Text Modeler using token occurrences\n",
      "5 - Standardize\n",
      "6 - One-Hot Encoding\n",
      "7 - Partial Principal Components Analysis\n",
      "8 - K-Means Clustering\n",
      "9 - eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features\n"
     ]
    }
   ],
   "source": [
    "print(\"model type: {}\".format(model.model_type))\n",
    "print(\"processes:\")\n",
    "for i, p in enumerate(model.blueprint.processes):\n",
    "    print(\"{} - {}\".format(i, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889bff7-ae81-46bc-abf4-63b213e4475d",
   "metadata": {},
   "source": [
    "### Extract tunable parameters as a dataframe. \n",
    "\n",
    "The function below returns all hyperparameters as a dataframe. Note- text pipelines are applied to each column of text, thus duplicate parameter names can exist.  `Keep duplicates=False` hides these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84f33a9-7da8-43cf-a66f-0f4339785199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>parameter_name</th>\n",
       "      <th>parameter_name_type</th>\n",
       "      <th>current_value</th>\n",
       "      <th>default_value</th>\n",
       "      <th>param_type</th>\n",
       "      <th>supports_grid_search</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>values</th>\n",
       "      <th>parameter_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>analyzer</td>\n",
       "      <td>analyzer_select</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[word, char]</td>\n",
       "      <td>eyJhcmciOiJhbmFseXplciIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>min_df</td>\n",
       "      <td>min_df_int</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fZGYiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_marisa</td>\n",
       "      <td>use_marisa_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfbWFyaXNhIiwidmlkIjoiOCJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>min_tc</td>\n",
       "      <td>min_tc_int</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fdGMiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>n_features</td>\n",
       "      <td>n_features_int</td>\n",
       "      <td>262144</td>\n",
       "      <td>262144</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1048576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJuX2ZlYXR1cmVzIiwidmlkIjoiMTIifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>norm</td>\n",
       "      <td>norm_select</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, l1, l2]</td>\n",
       "      <td>eyJhcmciOiJub3JtIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>segmenter</td>\n",
       "      <td>segmenter_select</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, japanese]</td>\n",
       "      <td>eyJhcmciOiJzZWdtZW50ZXIiLCJ2aWQiOiI4In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>smooth_idf</td>\n",
       "      <td>smooth_idf_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJzbW9vdGhfaWRmIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>stop_words</td>\n",
       "      <td>stop_words_select</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True, english]</td>\n",
       "      <td>eyJhcmciOiJzdG9wX3dvcmRzIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>sublinear_tf</td>\n",
       "      <td>sublinear_tf_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJzdWJsaW5lYXJfdGYiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>tokenizer</td>\n",
       "      <td>tokenizer_select</td>\n",
       "      <td>sklearn_tokenizer</td>\n",
       "      <td>sklearn_tokenizer</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sklearn_tokenizer, space, wordpunct, tweet, t...</td>\n",
       "      <td>eyJhcmciOiJ0b2tlbml6ZXIiLCJ2aWQiOiIxMiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_bns</td>\n",
       "      <td>use_bns_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfYm5zIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_char_preprocessing</td>\n",
       "      <td>use_char_preprocessing_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfY2hhcl9wcmVwcm9jZXNzaW5nIiwidm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_delta</td>\n",
       "      <td>use_delta_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfZGVsdGEiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_hashing</td>\n",
       "      <td>use_hashing_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfaGFzaGluZyIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_idf</td>\n",
       "      <td>use_idf_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfaWRmIiwidmlkIjoiOCJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>max_samples</td>\n",
       "      <td>max_samples_select</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None]</td>\n",
       "      <td>eyJhcmciOiJtYXhfc2FtcGxlcyIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>max_df</td>\n",
       "      <td>max_df_int</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfZGYiLCJ2aWQiOiIxMiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_term_frequency</td>\n",
       "      <td>use_term_frequency_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfdGVybV9mcmVxdWVuY3kiLCJ2aWQiOi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>binary</td>\n",
       "      <td>binary_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJiaW5hcnkiLCJ2aWQiOiI4In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>decode_error</td>\n",
       "      <td>decode_error_select</td>\n",
       "      <td>replace</td>\n",
       "      <td>replace</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[strict, ignore, replace]</td>\n",
       "      <td>eyJhcmciOiJkZWNvZGVfZXJyb3IiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>encoding</td>\n",
       "      <td>encoding_select</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[utf-8, latin-1]</td>\n",
       "      <td>eyJhcmciOiJlbmNvZGluZyIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>lowercase_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJsb3dlcmNhc2UiLCJ2aWQiOiIxMiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>include_lower</td>\n",
       "      <td>include_lower_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJpbmNsdWRlX2xvd2VyIiwidmlkIjoiOCJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>language</td>\n",
       "      <td>language_select</td>\n",
       "      <td>english</td>\n",
       "      <td>english</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[arabic, chinese, danish, dutch, english, finn...</td>\n",
       "      <td>eyJhcmciOiJsYW5ndWFnZSIsInZpZCI6IjYifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>K-Means Clustering</td>\n",
       "      <td>n_clusters</td>\n",
       "      <td>n_clusters_select</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[auto, pham]</td>\n",
       "      <td>eyJhcmciOiJuX2NsdXN0ZXJzIiwidmlkIjoiMTgifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Missing Values Imputed</td>\n",
       "      <td>arbimp</td>\n",
       "      <td>arbimp_int</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>-99999.0000</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJhcmJpbXAiLCJ2aWQiOiIyIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missing Values Imputed</td>\n",
       "      <td>min_count_na</td>\n",
       "      <td>min_count_na_int</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fY291bnRfbmEiLCJ2aWQiOiIyIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>One-Hot Encoding</td>\n",
       "      <td>max_features</td>\n",
       "      <td>max_features_select</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None]</td>\n",
       "      <td>eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIxNiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ordinal encoding of categorical variables</td>\n",
       "      <td>card_max</td>\n",
       "      <td>card_max_select</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None]</td>\n",
       "      <td>eyJhcmciOiJjYXJkX21heCIsInZpZCI6IjEifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ordinal encoding of categorical variables</td>\n",
       "      <td>min_support</td>\n",
       "      <td>min_support_int</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjEifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ordinal encoding of categorical variables</td>\n",
       "      <td>method</td>\n",
       "      <td>method_select</td>\n",
       "      <td>freq</td>\n",
       "      <td>freq</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, random, lex, freq, resp]</td>\n",
       "      <td>eyJhcmciOiJtZXRob2QiLCJ2aWQiOiIxIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Partial Principal Components Analysis</td>\n",
       "      <td>k</td>\n",
       "      <td>k_select</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[auto]</td>\n",
       "      <td>eyJhcmciOiJrIiwidmlkIjoiMTcifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value_float</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>float</td>\n",
       "      <td>False</td>\n",
       "      <td>-100000.0000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaXNzaW5nX3ZhbHVlIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>max_delta_step</td>\n",
       "      <td>max_delta_step_float</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfZGVsdGFfc3RlcCIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>base_margin_initialize</td>\n",
       "      <td>base_margin_initialize_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJiYXNlX21hcmdpbl9pbml0aWFsaXplIiwidm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>min_child_weight</td>\n",
       "      <td>min_child_weight_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fY2hpbGRfd2VpZ2h0IiwidmlkIjoiMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>colsample_bylevel</td>\n",
       "      <td>colsample_bylevel_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJjb2xzYW1wbGVfYnlsZXZlbCIsInZpZCI6Ij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>tree_method</td>\n",
       "      <td>tree_method_select</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[auto, exact, approx, hist]</td>\n",
       "      <td>eyJhcmciOiJ0cmVlX21ldGhvZCIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>colsample_bytree</td>\n",
       "      <td>colsample_bytree_float</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJjb2xzYW1wbGVfYnl0cmVlIiwidmlkIjoiMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>subsample</td>\n",
       "      <td>subsample_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJzdWJzYW1wbGUiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>max_bin</td>\n",
       "      <td>max_bin_int</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfYmluIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>min_split_loss</td>\n",
       "      <td>min_split_loss_float</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fc3BsaXRfbG9zcyIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>smooth_interval</td>\n",
       "      <td>smooth_interval_int</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJzbW9vdGhfaW50ZXJ2YWwiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>learning_rate_float</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJsZWFybmluZ19yYXRlIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>interval</td>\n",
       "      <td>interval_int</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJpbnRlcnZhbCIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>scale_pos_weight</td>\n",
       "      <td>scale_pos_weight_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJzY2FsZV9wb3Nfd2VpZ2h0IiwidmlkIjoiMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>reg_lambda</td>\n",
       "      <td>reg_lambda_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJyZWdfbGFtYmRhIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>max_depth</td>\n",
       "      <td>max_depth_int</td>\n",
       "      <td>[3, 5, 7]</td>\n",
       "      <td>3</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfZGVwdGgiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>random_state</td>\n",
       "      <td>random_state_int</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJyYW5kb21fc3RhdGUiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>num_parallel_tree</td>\n",
       "      <td>num_parallel_tree_int</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJudW1fcGFyYWxsZWxfdHJlZSIsInZpZCI6Ij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>n_estimators</td>\n",
       "      <td>n_estimators_int</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJuX2VzdGltYXRvcnMiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>reg_alpha</td>\n",
       "      <td>reg_alpha_float</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJyZWdfYWxwaGEiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            task_name          parameter_name  \\\n",
       "0   Auto-Tuned Word N-Gram Text Modeler using toke...                analyzer   \n",
       "24  Auto-Tuned Word N-Gram Text Modeler using toke...                  min_df   \n",
       "51  Auto-Tuned Word N-Gram Text Modeler using toke...              use_marisa   \n",
       "27  Auto-Tuned Word N-Gram Text Modeler using toke...                  min_tc   \n",
       "31  Auto-Tuned Word N-Gram Text Modeler using toke...              n_features   \n",
       "32  Auto-Tuned Word N-Gram Text Modeler using toke...                    norm   \n",
       "38  Auto-Tuned Word N-Gram Text Modeler using toke...               segmenter   \n",
       "39  Auto-Tuned Word N-Gram Text Modeler using toke...              smooth_idf   \n",
       "41  Auto-Tuned Word N-Gram Text Modeler using toke...              stop_words   \n",
       "42  Auto-Tuned Word N-Gram Text Modeler using toke...            sublinear_tf   \n",
       "44  Auto-Tuned Word N-Gram Text Modeler using toke...               tokenizer   \n",
       "46  Auto-Tuned Word N-Gram Text Modeler using toke...                 use_bns   \n",
       "47  Auto-Tuned Word N-Gram Text Modeler using toke...  use_char_preprocessing   \n",
       "48  Auto-Tuned Word N-Gram Text Modeler using toke...               use_delta   \n",
       "49  Auto-Tuned Word N-Gram Text Modeler using toke...             use_hashing   \n",
       "50  Auto-Tuned Word N-Gram Text Modeler using toke...                 use_idf   \n",
       "20  Auto-Tuned Word N-Gram Text Modeler using toke...             max_samples   \n",
       "18  Auto-Tuned Word N-Gram Text Modeler using toke...                  max_df   \n",
       "52  Auto-Tuned Word N-Gram Text Modeler using toke...      use_term_frequency   \n",
       "3   Auto-Tuned Word N-Gram Text Modeler using toke...                  binary   \n",
       "7   Auto-Tuned Word N-Gram Text Modeler using toke...            decode_error   \n",
       "8   Auto-Tuned Word N-Gram Text Modeler using toke...                encoding   \n",
       "14  Auto-Tuned Word N-Gram Text Modeler using toke...               lowercase   \n",
       "9   Auto-Tuned Word N-Gram Text Modeler using toke...           include_lower   \n",
       "12  Auto-Tuned Word N-Gram Text Modeler using toke...                language   \n",
       "29                                 K-Means Clustering              n_clusters   \n",
       "1                              Missing Values Imputed                  arbimp   \n",
       "23                             Missing Values Imputed            min_count_na   \n",
       "19                                   One-Hot Encoding            max_features   \n",
       "4           Ordinal encoding of categorical variables                card_max   \n",
       "26          Ordinal encoding of categorical variables             min_support   \n",
       "21          Ordinal encoding of categorical variables                  method   \n",
       "11              Partial Principal Components Analysis                       k   \n",
       "28  eXtreme Gradient Boosted Trees Classifier with...           missing_value   \n",
       "16  eXtreme Gradient Boosted Trees Classifier with...          max_delta_step   \n",
       "2   eXtreme Gradient Boosted Trees Classifier with...  base_margin_initialize   \n",
       "22  eXtreme Gradient Boosted Trees Classifier with...        min_child_weight   \n",
       "5   eXtreme Gradient Boosted Trees Classifier with...       colsample_bylevel   \n",
       "45  eXtreme Gradient Boosted Trees Classifier with...             tree_method   \n",
       "6   eXtreme Gradient Boosted Trees Classifier with...        colsample_bytree   \n",
       "43  eXtreme Gradient Boosted Trees Classifier with...               subsample   \n",
       "15  eXtreme Gradient Boosted Trees Classifier with...                 max_bin   \n",
       "25  eXtreme Gradient Boosted Trees Classifier with...          min_split_loss   \n",
       "40  eXtreme Gradient Boosted Trees Classifier with...         smooth_interval   \n",
       "13  eXtreme Gradient Boosted Trees Classifier with...           learning_rate   \n",
       "10  eXtreme Gradient Boosted Trees Classifier with...                interval   \n",
       "37  eXtreme Gradient Boosted Trees Classifier with...        scale_pos_weight   \n",
       "36  eXtreme Gradient Boosted Trees Classifier with...              reg_lambda   \n",
       "17  eXtreme Gradient Boosted Trees Classifier with...               max_depth   \n",
       "34  eXtreme Gradient Boosted Trees Classifier with...            random_state   \n",
       "33  eXtreme Gradient Boosted Trees Classifier with...       num_parallel_tree   \n",
       "30  eXtreme Gradient Boosted Trees Classifier with...            n_estimators   \n",
       "35  eXtreme Gradient Boosted Trees Classifier with...               reg_alpha   \n",
       "\n",
       "              parameter_name_type      current_value      default_value  \\\n",
       "0                 analyzer_select               word               word   \n",
       "24                     min_df_int                  2                  2   \n",
       "51              use_marisa_select               True               True   \n",
       "27                     min_tc_int                  0                  0   \n",
       "31                 n_features_int             262144             262144   \n",
       "32                    norm_select                 l2                 l2   \n",
       "38               segmenter_select               None               None   \n",
       "39              smooth_idf_select               True               True   \n",
       "41              stop_words_select                  0                  0   \n",
       "42            sublinear_tf_select              False              False   \n",
       "44               tokenizer_select  sklearn_tokenizer  sklearn_tokenizer   \n",
       "46                 use_bns_select              False              False   \n",
       "47  use_char_preprocessing_select               True               True   \n",
       "48               use_delta_select              False              False   \n",
       "49             use_hashing_select              False              False   \n",
       "50                 use_idf_select              False              False   \n",
       "20             max_samples_select                500                500   \n",
       "18                     max_df_int                0.8                0.8   \n",
       "52      use_term_frequency_select               True               True   \n",
       "3                   binary_select               True               True   \n",
       "7             decode_error_select            replace            replace   \n",
       "8                 encoding_select              utf-8              utf-8   \n",
       "14               lowercase_select               True               True   \n",
       "9            include_lower_select               True               True   \n",
       "12                language_select            english            english   \n",
       "29              n_clusters_select               auto               auto   \n",
       "1                      arbimp_int              -9999              -9999   \n",
       "23               min_count_na_int                  5                  5   \n",
       "19            max_features_select               None               None   \n",
       "4                 card_max_select               None               None   \n",
       "26                min_support_int                  5                  5   \n",
       "21                  method_select               freq               freq   \n",
       "11                       k_select               auto               auto   \n",
       "28            missing_value_float              -9999              -9999   \n",
       "16           max_delta_step_float                  0                  0   \n",
       "2   base_margin_initialize_select              False              False   \n",
       "22         min_child_weight_float                  1                  1   \n",
       "5         colsample_bylevel_float                  1                  1   \n",
       "45             tree_method_select               auto               auto   \n",
       "6          colsample_bytree_float                0.3                0.3   \n",
       "43                subsample_float                  1                  1   \n",
       "15                    max_bin_int                256                256   \n",
       "25           min_split_loss_float               0.01               0.01   \n",
       "40            smooth_interval_int                200                200   \n",
       "13            learning_rate_float               0.05               0.05   \n",
       "10                   interval_int                 10                 10   \n",
       "37         scale_pos_weight_float                  1                  1   \n",
       "36               reg_lambda_float                  1                  1   \n",
       "17                  max_depth_int          [3, 5, 7]                  3   \n",
       "34               random_state_int                153                153   \n",
       "33          num_parallel_tree_int                  1                  1   \n",
       "30               n_estimators_int               2500               2500   \n",
       "35                reg_alpha_float                  0                  0   \n",
       "\n",
       "   param_type supports_grid_search          min  \\\n",
       "0      select                  NaN          NaN   \n",
       "24        int                False       0.0000   \n",
       "51     select                  NaN          NaN   \n",
       "27        int                False       0.0000   \n",
       "31        int                False       1.0000   \n",
       "32     select                  NaN          NaN   \n",
       "38     select                  NaN          NaN   \n",
       "39     select                  NaN          NaN   \n",
       "41     select                  NaN          NaN   \n",
       "42     select                  NaN          NaN   \n",
       "44     select                  NaN          NaN   \n",
       "46     select                  NaN          NaN   \n",
       "47     select                  NaN          NaN   \n",
       "48     select                  NaN          NaN   \n",
       "49     select                  NaN          NaN   \n",
       "50     select                  NaN          NaN   \n",
       "20     select                  NaN          NaN   \n",
       "18        int                False       0.0000   \n",
       "52     select                  NaN          NaN   \n",
       "3      select                  NaN          NaN   \n",
       "7      select                  NaN          NaN   \n",
       "8      select                  NaN          NaN   \n",
       "14     select                  NaN          NaN   \n",
       "9      select                  NaN          NaN   \n",
       "12     select                  NaN          NaN   \n",
       "29     select                  NaN          NaN   \n",
       "1         int                False  -99999.0000   \n",
       "23        int                False       0.0000   \n",
       "19     select                  NaN          NaN   \n",
       "4      select                  NaN          NaN   \n",
       "26        int                False       1.0000   \n",
       "21     select                  NaN          NaN   \n",
       "11     select                  NaN          NaN   \n",
       "28      float                False -100000.0000   \n",
       "16      float                 True       0.0000   \n",
       "2      select                  NaN          NaN   \n",
       "22      float                 True       0.0100   \n",
       "5       float                 True       0.1000   \n",
       "45     select                  NaN          NaN   \n",
       "6       float                 True       0.0300   \n",
       "43      float                 True       0.0001   \n",
       "15        int                 True      16.0000   \n",
       "25      float                 True       0.0000   \n",
       "40        int                False       2.0000   \n",
       "13      float                 True       0.0005   \n",
       "10        int                False       2.0000   \n",
       "37      float                 True       0.0000   \n",
       "36      float                 True       0.0000   \n",
       "17        int                 True       1.0000   \n",
       "34        int                 True       0.0000   \n",
       "33        int                 True       1.0000   \n",
       "30        int                False       1.0000   \n",
       "35      float                 True       0.0000   \n",
       "\n",
       "                                max  \\\n",
       "0                               NaN   \n",
       "24  1000000000000000019884624838656   \n",
       "51                              NaN   \n",
       "27                              999   \n",
       "31                          1048576   \n",
       "32                              NaN   \n",
       "38                              NaN   \n",
       "39                              NaN   \n",
       "41                              NaN   \n",
       "42                              NaN   \n",
       "44                              NaN   \n",
       "46                              NaN   \n",
       "47                              NaN   \n",
       "48                              NaN   \n",
       "49                              NaN   \n",
       "50                              NaN   \n",
       "20                              NaN   \n",
       "18  1000000000000000019884624838656   \n",
       "52                              NaN   \n",
       "3                               NaN   \n",
       "7                               NaN   \n",
       "8                               NaN   \n",
       "14                              NaN   \n",
       "9                               NaN   \n",
       "12                              NaN   \n",
       "29                              NaN   \n",
       "1                             99999   \n",
       "23                            99999   \n",
       "19                              NaN   \n",
       "4                               NaN   \n",
       "26                            99999   \n",
       "21                              NaN   \n",
       "11                              NaN   \n",
       "28                         100000.0   \n",
       "16                            100.0   \n",
       "2                               NaN   \n",
       "22                         100000.0   \n",
       "5                               1.0   \n",
       "45                              NaN   \n",
       "6                               1.0   \n",
       "43                              1.0   \n",
       "15                             2048   \n",
       "25                         100000.0   \n",
       "40                             1000   \n",
       "13                              1.0   \n",
       "10                              500   \n",
       "37                     1000000000.0   \n",
       "36                        1000000.0   \n",
       "17                               16   \n",
       "34                       1000000000   \n",
       "33                               16   \n",
       "30                            20000   \n",
       "35                        1000000.0   \n",
       "\n",
       "                                               values  \\\n",
       "0                                        [word, char]   \n",
       "24                                                NaN   \n",
       "51                                      [False, True]   \n",
       "27                                                NaN   \n",
       "31                                                NaN   \n",
       "32                                     [None, l1, l2]   \n",
       "38                                   [None, japanese]   \n",
       "39                                      [False, True]   \n",
       "41                             [False, True, english]   \n",
       "42                                      [False, True]   \n",
       "44  [sklearn_tokenizer, space, wordpunct, tweet, t...   \n",
       "46                                      [False, True]   \n",
       "47                                      [False, True]   \n",
       "48                                      [False, True]   \n",
       "49                                      [False, True]   \n",
       "50                                      [False, True]   \n",
       "20                                             [None]   \n",
       "18                                                NaN   \n",
       "52                                      [False, True]   \n",
       "3                                       [False, True]   \n",
       "7                           [strict, ignore, replace]   \n",
       "8                                    [utf-8, latin-1]   \n",
       "14                                      [False, True]   \n",
       "9                                       [False, True]   \n",
       "12  [arabic, chinese, danish, dutch, english, finn...   \n",
       "29                                       [auto, pham]   \n",
       "1                                                 NaN   \n",
       "23                                                NaN   \n",
       "19                                             [None]   \n",
       "4                                              [None]   \n",
       "26                                                NaN   \n",
       "21                    [None, random, lex, freq, resp]   \n",
       "11                                             [auto]   \n",
       "28                                                NaN   \n",
       "16                                                NaN   \n",
       "2                                       [False, True]   \n",
       "22                                                NaN   \n",
       "5                                                 NaN   \n",
       "45                        [auto, exact, approx, hist]   \n",
       "6                                                 NaN   \n",
       "43                                                NaN   \n",
       "15                                                NaN   \n",
       "25                                                NaN   \n",
       "40                                                NaN   \n",
       "13                                                NaN   \n",
       "10                                                NaN   \n",
       "37                                                NaN   \n",
       "36                                                NaN   \n",
       "17                                                NaN   \n",
       "34                                                NaN   \n",
       "33                                                NaN   \n",
       "30                                                NaN   \n",
       "35                                                NaN   \n",
       "\n",
       "                                         parameter_id  \n",
       "0             eyJhcmciOiJhbmFseXplciIsInZpZCI6IjEwIn0  \n",
       "24                eyJhcmciOiJtaW5fZGYiLCJ2aWQiOiI2In0  \n",
       "51           eyJhcmciOiJ1c2VfbWFyaXNhIiwidmlkIjoiOCJ9  \n",
       "27                eyJhcmciOiJtaW5fdGMiLCJ2aWQiOiI2In0  \n",
       "31         eyJhcmciOiJuX2ZlYXR1cmVzIiwidmlkIjoiMTIifQ  \n",
       "32                 eyJhcmciOiJub3JtIiwidmlkIjoiMTAifQ  \n",
       "38            eyJhcmciOiJzZWdtZW50ZXIiLCJ2aWQiOiI4In0  \n",
       "39         eyJhcmciOiJzbW9vdGhfaWRmIiwidmlkIjoiMTAifQ  \n",
       "41         eyJhcmciOiJzdG9wX3dvcmRzIiwidmlkIjoiMTAifQ  \n",
       "42        eyJhcmciOiJzdWJsaW5lYXJfdGYiLCJ2aWQiOiI2In0  \n",
       "44           eyJhcmciOiJ0b2tlbml6ZXIiLCJ2aWQiOiIxMiJ9  \n",
       "46             eyJhcmciOiJ1c2VfYm5zIiwidmlkIjoiMTAifQ  \n",
       "47  eyJhcmciOiJ1c2VfY2hhcl9wcmVwcm9jZXNzaW5nIiwidm...  \n",
       "48            eyJhcmciOiJ1c2VfZGVsdGEiLCJ2aWQiOiI2In0  \n",
       "49        eyJhcmciOiJ1c2VfaGFzaGluZyIsInZpZCI6IjEwIn0  \n",
       "50               eyJhcmciOiJ1c2VfaWRmIiwidmlkIjoiOCJ9  \n",
       "20        eyJhcmciOiJtYXhfc2FtcGxlcyIsInZpZCI6IjEwIn0  \n",
       "18               eyJhcmciOiJtYXhfZGYiLCJ2aWQiOiIxMiJ9  \n",
       "52  eyJhcmciOiJ1c2VfdGVybV9mcmVxdWVuY3kiLCJ2aWQiOi...  \n",
       "3                 eyJhcmciOiJiaW5hcnkiLCJ2aWQiOiI4In0  \n",
       "7         eyJhcmciOiJkZWNvZGVfZXJyb3IiLCJ2aWQiOiI2In0  \n",
       "8             eyJhcmciOiJlbmNvZGluZyIsInZpZCI6IjEwIn0  \n",
       "14           eyJhcmciOiJsb3dlcmNhc2UiLCJ2aWQiOiIxMiJ9  \n",
       "9        eyJhcmciOiJpbmNsdWRlX2xvd2VyIiwidmlkIjoiOCJ9  \n",
       "12             eyJhcmciOiJsYW5ndWFnZSIsInZpZCI6IjYifQ  \n",
       "29         eyJhcmciOiJuX2NsdXN0ZXJzIiwidmlkIjoiMTgifQ  \n",
       "1                 eyJhcmciOiJhcmJpbXAiLCJ2aWQiOiIyIn0  \n",
       "23        eyJhcmciOiJtaW5fY291bnRfbmEiLCJ2aWQiOiIyIn0  \n",
       "19       eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIxNiJ9  \n",
       "4              eyJhcmciOiJjYXJkX21heCIsInZpZCI6IjEifQ  \n",
       "26         eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjEifQ  \n",
       "21                eyJhcmciOiJtZXRob2QiLCJ2aWQiOiIxIn0  \n",
       "11                     eyJhcmciOiJrIiwidmlkIjoiMTcifQ  \n",
       "28     eyJhcmciOiJtaXNzaW5nX3ZhbHVlIiwidmlkIjoiMTkifQ  \n",
       "16    eyJhcmciOiJtYXhfZGVsdGFfc3RlcCIsInZpZCI6IjE5In0  \n",
       "2   eyJhcmciOiJiYXNlX21hcmdpbl9pbml0aWFsaXplIiwidm...  \n",
       "22  eyJhcmciOiJtaW5fY2hpbGRfd2VpZ2h0IiwidmlkIjoiMT...  \n",
       "5   eyJhcmciOiJjb2xzYW1wbGVfYnlsZXZlbCIsInZpZCI6Ij...  \n",
       "45        eyJhcmciOiJ0cmVlX21ldGhvZCIsInZpZCI6IjE5In0  \n",
       "6   eyJhcmciOiJjb2xzYW1wbGVfYnl0cmVlIiwidmlkIjoiMT...  \n",
       "43           eyJhcmciOiJzdWJzYW1wbGUiLCJ2aWQiOiIxOSJ9  \n",
       "15             eyJhcmciOiJtYXhfYmluIiwidmlkIjoiMTkifQ  \n",
       "25    eyJhcmciOiJtaW5fc3BsaXRfbG9zcyIsInZpZCI6IjE5In0  \n",
       "40   eyJhcmciOiJzbW9vdGhfaW50ZXJ2YWwiLCJ2aWQiOiIxOSJ9  \n",
       "13     eyJhcmciOiJsZWFybmluZ19yYXRlIiwidmlkIjoiMTkifQ  \n",
       "10            eyJhcmciOiJpbnRlcnZhbCIsInZpZCI6IjE5In0  \n",
       "37  eyJhcmciOiJzY2FsZV9wb3Nfd2VpZ2h0IiwidmlkIjoiMT...  \n",
       "36         eyJhcmciOiJyZWdfbGFtYmRhIiwidmlkIjoiMTkifQ  \n",
       "17           eyJhcmciOiJtYXhfZGVwdGgiLCJ2aWQiOiIxOSJ9  \n",
       "34       eyJhcmciOiJyYW5kb21fc3RhdGUiLCJ2aWQiOiIxOSJ9  \n",
       "33  eyJhcmciOiJudW1fcGFyYWxsZWxfdHJlZSIsInZpZCI6Ij...  \n",
       "30       eyJhcmciOiJuX2VzdGltYXRvcnMiLCJ2aWQiOiIxOSJ9  \n",
       "35           eyJhcmciOiJyZWdfYWxwaGEiLCJ2aWQiOiIxOSJ9  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_to_df(model.get_advanced_tuning_parameters(), keep_duplicates=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d8862",
   "metadata": {},
   "source": [
    "## Use hyperopt's Bayesian tuning approach\n",
    "The hyperopt package allows one to use the Tree of Parzen Estimators (Bayesian) approach to hyperparameter tuning, and you can leverage this approach with DataRobot through the API. The objective function passed into hyperopt's optimization functions is very flexible, allowing you to fit models in DataRobot and bring the automatically calculated validation, cross validation, or holdout metrics back into hyperopt for any metric available in the DataRobot project. For custom metrics not available in DataRobot, you would request the training predictions through the DataRobot API in the `objective()` function and calculate those yourself.  \n",
    "<br>\n",
    "### Objective function\n",
    "First, define that objective function. As noted above, this hyperopt objective function gives us the flexibility to identify optimal parameter combinations for metrics not native to DataRobot, but note that the blueprint itself will not directly optimize its parameters to a custom objective function, or even a native DataRobot objective function that was not the project's default. It will use the project's default objective function (logloss in this example) in the optimization of any parameters not listed in the search space.   \n",
    "<br>\n",
    "Note that Python converts integers here to numpy.int64, but DataRobot won't like that. So convert back to `int` type with `.tolist()`, as seen in line 4 of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7102f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(search_space):\n",
    "    ATS_hyperopt.set_parameter(parameter_name=\"subsample\", value=[search_space[\"subsample\"]])\n",
    "    ATS_hyperopt.set_parameter(\n",
    "        parameter_name=\"colsample_bytree\", value=[search_space[\"colsample_bytree\"]]\n",
    "    )\n",
    "    ATS_hyperopt.set_parameter(\n",
    "        parameter_name=\"max_depth\", value=[search_space[\"max_depth\"].tolist()]\n",
    "    )\n",
    "    job = ATS_hyperopt.run()\n",
    "    tuned_model = job.get_result_when_complete()\n",
    "\n",
    "    loss_metric = tuned_model.metrics[model.project.metric][\"crossValidation\"]\n",
    "    # Here you could extract the 'training predictions' from DataRobot and return your own custom metric\n",
    "    return {\"loss\": loss_metric, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37804a",
   "metadata": {},
   "source": [
    "### Search space\n",
    "\n",
    "Define the search space using the random functions from the `hp` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d15b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.25, 0.75),\n",
    "    \"max_depth\": hp.randint(\"max_depth\", 3, 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0dca7e",
   "metadata": {},
   "source": [
    "### Optimization algorithm\n",
    "Select the Tree of Parzen Estimators (tpe) tuning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc46e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7b7fd",
   "metadata": {},
   "source": [
    "### Commence tuning\n",
    "Run the models across the random search grid using the TPE approach, and record results in the `best_params` object. Print the optimized hyperparameters of the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29037126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [16:11<00:00, 97.12s/trial, best loss: 0.356186]\n",
      "CPU times: user 1.59 s, sys: 309 ms, total: 1.9 s\n",
      "Wall time: 16min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ATS_hyperopt = dr.models.advanced_tuning.AdvancedTuningSession(model)\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective, space=search_space, algo=algorithm, max_evals=10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7139ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.6216935341616984, 'max_depth': 3, 'subsample': 0.8677904050238235}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca96654",
   "metadata": {},
   "source": [
    "### Extract top models and evaluate\n",
    "\n",
    "Find top model from hyperopt tuning session, by conditioning on the model type and cross validation score. Note that the hyperopt tuning was able to improve upon the orginal top model, and that it is now the best model on the Leaderboard (among those that have been cross-validated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c17b152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top hyperopt model</td>\n",
       "      <td>0.356186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>top cross validated model</td>\n",
       "      <td>0.356186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>original recommended model</td>\n",
       "      <td>0.357984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  cv_score\n",
       "0          top hyperopt model  0.356186\n",
       "1   top cross validated model  0.356186\n",
       "2  original recommended model  0.357984"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_with_cv = [\n",
    "    m for m in project.get_models() if m.metrics[\"LogLoss\"][\"crossValidation\"] is not None\n",
    "]\n",
    "top_hyperopt_model = [\n",
    "    m\n",
    "    for m in models_with_cv\n",
    "    if (abs(m.metrics[\"LogLoss\"][\"crossValidation\"] - np.min(trials.losses())) < 0.00001)\n",
    "    & (\"Unsupervised Learning Features\" in m.model_type)\n",
    "][0]\n",
    "top_cv_model = models_with_cv[\n",
    "    np.argmin([m.metrics[\"LogLoss\"][\"crossValidation\"] for m in models_with_cv])\n",
    "]\n",
    "rec_model = dr.Model.get(project.id, project.recommended_model().parent_model_id)\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"model\": \"top hyperopt model\",\n",
    "            \"cv_score\": top_hyperopt_model.metrics[\"LogLoss\"][\"crossValidation\"],\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"top cross validated model\",\n",
    "            \"cv_score\": top_cv_model.metrics[\"LogLoss\"][\"crossValidation\"],\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"original recommended model\",\n",
    "            \"cv_score\": rec_model.metrics[\"LogLoss\"][\"crossValidation\"],\n",
    "        },\n",
    "    ]\n",
    ").sort_values(by=[\"cv_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888d349",
   "metadata": {},
   "source": [
    "Now, train the top hyperopt model to 100% of the training data to prepare it for production. First you'll confirm that the holdout has been unlocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f267f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.unlock_holdout()\n",
    "top_hyperopt_model_100_job = top_hyperopt_model.train(sample_pct=100)\n",
    "new_model = [j for j in project.get_model_jobs() if str(j.id) == str(top_hyperopt_model_100_job)][\n",
    "    0\n",
    "].get_result_when_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c739319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top hyperopt model has a LogLoss of 0.35519599999999996\n",
      "The original recommended model has a LogLoss of 0.356794\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The top hyperopt model has a LogLoss of \"\n",
    "    + str(new_model.metrics[\"LogLoss\"][\"crossValidation\"])\n",
    ")\n",
    "print(\n",
    "    \"The original recommended model has a LogLoss of \"\n",
    "    + str(project.recommended_model().metrics[\"LogLoss\"][\"crossValidation\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2298d9-9bda-4d33-8546-1a93832ac83c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "In addition to the flexible parameter tuning shown off in [the aforementioned AI Accelerator](https://github.com/datarobot-community/ai-accelerators/blob/main/advanced-experimentation/Hyperparameter_Optimization/HyperParam_Opt_Core_Concepts.ipynb), you can also leverage open source tuning modules like hyperopt with DataRobot. If you are aiming to optimize the cross-validated model scores, then even in a sequential Bayesian optimization algorithm you can still leverage DataRobot's parallelization of cross validation folds across modeling workers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
