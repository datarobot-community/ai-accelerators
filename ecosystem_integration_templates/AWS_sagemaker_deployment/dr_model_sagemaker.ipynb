{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import datarobot as dr\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some logging paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure formatting for logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"[%(asctime)s][%(name)s][%(levelname)s]: %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "log.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable configuration\n",
    "In this cell, define all of the variables and access tokens.\n",
    "  \n",
    "#### DataRobot\n",
    "`dr_host`: The DataRobot cluster you are connecting to. Please ensure that the `/api/v2` part of the URL is still in the string.  \n",
    "`api_key`: The API key of the DataRobot user used to interact with the platform.\n",
    "  \n",
    "#### AWS\n",
    "`aws_region`: The AWS region that everything will be deployed to.  \n",
    "`aws_access_key_id`: AWS Access Key for authentication.  \n",
    "`aws_secret_access_key`: AWS Secret Access Key for authentication.  \n",
    "`aws_session_token`: AWS Session Token for authentication.  \n",
    "  \n",
    "`s3_bucket`: The name of the S3 bucket that will be created for uploading your Scoring Code JAR file into.  \n",
    "`aws_ecr_repo`: The name of the ECR repo into which you upload your runtime docker image.  \n",
    "`sagemaker_execution_role_name`: The name of the IAM Role that will be created to allow SageMaker to interact with S3 and other SageMaker services.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataRobot Variables\n",
    "\n",
    "# DataRobot Host\n",
    "dr_host = \"https://app.datarobot.com/api/v2\"\n",
    "\n",
    "# DataRobot API Key\n",
    "api_key = \"<API_TOKEN>\"\n",
    "\n",
    "\n",
    "# AWS Variables\n",
    "s3_bucket = \"<YOUR_S3_BUCKET_NAME>\"\n",
    "aws_ecr_repo = \"<YOUR_ECR_REPO_NAME>\"\n",
    "sagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-Demo\"\n",
    "aws_region = \"us-east-1\"\n",
    "\n",
    "aws_access_key_id = \"\"\n",
    "aws_secret_access_key = \"\"\n",
    "aws_session_token = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dr.Client(\n",
    "    token=api_key,\n",
    "    endpoint=dr_host,\n",
    "    user_agent_suffix=\"AIA-E2E-AWS-7\",  # Optional but helps DataRobot improve this workflow\n",
    ")\n",
    "\n",
    "dr.client._global_client = client\n",
    "# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n",
    "# dr.Client(config_path = 'path-to-drconfig.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "This section of the notebook focueses on the steps for creating and exporting an ML model developed within DataRobot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project and initiate Autopilot\n",
    "\n",
    "In the following snippet you will upload your training data to DataRobot. This example uses a dataset of Lending Club loans to predict if a loan will default or not.\n",
    "\n",
    "This example sets an advanced option for the project to include only models that are compatible with Scoring Code Export. Java Scoring Code can be downloaded as a binary file or compiled, and contains all of the data transformations, feature engineering and final model parameters from the DataRobot Model . Since the data and feature engineering pipeline are completely contained in the portable JAR file, predictions can be made outside of DataRobot, as long as the scoring data is in the same format as the training data. More information can be [found in the DataRobot documentation](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/index.html#model-support).\n",
    "\n",
    "Next, you will initiate Autopilot to build models.\n",
    "\n",
    "If you already have a model that you want to deploy, then this part can be skipped, but you must manually define the project and model ID below to continue using the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project, kick off Autopilot, and wait for completion\n",
    "df = pd.read_csv(\"training_data/10K_Lending_Club_Loans.csv\")\n",
    "\n",
    "advanced_options = dr.AdvancedOptions(\n",
    "    blend_best_models=False, scoring_code_only=True, prepare_model_for_deployment=True\n",
    ")\n",
    "\n",
    "project = dr.Project.create(\n",
    "    sourcedata=df,\n",
    "    project_name=\"DR_Demo_Sagemaker_{}\".format(\n",
    "        datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    ),\n",
    ")\n",
    "project.analyze_and_model(\n",
    "    target=\"is_bad\", worker_count=-1, advanced_options=advanced_options\n",
    ")\n",
    "project.wait_for_autopilot(verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our Project ID and the ID of the top model in the leaderboard for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your Project ID and Model ID of the top rated model on the leaderboard\n",
    "project_id = project.id\n",
    "top_model = project.get_top_model()\n",
    "model_id = top_model.id\n",
    "\n",
    "log.info(\"Project ID: {} | Model ID: {}\".format(project_id, model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export a DataRobot model\n",
    "\n",
    "Use the following cells to download the model as a Scoring Code JAR file (in a local directory called model) and then compress that file into a .tar.gz archive to upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a helper function that downloads your JAR file to your local system from a target server\n",
    "# The output returns the model path name if the file has been downloaded and returns None if not\n",
    "\n",
    "\n",
    "def get_scoring_code(session, host, project_id, model_id):\n",
    "    apiEndpoint = format(\n",
    "        \"{}/projects/{}/models/{}/scoringCode/\".format(host, project_id, model_id)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        r = session.get(apiEndpoint)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        log.error(err)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(api_key)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "log.info(\"Getting scoring code jar file from DataRobot location: \" + dr_host)\n",
    "# Get scoring code jar\n",
    "output = get_scoring_code(session, dr_host, project_id, model_id)\n",
    "if output is None:\n",
    "    log.error(\"download failed\")\n",
    "else:\n",
    "    # Model name is grabbed from Content-Disposition header, which provides a dynamically generated suggested name for the model (usually model_id.jar)\n",
    "    modeldir = \"model/\"\n",
    "\n",
    "    # cCeate local model directory if it doesn't exist already\n",
    "    if not os.path.exists(modeldir):\n",
    "        os.mkdir(modeldir)\n",
    "\n",
    "    fd = output.headers.get(\"Content-Disposition\")\n",
    "    modelname = fd.split(\";\")[1].strip().split(\"=\")[1]\n",
    "    modelpath = modeldir + modelname\n",
    "\n",
    "    with open(modelpath, \"wb\") as f:\n",
    "        f.write(output.content)\n",
    "\n",
    "    log.info(\"Scoring Code jar downloaded to {}\".format(modelpath))\n",
    "\n",
    "    # Compress the jar file into a tar.gz as required by SageMaker\n",
    "    log.info(\"Compressing jar file into tar.gz\")\n",
    "    tgz_name = modelname + \".tar.gz\"\n",
    "    tgz_path = modeldir + tgz_name\n",
    "\n",
    "    with tarfile.open(tgz_path, \"w:gz\") as tar:\n",
    "        tar.add(modelpath, arcname=os.path.basename(modelpath))\n",
    "\n",
    "    log.info(\"COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import to AWS\n",
    "\n",
    "This section of the notebook focuses on the steps required to prepare AWS for hosting a DataRobot model within SageMaker. It includes examples for how to make predictions against the model for both real-time and batch use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download docker runtime image\n",
    "\n",
    "This step will pull down the scoring-inference-code-sagemaker docker image that will be used to run the Scoring Code JAR file in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Pull down the scoring-inference-code-sagemaker image that will be pushed to AWS ECR for hosting our Scoring Code models in Sagemaker\n",
    "docker pull datarobot/scoring-inference-code-sagemaker:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an AWS session connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AWS Boto3 Session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token,\n",
    "    region_name=aws_region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an AWS ECR repo to hold the `scoring-inference-code-sagemaker` docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AWS ECR repo\n",
    "log.info(\"Creating ECR Repo to hold our base image for running scoring code jar file.\")\n",
    "ecr_client = session.client(\"ecr\")\n",
    "\n",
    "ecr_response = ecr_client.create_repository(repositoryName=aws_ecr_repo)\n",
    "\n",
    "log.info(\"ECR Name: {}\".format(ecr_response.get(\"repository\").get(\"repositoryName\")))\n",
    "log.info(\"ECR ARN: {}\".format(ecr_response.get(\"repository\").get(\"repositoryArn\")))\n",
    "log.info(\"ECR URI: {}\".format(ecr_response.get(\"repository\").get(\"repositoryUri\")))\n",
    "\n",
    "ecr_repo_uri = ecr_response.get(\"repository\").get(\"repositoryUri\")\n",
    "ecr_registry_id = ecr_response.get(\"repository\").get(\"registryId\")\n",
    "registry_url = ecr_registry_id + \".dkr.ecr.\" + aws_region + \".amazonaws.com\"\n",
    "\n",
    "log.info(\"ECR Repo created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can push `scoring-inference-code-sagemaker` to the ECR repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$ecr_repo_uri\" \"$registry_url\" \"$aws_access_key_id\" \"$aws_secret_access_key\" \"$aws_session_token\" \"$aws_region\"\n",
    "# Push datarobot/scoring-inference-code-sagemaker:latest to ECR Repo\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=$3\n",
    "export AWS_SECRET_ACCESS_KEY=$4\n",
    "export AWS_SESSION_TOKEN=$5\n",
    "\n",
    "docker login -u AWS -p $(aws ecr get-login-password --region $6) $2\n",
    "docker tag datarobot/scoring-inference-code-sagemaker:latest $1:latest\n",
    "docker push $1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an S3 Bucket\n",
    "\n",
    "This S3 bucket stores your DataRobot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 Bucket\n",
    "log.info(\"Creating S3 Bucket {}\".format(s3_bucket))\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "try:\n",
    "    s3.create_bucket(Bucket=s3_bucket)\n",
    "    log.info(\"S3 Bucket Creation Complete!\")\n",
    "except ClientError as e:\n",
    "    log.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, upload the Scoring Code JAR file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload scoring code jar tarball to AWS S3\n",
    "log.info(\"Uploading {} to S3 Bucket: {}\".format(tgz_name, s3_bucket))\n",
    "s3 = session.resource(\"s3\")\n",
    "\n",
    "s3_obj_name_model = \"sagemaker/models/\" + tgz_name\n",
    "try:\n",
    "    s3.meta.client.upload_file(tgz_path, s3_bucket, s3_obj_name_model)\n",
    "    log.info(\"S3 Upload Complete!\")\n",
    "except ClientError as e:\n",
    "    log.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload sample data to S3\n",
    "\n",
    "In this cell, you upload a sample dataset to make batch predictions in SageMaker. This dataset is specifically designed for the model that was created earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Batch Scoring Data to S3\n",
    "batch_path = \"scoring_data/10K_Lending_Club_Loans_scoring.csv\"\n",
    "\n",
    "log.info(\"Uploading {} to S3 Bucket: {}\".format(batch_path, s3_bucket))\n",
    "s3 = session.resource(\"s3\")\n",
    "\n",
    "s3_obj_name_csv = \"sagemaker/\" + batch_path\n",
    "try:\n",
    "    s3.meta.client.upload_file(batch_path, s3_bucket, s3_obj_name_csv)\n",
    "    batch_input_file = \"s3://\" + s3_bucket + \"/\" + s3_obj_name_csv\n",
    "    log.info(\"S3 Upload Complete!\")\n",
    "except ClientError as e:\n",
    "    log.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will create an IAM role for SageMaker that will grant access to run things within SageMaker itself, and to allow for access to the S3 bucket contianing the uploaded Scoring Code model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM Role for Sagemaker to use\n",
    "log.info(\"Creating Execution IAM Role for Sagemaker to use\")\n",
    "iam = session.client(\"iam\")\n",
    "iamr = session.resource(\"iam\")\n",
    "\n",
    "role_policy = json.dumps(\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Action\": [\"s3:ListBucket\"],\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket],\n",
    "            },\n",
    "            {\n",
    "                \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket + \"/*\"],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "log.info(\"Creating Exeuction S3 Access Policy\")\n",
    "policy = iam.create_policy(\n",
    "    PolicyName=sagemaker_execution_role_name + \"-policy\", PolicyDocument=role_policy\n",
    ")\n",
    "\n",
    "policy_arn = policy.get(\"Policy\").get(\"Arn\")\n",
    "\n",
    "assume_role_policy_document = json.dumps(\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "log.info(\"Creating actual role\")\n",
    "role = iam.create_role(\n",
    "    RoleName=sagemaker_execution_role_name,\n",
    "    AssumeRolePolicyDocument=assume_role_policy_document,\n",
    ")\n",
    "\n",
    "# Attach our execution Policy\n",
    "log.info(\"Attaching Execution Policy to Role\")\n",
    "response = iam.attach_role_policy(\n",
    "    RoleName=sagemaker_execution_role_name, PolicyArn=policy_arn\n",
    ")\n",
    "\n",
    "# Attach the AmazonSageMakerFullAccess Policy\n",
    "log.info(\"Attaching AmazonSageMakerFullAccess Policy to Role\")\n",
    "response = iam.attach_role_policy(\n",
    "    RoleName=sagemaker_execution_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n",
    ")\n",
    "\n",
    "role_resource = iamr.Role(sagemaker_execution_role_name)\n",
    "log.info(\"IAM Role Info:\")\n",
    "log.info(\"IAM Role Name: {}\".format(role_resource.name))\n",
    "log.info(\"IAM Role ARN: {}\".format(role_resource.arn))\n",
    "log.info(\"IAM Role Policies:\")\n",
    "for p in role_resource.attached_policies.all():\n",
    "    log.info(p)\n",
    "\n",
    "log.info(\"COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in sagemaker\n",
    "log.info(\"Creating Sagemaker Model\")\n",
    "sm_client = session.client(\"sagemaker\")\n",
    "aws_model_name = modelname.split(\".\")[0]\n",
    "\n",
    "response = sm_client.create_model(\n",
    "    ModelName=aws_model_name,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": ecr_repo_uri + \":latest\",\n",
    "        \"ImageConfig\": {\"RepositoryAccessMode\": \"Platform\"},\n",
    "        \"Mode\": \"SingleModel\",\n",
    "        \"ModelDataUrl\": \"s3://\" + s3_bucket + \"/\" + s3_obj_name_model,\n",
    "    },\n",
    "    ExecutionRoleArn=role_resource.arn,\n",
    ")\n",
    "\n",
    "if response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n",
    "    log.error(\"Error when creating model in Sagemaker\")\n",
    "else:\n",
    "    log.info(\"Sagemaker Model Created!\")\n",
    "    log.info(\"model name: {}\".format(aws_model_name))\n",
    "    log.info(\"model arn: {}\".format(response.get(\"ModelArn\")))\n",
    "    log.info(\"COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker endpoint configuration\n",
    "\n",
    "This is used as part of the assembly of a SageMaker Endpoint that is required for real time API prediction requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sagemaker Endpoint Configuration\n",
    "log.info(\"Creating Sagemaker Model Endpoint Configuration\")\n",
    "aws_endpoint_config_name = aws_model_name + \"-ec\"\n",
    "\n",
    "ec_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=aws_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant-1\",\n",
    "            \"ModelName\": aws_model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "if ec_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n",
    "    log.error(\"Error when creating model in Sagemaker\")\n",
    "else:\n",
    "    log.info(\"Sagemaker Model Endpoint Configuration Created!\")\n",
    "    log.info(\"endpoint configuration name: {}\".format(aws_endpoint_config_name))\n",
    "    log.info(\n",
    "        \"endpoint configuration arn: {}\".format(ec_response.get(\"EndpointConfigArn\"))\n",
    "    )\n",
    "    log.info(\"COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to create a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sagemaker Endpoint\n",
    "log.info(\"Creating Sagemaker Model Endpoint... This process can take a few minutes\")\n",
    "aws_endpoint_name = aws_model_name + \"-ep\"\n",
    "\n",
    "ep_response = sm_client.create_endpoint(\n",
    "    EndpointName=aws_endpoint_name,\n",
    "    EndpointConfigName=aws_endpoint_config_name,\n",
    ")\n",
    "\n",
    "if ep_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n",
    "    log.error(\"Error when sending endpoint creation request to Sagemaker\")\n",
    "    log.error(ep_response)\n",
    "else:\n",
    "    i = 0\n",
    "    while i < 30:\n",
    "        status_r = sm_client.describe_endpoint(EndpointName=aws_endpoint_name)\n",
    "        status = status_r.get(\"EndpointStatus\")\n",
    "        log.info(\"Endpoint Creation Status: {}\".format(status_r.get(\"EndpointStatus\")))\n",
    "\n",
    "        if status == \"InService\":\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(20)\n",
    "            i = i + 1\n",
    "\n",
    "    if status == \"InService\":\n",
    "        log.info(\"Sagemaker Model Endpoint Created!\")\n",
    "        log.info(\"Endpoint Name: {}\".format(status_r.get(\"EndpointName\")))\n",
    "        log.info(\"Endpoint ARN: {}\".format(status_r.get(\"EndpointArn\")))\n",
    "        invocation_url = \"https://runtime.sagemaker.{}.amazonaws.com/endpoints/{}/invocations\".format(\n",
    "            aws_region, status_r.get(\"EndpointName\")\n",
    "        )\n",
    "        log.info(\"Endpoint API URL: {}\".format(invocation_url))\n",
    "        log.info(\"COMPLETE!\")\n",
    "    else:\n",
    "        log.error(\"Sagemaker did not return an 'InService' status in time!\")\n",
    "        log.error(\"Last status received: {}\".format(status))\n",
    "        log.error(status_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicion examples\n",
    "The following cells will show how to make predictions against the deployed model using both batch and real-time methods.\n",
    "  \n",
    "### Create SageMaker Batch Transform Job\n",
    "\n",
    "Use this cell to programatically create a batch transform job in SageMaker that can be used for batch predictions.  This job reads in a CSV that you previously uploaded to an S3 bucket. The output of the job will then be written to another folder (`scoring_output`) that will exist in the S3 bucket that you previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch transform job for batch predictions\n",
    "log.info(\"Creating Sagemaker Batch Transform Job\")\n",
    "btj_client = session.client(\"sagemaker\")\n",
    "\n",
    "job_name = (\n",
    "    aws_model_name + \"-batch-transform-job-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    ")\n",
    "\n",
    "batch_output_folder = \"s3://\" + s3_bucket + \"/scoring_output/\"\n",
    "response = btj_client.create_transform_job(\n",
    "    TransformJobName=job_name,\n",
    "    ModelName=aws_model_name,\n",
    "    TransformInput={\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input_file}\n",
    "        },\n",
    "        \"ContentType\": \"string\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"SplitType\": \"None\",\n",
    "    },\n",
    "    TransformOutput={\n",
    "        \"S3OutputPath\": batch_output_folder,\n",
    "        \"Accept\": \"string\",\n",
    "        \"AssembleWith\": \"None\",\n",
    "    },\n",
    "    TransformResources={\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n",
    ")\n",
    "\n",
    "# Response\n",
    "log.info(\"Running Sagemaker Batch Transform Job {}\".format(job_name))\n",
    "i = 0\n",
    "while i < 30:\n",
    "    status_r = btj_client.describe_transform_job(TransformJobName=job_name)\n",
    "    status = status_r.get(\"TransformJobStatus\")\n",
    "    log.info(\"Batch Job Status: {}\".format(status_r.get(\"TransformJobStatus\")))\n",
    "\n",
    "    if status not in [\"InProgress\", \"Stopping\"]:\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(20)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View batch transform Job results\n",
    "\n",
    "In this cell you download the results file from the batch transform job that you just ran in SageMaker and output the contents of the dataframe to show what was scored.  \n",
    "\n",
    "In this case, you are scoring a binary classification model, so your output will be two columns that contain the scores of our positive and negative classes, which will translate into whether a potential loan will default or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client(\"s3\")\n",
    "output_dir = \"scoring_output\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "for key in s3.list_objects(Bucket=s3_bucket)[\"Contents\"]:\n",
    "    if \".out\" in key[\"Key\"]:\n",
    "        s3.download_file(s3_bucket, key[\"Key\"], \"scoring_output/output.csv\")\n",
    "\n",
    "df = pd.read_csv(output_dir + \"/output.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time predictions with SageMaker\n",
    "\n",
    "This cell shows how to interact with the SageMaker endpoint that you previously created for your model to use with real time prediction workloads.  \n",
    "\n",
    "You will be using the AWS boto3 client and making a call to the SageMaker endpoint to score a row of data from a CSV file and then print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_client = session.client(\"sagemaker-runtime\")\n",
    "\n",
    "buffer = open(\"scoring_data/1_row_Lending_Club_Loans_scoring.csv\")\n",
    "payload = buffer.read()\n",
    "\n",
    "response = s_client.invoke_endpoint(\n",
    "    EndpointName=aws_endpoint_name, ContentType=\"text/csv\", Body=payload\n",
    ")\n",
    "\n",
    "data = response.get(\"Body\").read()\n",
    "log.info(\"Scoring output:\\n{}\".format(data.decode(\"utf-8\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
