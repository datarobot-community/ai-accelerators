{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afd1ad56-d456-4031-b49a-fa93166294bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# End to end ML workflow with Databricks\n",
    "\n",
    "This notebook illustrates and end-to-end data science workflow using features of both DataRobot and Databricks. You will leverage DataRobot for model training, selection, and MLOps monitoring while using Databricks to facilitate datasource access and utilize the Spark backbone to perform distributed scoring to support large-scale use cases.\n",
    "\n",
    "This notebook covers the following steps:\n",
    "- Acquiring a training dataset from a data table\n",
    "- Building a new DataRobot project\n",
    "- Deploying a recommended model\n",
    "- Scoring via Spark using DataRobot's exportable Java scoring code\n",
    "- Scoring via prediction API\n",
    "- Reporting monitoring data to DataRobot's MLOps agent framework\n",
    "- Writing results back to a new table\n",
    "\n",
    "Prior to execution, you need to install a few dependencies to the Databricks cluster:\n",
    "- **datarobot**, provided via PyPI (Python library used to communicate with the DataRobot platform)\n",
    "- **com.datarobot:datarobot-prediction:2.2.1**, provided via Maven Central (Java library used to establish interface with DataRobot scoring code)\n",
    "- **com.datarobot:scoring-code-spark-api_3.0.0:0.0.4**, provided via Maven Central (Java library used to wrap scoring code with Spark functionality)\n",
    "- **mlops_utils_for_spark_3_2_0_8_1_0-4c992.jar**, provided via downloadable MLOps package which is available on the Developer Tools page in the DataRobot UI (Java library used to report monitoring statistics to MLOps Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new Autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d02cfdc-4829-4e2e-95f3-2dbc463a6512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "import datarobot as dr\n",
    "import pandas as pd\n",
    "from py4j.java_gateway import java_import\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import requests\n",
    "\n",
    "api_key = \"\"  # Get this from the Developer Tools page in the DataRobot UI\n",
    "endpoint = \"https://app.datarobot.com/\"  # This should be the URL you use to access the DataRobot UI\n",
    "\n",
    "client = dr.Client(\n",
    "    token=api_key,\n",
    "    endpoint=endpoint,\n",
    "    user_agent_suffix=\"AIA-E2E-DBX-8\",  # Optional but helps DataRobot improve this workflow\n",
    ")\n",
    "\n",
    "dr.client._global_client = client\n",
    "\n",
    "# Set these to empty strings to create a new project and/or deployment\n",
    "project_id = \"\"\n",
    "deployment_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x7fb7a067f940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.Client()\n",
    "# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n",
    "# dr.Client(config_path = 'path-to-drconfig.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dccce514-7348-4598-877c-a7e57f88fbc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import data\n",
    "\n",
    "Here you'll pull in some data to work with. If a data table is available, you can provide the input table name, destination table name, and target feature in this cell. If none of those are provided, load the sample dataset provided by Databricks. This is also where any necessary data preparation would occur before sending the dataset to DataRobot. Note that DataRobot does not currently ingest Spark dataframes directly, so the dataframe will need to be converted to a Pandas dataframe prior to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "660234be-64c8-44a4-9f60-bf11bed12918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_table = \"\"\n",
    "scoring_table = \"\"\n",
    "target = \"\"\n",
    "\n",
    "if training_table == \"\":\n",
    "    scoring_table = \"white_wine_scored\"\n",
    "    target = \"quality\"\n",
    "    input_df = (\n",
    "        spark.read.option(\"header\", True)\n",
    "        .option(\"delimiter\", \";\")\n",
    "        .csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\")\n",
    "    )\n",
    "    input_df = input_df.select(\n",
    "        [col(column).alias(column.replace(\" \", \"_\")) for column in input_df.columns]\n",
    "    )\n",
    "else:\n",
    "    input_df = sql(\"select * from %s\" % (training_table))\n",
    "\n",
    "df = input_df.toPandas()\n",
    "display(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "731072e7-6ef9-4478-a1bc-161ff5b836d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a project\n",
    "\n",
    "The Pandas dataframe is uploaded to the DataRobot platform and a name is given to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d39173-8ee4-4975-959a-2c50092853b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a project wothout setting the target\n",
    "if project_id == \"\":\n",
    "    project = dr.Project.create(\n",
    "        project_name=\"New Test Project (Databricks)\", sourcedata=df\n",
    "    )\n",
    "    print(project.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9c75f3-1079-4bc6-b978-b73d788b1d72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modeling\n",
    "\n",
    "### Set the target feature\n",
    "\n",
    "Here you can define any advanced options needed for your project, including the Autopilot mode you wish to run (Standard Autopilot, Quick Mode, Comprehensive Mode, Manual). This API call will set our desired target feature and then kick off the EDA2 process, followed immediately by model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4840eaf6-28af-4666-81cd-c2fce658885d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if project_id == \"\":\n",
    "    mode = dr.enums.AUTOPILOT_MODE.QUICK\n",
    "\n",
    "    project.analyze_and_model(\n",
    "        target=target,\n",
    "        mode=mode,\n",
    "        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n",
    "        max_wait=600,\n",
    "    )\n",
    "    # When you get control back, that means EDA is finished and model jobs are in flight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9bf2359-a51a-4a59-937a-e515335a4cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Start Autopilot\n",
    "\n",
    "This optional API call will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb73f7be-4315-4258-962f-39318629b08a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if project_id == \"\":\n",
    "    # This is helpful if you want to keep execution serial:\n",
    "    project.wait_for_autopilot()\n",
    "\n",
    "    # Otherwise you can periodically ask the project for its current autopilot status:\n",
    "    # project.stage\n",
    "    # project.get_model_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1148bf2f-6e33-460b-a2dd-ea5f9e514df7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### List models\n",
    "\n",
    "This API call outputs a list of all the models trained in the project, sorted by the selected validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b62c3b6-3cdf-479c-b845-a16a86f0194a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optionally, skip Autopilot and start here:\n",
    "if project_id != \"\":\n",
    "    project = dr.Project.get(project_id)\n",
    "\n",
    "# Pull the list of all models. You can iterate over these and examine them.\n",
    "project.get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c3004ad-75b9-4711-a5ea-bb7b27305dff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Retrieve the recommended model\n",
    "\n",
    "DataRobot provides a recommendation for an accurate and performant model at the end of Autopilot. This API call will fetch that recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7c4e19a-6967-44d9-9c6e-adb8f210e212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(dr.ModelRecommendation.get_all(project.id))\n",
    "rec = dr.ModelRecommendation.get(\n",
    "    project_id=project.id,\n",
    "    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n",
    ")\n",
    "selection = rec.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74ce0200-074c-4a65-bde0-627d0feaef97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Deploy a model\n",
    "\n",
    "If no deployment ID was specified during setup, deploy DataRobot's recommended model. This will make the model available via the dedicated prediction API, and will wrap the model in our MLOps monitoring framework. Optional monitoring features are also enabled here, including accuracy tracking and data drift monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "872cc93f-8916-4321-99a2-8fe7175277ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When you are happy with your model you can automate deployment\n",
    "if deployment_id == \"\":\n",
    "    prediction_server = dr.PredictionServer.list()[\n",
    "        0\n",
    "    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n",
    "    deployment = dr.Deployment.create_from_learning_model(\n",
    "        model_id=selection.id,\n",
    "        label=\"New Test Deployment\",\n",
    "        description=\"Some extra data that I can use to search later.\",\n",
    "        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n",
    "    )\n",
    "    deployment.update_association_id_settings(\n",
    "        column_names=[\"id\"], required_in_prediction_requests=False\n",
    "    )\n",
    "    deployment.update_drift_tracking_settings(\n",
    "        target_drift_enabled=True, feature_drift_enabled=True\n",
    "    )\n",
    "else:\n",
    "    deployment = dr.Deployment.get(deployment_id)\n",
    "\n",
    "print(deployment.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4eee441-7963-4b58-9092-bb570344e766",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Score a Spark Dataframe\n",
    "\n",
    "The Spark wrapper that you imported into your cluster allows you to use the distributed power of the Spark cluster to quickly score large datasets. The following cells provide examples of scoring a Spark dataframe using Python or Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c351a3a-12ca-4edc-851d-71d51d6e6a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Score with Python\n",
    "\n",
    "Python can be used to invoke the Java methods you provide to score with DataRobot models. The method call on **line 7** dynamically reaches out to the DataRobot platform to download the Scoring Code and make it available in your classpath. To avoid waiting for the network transfer, the scoring code can be downloaded ahead of time and imported as a new library in the Databricks cluster.\n",
    "\n",
    "In order to perform the scoring transformation on the Spark dataframe, you must convert it to a Java dataframe and then back to a PySpark dataframe after scoring. You also capture the overall time it took to score in order to report that metric back to DataRobot MLOps in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef8dcc71-510d-4813-8f05-05147e1a95a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "java_import(spark._jvm, \"com.datarobot.prediction.Predictors\")\n",
    "java_import(spark._jvm, \"com.datarobot.prediction.spark30.Model\")\n",
    "java_import(spark._jvm, \"com.datarobot.prediction.spark30.Predictors\")\n",
    "\n",
    "start_time = (\n",
    "    time.time()\n",
    ")  # Grab timestamps before and after scoring to provide MLOps with an estimated execution time.\n",
    "# This next method call will use the endpoint, API token, and Deployment ID that were defined in previous cells to fetch our Scoring Code.\n",
    "dr_model = (\n",
    "    spark._jvm.com.datarobot.prediction.spark30.Predictors.getPredictorFromDeployment(\n",
    "        endpoint, deployment.id, api_key\n",
    "    )\n",
    ")\n",
    "\n",
    "output_df = DataFrame(\n",
    "    dr_model.transform(input_df._jdf), spark\n",
    ")  # Apply the scoring transformation\n",
    "score_time = (\n",
    "    time.time() - start_time\n",
    ")  # Get the total runtime of the fetching and scoring process\n",
    "\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789bedb8-1e15-48ca-971f-0956865e3939",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Score with Scala\n",
    "\n",
    "The following cell performs the same scoring action as the previous one, only using Scala instead of Python.\n",
    "\n",
    "This cell is commented out by default since variable values aren't shared between language contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e793c313-767a-4f7d-8506-c5bc76612c5e",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "/**\n",
    "import com.datarobot.prediction.spark30.Predictors\n",
    "\n",
    "val apiKey = \"\" //Provide DataRobot API token here\n",
    "val endpoint = \"https://app.datarobot.com/\" //This is the URL that you use to access the DataRobot UI\n",
    "val deploymentId = \"\" //The ID oif the deployment you'd like to use for scoring\n",
    "val inputDf = sql(\"select * from loans\") //Substitute a table name here\n",
    "\n",
    "val javaModel = Predictors.getPredictorFromDeployment(endpoint,deploymentId,apiKey)\n",
    "\n",
    "val outputDf = javaModel.transform(inputDf)\n",
    "display(outputDf)\n",
    "**/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd7788c-01af-4171-9650-09d70ebbebca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Score with the Prediction API\n",
    "\n",
    "This cell demonstrates scoring using a Pandas dataframe and the native DataRobot prediction API. This scoring method is limited to payloads under 50MB, so is not ideal for large datasets. An advantage to using this method would be easier access to monitoring data, since it does not require setup of the agent-based external monitoring framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e43e6153-a4de-46ae-8887-c61455dd0161",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "host = \"https://example.dynamic.orm.datarobot.com\"  # This should be the URL of your prediction server, which you can find in the Deployment Overview page of the UI\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "    \"Accept\": \"text/csv\",\n",
    "    \"datarobot-key\": \"\",  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n",
    "    \"Authorization\": \"Bearer %s\" % (api_key),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"passthroughColumnsSet\": \"all\"  # This line tells the API to reflect back the input data along with the predictions\n",
    "}\n",
    "\n",
    "data = df.to_json(orient=\"records\")\n",
    "response = requests.post(\n",
    "    \"{:}/predApi/v1.0/deployments/{:}/predictions\".format(host, deployment.id),\n",
    "    data=data,\n",
    "    headers=headers,\n",
    "    params=params,\n",
    ")\n",
    "\n",
    "api_df = pd.read_csv(\n",
    "    StringIO(response.text)\n",
    ")  # Here you read the API's CSV output into a Pandas dataframe\n",
    "display(api_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d315e7e-6129-4bff-afbd-02a5257ff268",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Report monitoring data\n",
    "\n",
    "Pass monitoring data to the appropriate message channel - a Kafka topic, in this case. From there our external monitoring agent will pick up this data and pass it back to the DataRobot platform for display in the MLOps dashboard.\n",
    "\n",
    "Note that **this cell is provided as an example only**, and will not be executable without completing the full setup of the MLOps monitoring agent. More information regarding MLOps Monitoring Agent setup can be found [in the DataRobot documentation](https://docs.datarobot.com/en/docs/mlops/deployment/mlops-agent/monitoring-agent/index.html). This code cell illustrates the client library invocation that will push monitoring data to a message queue. More services need to be setup external to this notebook to complete the transfer of monitoring data to DataRobot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90ae7d33-9166-45aa-ab4d-54f6cadbc994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# java_import(spark._jvm, \"com.datarobot.mlops_spark_utils.MLOpsSparkUtils\")\n",
    "# channelConfig = \"spooler_type=kafka;kafka_topic_name=monitoring-agent-topic\"\n",
    "\n",
    "# spark._jvm.com.datarobot.mlops_spark_utils.MLOpsSparkUtils.reportPredictions(\n",
    "#     output_df._jdf, # scoring data\n",
    "#     deployment.id, # DeploymentId\n",
    "#     selection.id, # ModelId\n",
    "#     channelConfig, # MLOps channel configuration\n",
    "#     float(score_time), # scoring time\n",
    "#     ['target_1_PREDICTION','target_0_PREDICTION'] # target columns\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a42f77b-6ab8-4c15-873f-6aeb6f3d8709",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write Results\n",
    "You can now write our results back to a table. In this case you'll create a new table since the original source table's schema doesn't include columns to hold the scores or prediction explanations. In this example you are converting the results from the DataRobot Prediction API back to a Spark dataframe to facilitate writing to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d88ff6-c099-45b5-b6ac-fc31c337de4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "api_spark_df = spark.createDataFrame(api_df)\n",
    "api_spark_df.write.mode(\"overwrite\").saveAsTable(scoring_table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "End to End",
   "notebookOrigID": 613597050192562,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
