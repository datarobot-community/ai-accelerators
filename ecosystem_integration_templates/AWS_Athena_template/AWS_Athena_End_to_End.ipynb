{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><H1>DataRobot AutoML end-to-end with Amazon Athena</H1></center>\n",
    "\n",
    "<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n",
    "</td>\n",
    "<td><font size=10> + </font> </td>\n",
    "<td> <img src=\"https://vectorwiki.com/images/1BalA__aws-athena.svg\" height=100px width=100px> </td>\n",
    "\n",
    "Author: Biju Krishnan\n",
    "\n",
    "[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>\n",
    "This example notebook outlines the following tasks: <p>\n",
    "<ol>\n",
    "<li> Read in an Amazon Athena table and upload it to DataRobot's AI Catalog </li>\n",
    "<li> Create a project with the dataset</li>\n",
    "<li> Deploy the top performing model to a DataRobot prediction server </li>\n",
    "<li> Make batch predictions with a test dataset </li>\n",
    "</ol>\n",
    "<p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables can aso be fetched from a secret store or config files\n",
    "DATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n",
    "# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n",
    "\n",
    "DATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n",
    "# The API Token can be found by click the avatar icon and then </> Developer Tools\n",
    "\n",
    "client = dr.Client(\n",
    "    token=DATAROBOT_API_TOKEN,\n",
    "    endpoint=DATAROBOT_ENDPOINT,\n",
    "    user_agent_suffix=\"AIA-E2E-AWS-16\",  # Optional but helps DataRobot improve this workflow\n",
    ")\n",
    "\n",
    "dr.client._global_client = client\n",
    "\n",
    "AWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\n",
    "AWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "You can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT, ssl_verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line reads the driver object needed for creating a datastore\n",
    "athena_driver = [drv for drv in dr.DataDriver.list() if drv.canonical_name == \"AWS Athena (v5)\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "### Create a data connection\n",
    "\n",
    "Use the cell below to define the parameters required to make a connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_s3_bucket = \"e2eaccelerator09122022\"  # Specifythe name of the bucket followed by any prefix, later you format it as an S3 URI\n",
    "\n",
    "jdbc_url = \"jdbc:awsathena://athena.eu-west-1.amazonaws.com;AwsRegion=eu-west-1;S3OutputLocation=s3://{}/\".format(\n",
    "    athena_s3_bucket\n",
    ")\n",
    "\n",
    "query = 'SELECT * FROM \"new_york_taxi\".\"input\" limit 10000;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data connection (AKA a datastore)\n",
    "\n",
    "DR_DATASTORE_NAME = \"ATHENA Data Connection\"  # This name can be altered\n",
    "\n",
    "# Checking if datastore already exists\n",
    "for dstore in dr.DataStore.list():\n",
    "    if dstore.canonical_name == DR_DATASTORE_NAME:\n",
    "        datastore_flag = False\n",
    "        datastore = dstore\n",
    "        break\n",
    "    else:\n",
    "        datastore_flag = True\n",
    "\n",
    "if datastore_flag:\n",
    "    datastore = dr.DataStore.create(\n",
    "        data_store_type=\"jdbc\",\n",
    "        canonical_name=\"ATHENA Data Connection\",  # This name can be replaced\n",
    "        driver_id=athena_driver.id,\n",
    "        jdbc_url=jdbc_url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data connection based on a query\n",
    "# The Athena JDBC driver only supports query-based ingestion\n",
    "\n",
    "params = dr.DataSourceParameters(data_store_id=datastore.id, query=query)\n",
    "\n",
    "DR_DATASOURCE_NAME = \"ATHENA Data Source\"  # This name can be altered\n",
    "\n",
    "for dsource in dr.DataSource.list():\n",
    "    if dsource.canonical_name == DR_DATASOURCE_NAME:\n",
    "        datasource_flag = False\n",
    "        datasource = dsource\n",
    "        break\n",
    "    else:\n",
    "        datasource_flag = True\n",
    "\n",
    "if datasource_flag:\n",
    "    datasource = dr.DataSource.create(\n",
    "        data_source_type=\"jdbc\",\n",
    "        canonical_name=\"ATHENA Data Source\",  # This name can be altered\n",
    "        params=params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet creates a snapshot of the Athena table and stores it in the AI Catalog\n",
    "\n",
    "datarobot_dataset = dr.Dataset.create_from_data_source(\n",
    "    data_source_id=datasource.id, username=AWS_KEY, password=AWS_SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project and initiate Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take several minutes to complete execution\n",
    "# An AutoML project named \"E2E Demo Amazon Athena\" is created with \"tip_amount\" as the target column\n",
    "# Quick mode is designated, however other modes are also available\n",
    "\n",
    "\n",
    "EXISTING_PROJECT_ID = None  # If you've already created a project, replace None with the ID here\n",
    "\n",
    "if EXISTING_PROJECT_ID is None:\n",
    "    # Create project and pass in data\n",
    "    project = dr.Project.create_from_dataset(\n",
    "        datarobot_dataset.id, project_name=\"E2E Demo Amazon Athena\"\n",
    "    )\n",
    "\n",
    "    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n",
    "    project.analyze_and_model(target=\"tip_amount\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\")\n",
    "else:\n",
    "    # Fetch the existing project\n",
    "    project = dr.Project.get(EXISTING_PROJECT_ID)\n",
    "\n",
    "project.wait_for_autopilot(check_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top-performing model\n",
    "\n",
    "Once the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_metric(models, test_set, metric):\n",
    "    models_with_score = [model for model in models if model.metrics[metric][test_set] is not None]\n",
    "\n",
    "    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n",
    "\n",
    "\n",
    "models = project.get_models()\n",
    "\n",
    "metric = project.metric\n",
    "\n",
    "# Get the top-performing model\n",
    "model_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n",
    "\n",
    "print(\n",
    "    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n",
    "        model=str(model_top), metric=metric\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a model\n",
    "\n",
    "Note that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction server\n",
    "prediction_server = dr.PredictionServer.list()[0]\n",
    "\n",
    "# Create a deployment\n",
    "deployment = dr.Deployment.create_from_learning_model(\n",
    "    model_top.id,\n",
    "    label=\"E2E Amazon Athena Test\",\n",
    "    description=\"Model trained on New York Taxi trips dataset\",\n",
    "    default_prediction_server_id=prediction_server.id,\n",
    ")\n",
    "deployment.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "DataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run a batch prediction job you need to store the Amazon Athena Credentials in the DataRobot credentials manager\n",
    "\n",
    "DR_CREDENTIAL_NAME = \"Amazon Athena Credentials\"  # Choose a name\n",
    "for cred in dr.Credential.list():\n",
    "    if cred.name == DR_CREDENTIAL_NAME:\n",
    "        cred_flag = False\n",
    "        athena_credential_id = cred.credential_id\n",
    "        break\n",
    "    else:\n",
    "        cred_flag = True\n",
    "\n",
    "# Create credentials in DataRobot credential store if they do not exist\n",
    "if cred_flag:\n",
    "    credential = dr.Credential.create_basic(\n",
    "        name=DR_CREDENTIAL_NAME,  # The username and password is the AWS KEY and SECRET respectively\n",
    "        user=AWS_KEY,\n",
    "        password=AWS_SECRET,\n",
    "    )\n",
    "    athena_credential_id = credential.credential_id\n",
    "\n",
    "print(athena_credential_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\n",
    "for cred in dr.Credential.list():\n",
    "    if cred.name == DR_CREDENTIAL_NAME:\n",
    "        cred_flag = False\n",
    "        s3_credential_id = cred.credential_id\n",
    "        break\n",
    "    else:\n",
    "        cred_flag = True\n",
    "\n",
    "# Create credentials in DataRobot credential store if it does not exist\n",
    "if cred_flag:\n",
    "    credential = dr.Credential.create_s3(\n",
    "        name=DR_CREDENTIAL_NAME,\n",
    "        aws_access_key_id=AWS_KEY,\n",
    "        aws_secret_access_key=AWS_SECRET,\n",
    "        # aws_session_token= <Optional>\n",
    "    )\n",
    "    s3_credential_id = credential.credential_id\n",
    "\n",
    "print(s3_credential_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predictions snippet\n",
    "\n",
    "The snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example scores the training data but there needs to be an Athena table with test data.\n",
    "\n",
    "job = dr.BatchPredictionJob.score(\n",
    "    deployment=deployment.id,\n",
    "    intake_settings={\n",
    "        \"type\": \"jdbc\",\n",
    "        \"query\": \"select * from new_york_taxi.input limit 1000\",  # This has to be a query, since the JDBC driver does not seem to understand table schema structure\n",
    "        \"data_store_id\": datastore.id,  # The ID of the datastore you want\n",
    "        \"credential_id\": athena_credential_id,  # The credentialid of the credentials stored in your credentials manager\n",
    "    },\n",
    "    output_settings={\n",
    "        \"type\": \"s3\",\n",
    "        \"url\": \"s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\",  # Note this has to be a filename and not just a bucket name\n",
    "        \"credential_id\": s3_credential_id,\n",
    "    },\n",
    ")\n",
    "job.wait_for_completion()\n",
    "job.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font family=verdana>\n",
    "<p>\n",
    "The output of the batch predictions is available under the path s3://e2eaccelerator09122022/predictions/output/\n",
    "<pre><code><font color=grey size=1>\n",
    "aws s3 ls s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\n",
    "2022-12-12 17:47:49      22725 new_york_taxi_predictions.csv\n",
    "</font></code></pre>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
