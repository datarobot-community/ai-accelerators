{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end modeling workflow with Azure\n",
    "\n",
    "Author: Brent Hinks (2023-01-27)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook illustrates an end-to-end data science workflow using DataRobot. The workflow ingests a dataset hosted in an Azure blob container, trains a series of models using DataRobot's AutoML capabilities, deploys a recommended model, and sets up a batch prediction job that writes predictions back to the original container.\n",
    "\n",
    "In this notebook you'll cover the following steps:\n",
    "\n",
    "- Acquiring a training dataset from an Azure storage container\n",
    "- Building a new DataRobot project\n",
    "- Deploying a recommended model\n",
    "- Scoring via batch prediction API\n",
    "- Writing results back to a source Azure container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Prior to execution, ensure that the following dependencies are available in your notebook environment:\n",
    "\n",
    "- **datarobot**, provided via PyPi (Python library used to communicate with the DataRobot platform)\n",
    "- **azure.storage.blob**, provided via PyPi (Python library used to access Azure storage services)\n",
    "- **pandas**, provided via PyPi (common data science library)\n",
    "- **Azure CLI**, used to authenticate to Azure. You can reference [installation instructions](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "The first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import datarobot as dr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DataRobot connection info here\n",
    "DATAROBOT_API_TOKEN = \"\"\n",
    "DATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n",
    "\n",
    "client = dr.Client(\n",
    "    token=DATAROBOT_API_TOKEN,\n",
    "    endpoint=DATAROBOT_ENDPOINT,\n",
    "    user_agent_suffix=\"AIA-E2E-AZURE-78\",  # Optional but helps DataRobot improve this workflow\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Azure connection blob info here\n",
    "AZURE_STORAGE_ACCOUNT = \"\"\n",
    "AZURE_STORAGE_CONTAINER = \"\"\n",
    "\n",
    "# Find this value by following the \"Access keys\" link from your storage account in the Azure console\n",
    "AZURE_STORAGE_ACCESS_KEY = \"\"\n",
    "\n",
    "# Provide dataset filenames and the modeling target feature\n",
    "AZURE_INPUT_FILE = \"input.csv\"\n",
    "AZURE_OUTPUT_FILE = \"scored.csv\"\n",
    "AZURE_INPUT_TARGET = \"target\"\n",
    "\n",
    "# Set name for Azure credentials in DataRobot\n",
    "DR_CREDENTIAL_NAME = \"Azure_{}\".format(AZURE_STORAGE_ACCOUNT)\n",
    "\n",
    "project_id = None\n",
    "deployment_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next cell, which creates the storage service client, you should run `az login` from your terminal to establish an authenticated session to Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_url = \"https://{}.blob.core.windows.net\".format(AZURE_STORAGE_ACCOUNT)\n",
    "blob_service_client = BlobServiceClient(account_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "Load the dataset stored in your Azure container into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_client = blob_service_client.get_container_client(\n",
    "    container=AZURE_STORAGE_CONTAINER\n",
    ")\n",
    "downloaded_blob = container_client.download_blob(AZURE_INPUT_FILE)\n",
    "\n",
    "df = pd.read_csv(StringIO(downloaded_blob.content_as_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that proper Azure credentials are stored in DataRobot. This credential can be used in the future to automate data reads and writes in scoring jobs. Check for an existing credential matching the name we provided above. If none is found, then create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to look up the ID of the credential object created.\n",
    "credential = None\n",
    "for cred in dr.Credential.list():\n",
    "    if cred.name == DR_CREDENTIAL_NAME:\n",
    "        credential = cred\n",
    "\n",
    "if credential == None:\n",
    "    credential = dr.Credential.create_azure(\n",
    "        name=DR_CREDENTIAL_NAME,\n",
    "        azure_connection_string=\"DefaultEndpointsProtocol=https;AccountName={};AccountKey={};\".format(\n",
    "            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_ACCESS_KEY\n",
    "        ),\n",
    "    )\n",
    "\n",
    "credential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Create a project\n",
    "\n",
    "Create a new project in DataRobot and upload the data stored in your dataframe. After that you will set the target and start the AutoML process.\n",
    "\n",
    "If a `project_id` was supplied above, skip these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project without setting the target\n",
    "if project_id == None:\n",
    "    project = dr.Project.create(project_name=\"New Test Project (Azure)\", sourcedata=df)\n",
    "    print(project.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initate Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_id == None:\n",
    "    mode = dr.enums.AUTOPILOT_MODE.QUICK\n",
    "\n",
    "    project.analyze_and_model(\n",
    "        target=AZURE_INPUT_TARGET,\n",
    "        mode=mode,\n",
    "        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n",
    "        max_wait=600,\n",
    "    )\n",
    "    # When you get control back, that means EDA is finished and model jobs are in flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_id == None:\n",
    "    # This is helpful if you want to keep execution serial:\n",
    "    project.wait_for_autopilot()\n",
    "\n",
    "    # Otherwise you can periodically ask the project for its current Autopilot status:\n",
    "    # project.stage\n",
    "    # project.get_model_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and deploy a model\n",
    "\n",
    "Review DataRobot's model recommendations and select one for deployment. If `deployment_id` was supplied above, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dr.ModelRecommendation.get_all(project.id))\n",
    "rec = dr.ModelRecommendation.get(\n",
    "    project_id=project.id,\n",
    "    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n",
    ")\n",
    "selection = rec.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with your model you can automate deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deployment_id == None:\n",
    "    prediction_server = dr.PredictionServer.list()[\n",
    "        0\n",
    "    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n",
    "    deployment = dr.Deployment.create_from_learning_model(\n",
    "        model_id=selection.id,\n",
    "        label=\"New Test Deployment (Azure)\",\n",
    "        description=\"Some extra data that I can use to search later.\",\n",
    "        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n",
    "    )\n",
    "    deployment.update_association_id_settings(\n",
    "        column_names=[\"id\"], required_in_prediction_requests=False\n",
    "    )\n",
    "    deployment.update_drift_tracking_settings(\n",
    "        target_drift_enabled=True, feature_drift_enabled=True\n",
    "    )\n",
    "else:\n",
    "    deployment = dr.Deployment.get(deployment_id)\n",
    "\n",
    "print(deployment.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make batch predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a batch prediction job that will read in your training dataset, produce scores with optional explanations, and write the results back to the original container. If any errors occur along the way, get details from `job.get_status()` to assist in troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = dr.BatchPredictionJob.score(\n",
    "    deployment=deployment.id,\n",
    "    intake_settings={\n",
    "        \"type\": \"azure\",\n",
    "        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n",
    "            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_INPUT_FILE\n",
    "        ),\n",
    "        \"credential_id\": credential.credential_id,\n",
    "    },\n",
    "    output_settings={\n",
    "        \"type\": \"azure\",\n",
    "        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n",
    "            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_OUTPUT_FILE\n",
    "        ),\n",
    "        \"credential_id\": credential.credential_id,\n",
    "    },\n",
    "    # Uncomment the next line to include prediction explanations.\n",
    "    # max_explanations=3,\n",
    "    passthrough_columns_set=\"all\",\n",
    ")\n",
    "job.wait_for_completion()\n",
    "job.get_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7f463399ecf6fc5046c51124d44b143cef2f2a6e53653f19ca35e0b63f07cf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
